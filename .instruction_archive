<instructions>
<initial_instructions>
The final instructions are in the end of this file. Please review them carefully. The goal is to reproduce the results of the paper. I will provide the paper, my original plan and some criticism. You are in an environment managed by "uv" (NOT uvicorn). To install dependencies, use "uv pip install [dependency]".
</initial_instructions>
<paper>
arXiv:2110.15794v1 [cs.CL] 26 Oct 2021
CLAUSEREC: A Clause Recommendation Framework for AI-aided
Contract Authoring
Aparna Garimella
Adobe Research
garimell@adobe.com
Vinay Aggarwal
Adobe Research
vinayagg@adobe.com
Anandhavelu N
Adobe Research
anandvn@adobe.com
Abstract
Contracts are a common type of legal docu-
ment that frequent in several day-to-day busi-
ness workflows. However, there has been very
limited NLP research in processing such doc-
uments, and even lesser in generating them.
These contracts are made up of clauses, and
the unique nature of these clauses calls for
specific methods to understand and generate
such documents. In this paper, we intro-
duce the task of clause recommendation, as
a first step to aid and accelerate the author-
ing of contract documents. We propose a two-
staged pipeline to first predict if a specific
clause type is relevant to be added in a con-
tract, and then recommend the top clauses for
the given type based on the contract context.
We pretrain BERT on an existing library of
clauses with two additional tasks and use it
for our prediction and recommendation. We
experiment with classification methods and
similarity-based heuristics for clause relevance
prediction, and generation-based methods for
clause recommendation, and evaluate the re-
sults from various methods on several clause
types. We provide analyses on the results, and
further outline the advantages and limitations
of the various methods for this line of research.
1 Introduction
A contract is a legal document between at least two
parties that outlines the terms and conditions of the
parties to an agreement. Contracts are typically in
textual format, thus providing a huge potential for
NLP applications in the space of legal documents.
However, unlike most natural language corpora that
are typically used in NLP research, contract lan-
guage is repetitive with high inter-sentence similar-
ities and sentence matches (Simonson et al., 2019),
calling for new methods specific to legal language
to understand and generate contract documents.
A contract is essentially made up of clauses,
which are provisions to address specific terms of
the agreement, and which form the legal essence
Balaji Vasan Srinivasan
Adobe Research
balsrini@adobe.com
Rajiv Jain
Adobe Research
rajijain@adobe.com
of the contract. Drafting a contract involves select-
ing an appropriate template (with skeletal set of
clauses), and customizing it for the specific pur-
pose, typically via adding, removing, or modifying
the various clauses in it. Both these stages involve
manual effort and domain knowledge, and hence
can benefit from assistance from NLP methods that
are trained on large collections of contract docu-
ments. In this paper, we attempt to take the first
step towards AI-assisted contract authoring, and
introduce the task of clause recommendation, and
propose a two-staged approach to solve it.
There have been some recent works on item-
based and content-based recommendations. Wang
and Fu (2020) reformulated the next sentence pre-
diction task in BERT (Devlin et al., 2019) as
next purchase prediction task to make a collabora-
tive filtering based recommendation system for e-
commerce setting. Malkiel et al. (2020) introduced
RecoBERT leveraging textual description of items
such as titles to build an item-to-item recommen-
dation system for wine and fashion domains. In
the space of text-based content recommendations,
Bhagavatula et al. (2018) proposed a method to rec-
ommend citations in academic paper drafts without
using metadata. However, legal documents remain
unexplored, and it is not straightforward to extend
these methods to recommend clauses in contracts,
as these documents are heavily domain-specific
and recommending content in them requires spe-
cific understanding of their language.
In this paper, clause recommendation is defined
as the process of automatically providing recom-
mendations of clauses that may be added to a given
contract while authoring it. We propose a two-
staged approach: first, we predict if a given clause
type is relevant to be added to the given input con-
tract; examples of clause types include governing
laws, confidentiality, etc. Next, if a given clause
type is predicted as relevant, we provide context-
aware recommendations of clauses belonging to
Figure 1: CLAUSEREC pipeline: Binary classification + generation for clause recommendation.
the given type for the input contract. We develop
CONTRACTBERT, by further pre-training BERT
using two additional tasks, and use it as the under-
lying language model in both the stages to adapt
it to contracts. To the best of our knowledge, this
is the first effort towards developing AI assistants
for authoring and generating long domain-specific
legal contracts.
2 Methodology
A contract can be viewed as a collection of clauses
with each clause comprising of: (a) the clause la-
bel that represents the type of the clause and (b)
the clause content. Our approach consists of two
stages: (1) clause type relevance prediction: pre-
dicting if a given clause type that is not present in
the given contract may be relevant to it, and (2)
clause recommendation: recommending clauses
corresponding to the given type that may be rele-
vant to the contract. Figure 1 shows an overview of
our proposed pipeline.
First, we build a model to effectively represent a
contract by further pre-training BERT, a pre-trained
Transformer-based encoder (Devlin et al., 2019),
on contracts to bias it towards legal language. We
refer to the resulting model as CONTRACTBERT.
In addition to masked language modelling and next
sentence prediction, CONTRACTBERT is trained
to predict (i) if the words in a clause label belong
to a specific clause, and (ii) if two sentences be-
long to the same clause, enabling the embeddings
of similar clauses to cluster together. Figure 2
and 3 show the difference in the performance of
BERT and CONTRACTBERT to get a meaningful
clause embedding. BERT is unable to differen-
tiate between the clauses of different types as it
is unfamiliar with legal language. On the other
Figure 2: Clustering of clauses using BERT
Embedding
Figure 3: Clustering of clauses using ContractBERT
Embedding
hand, CONTRACTBERT is able to cluster similar
clause types closely while ensuring the separation
between clauses of two different types.
2.1 Clause Type Relevance Prediction
Given a contract and a specific target clause type,
the first stage involves predicting if the given type
may be relevant to be added to the contract. We
train binary classifiers for relevance prediction for
each of the target clause types. Given an input
contract, we obtain its CONTRACTBERT repre-
sentation as shown in Figure 1. Since the number
of tokens in the contracts are usually very large
(≫512), we obtain the contextual representations
of each of the clauses present and average their
[CLS] embeddings to obtain the contract represen-
tation ct_rep. This representation is fed as input to
a binary classifier which is a small fully-connected
neural network that is trained using binary cross
entropy loss. We use a probability score of over
0.5 as a positive prediction, i.e., the target clause
type is relevant to the input contract.
2.2 Clause Content Recommendation
Once a target clause type is predicted as relevant,
the next stage is to recommend clause content cor-
responding to the given type for the contract. We
model this as a sequence-to-sequence generation
task, where the the input includes the given contract
and clause label, and the output contains relevant
clause content that may be added to the contract.
We start with a transformer-based encoder-decoder
architecture (Vaswani et al., 2017), follow (Liu
and Lapata, 2019) and initialize our encoder with
CONTRACTBERT. We then train the transformer
decoder for generating clause content. As men-
tioned above, the inputs for the encoder comprise
of a contract and a target clause type.
We calculate the representations of all possible
clauses belonging to the given type in the dataset
using CONTRACTBERT, and their [CLS] token’s
embeddings are averaged, to obtain a target clause
type representation trgt_cls_rep.This trgt_cls_rep
and the contract representation ct_rep are averaged
to obtain the encoding of the given contract and
target clause type, which is used as input to the de-
coder. Note that since CONTRACTBERT is already
pre-trained on the contracts, we do not need to train
the encoder again for clause generation. Given the
average of the contract and target clause type repre-
sentation as input, the decoder is trained to generate
the appropriate clause belonging to the target type
which might be relevant to the contract. Note that
our generation method provides a single clause as
recommendation. On the other hand, with retrieval-
based methods, we can obtain multiple suggestions
for a given clause type using similarity measures.
3 Experiments and Evaluation
We evaluate three methods for clause type rele-
vance prediction + clause recommendation: (1)
Binary classification + clause generation, which
is our proposed approach; (2) Collaborating filter-
ing + similarity-based retrieval; and (3) Document
similarity + similarity-based retrieval.
Collaborating filtering (CF) + similarity-based
retrieval. Clause type relevance prediction can be
seen as an item-item based CF task (Linden et al.,
2003) with contracts as users and clause types as
items. We construct a contract-clause type matrix,
equivalent to the user-item matrix. If contract u
contains clause type i, the cell (u, i) gets the value
1, otherwise 0. We then compute the similarity
between all the clause type pairs (i, j), using an
adjusted cosine similarity, given by,
sim(i, j) =
U
u (r(u,i)−
¯
ru)(r(u,j)−
¯
rj )
U
u r2
(u,i)
U
u r2
(u,j)
(1)
We obtain the item similarity matrix using this co-
sine score, and use it to predict if a target clause
type t is relevant to a given contract. We compute
the score for t using the weighted sum of the score
of the other similar clause types, given by,
score(u, t) =
I
j sim(t, j)(r(u,j)−
¯
rj )
I
j sim(t, j) + ¯ rt
(2)
If t gets a high score and is not already present
in the contract, it is recommended. We experiment
with multiple thresholds above which a clause type
may be recommended.
Given a clause library containing all possi-
ble clause types and their corresponding clauses,
clause content recommendation can be seen as a
similarity-based retrieval task. For a given con-
tract and a target clause type t, we use ct_rep and
trgt_cls_rep, and find cosine similarities with each
of the clauses belonging to t to find the most similar
clauses that may be relevant to the given contract.
We do so by computing the similarity of either (i)
ct_rep or (ii) (ct_rep + trgt_cls_rep)/2, with indi-
vidual clause representations.
Document similarity + similarity-based re-
trieval. This is based on using similar documents
to determine if a target clause type t can be rec-
ommended for a given contract. The hypothesis is
that similar contracts tend to have similar clause
types. To find similar documents, we compute
cosine similarities between the given contract’s rep-
resentations ct_rep with those of all the contracts
in the (training) dataset to identify the top k similar
contracts. If t is present in any of the k similar con-
tracts and is not present in the given contract, it is
recommended as a relevant clause type to be added
CLAUSE
TYPE
METHOD PREC. REC. ACC. F1
Governing CF-based 0.5889 0.8166 0.6243 0.6843
Laws Doc sim-based 0.7882 0.6225 0.7276 0.6957
Binary classification 0.6898 0.7535 0.7082 0.7203
Severability CF-based 0.6396 0.9091 0.6987 0.7509
Doc sim-based 0.7156 0.8182 0.7467 0.7635
Binary classification 0.7654 0.8042 0.7790 0.7843
Notices CF-based 0.5533 0.8810 0.5885 0.6797
Doc sim-based 0.7825 0.7257 0.7640 0.7530
Binary classification 0.6850 0.7605 0.7079 0.7208
Counterparts CF-based 0.6133 0.8899 0.6657 0.7262
Doc sim-based 0.7156 0.8182 0.7467 0.7635
Binary classification 0.7784 0.8259 0.7961 0.8014
Entire CF-based 0.6197 0.8173 0.6591 0.7049
Agreements Doc sim-based 0.9006 0.6623 0.7953 0.7633
Binary classification 0.7480 0.8158 0.7713 0.7804
Table 1: Clause type relevance prediction results.
to the contract. We experiment with k ∈{1, 5}.
Similarity-based retrieval for clause content recom-
mendation is same as above.
Metrics. We evaluate the performance of clause
type relevance prediction using precision, recall,
accuracy and F1-score metrics, and that of the
clause content recommendation using ROUGE
(Lin, 2004) score.
Data. We use the LEDGAR dataset introduced
by Tuggener et al. (2020). It contains contracts
from the U.S. Securities and Exchange Commis-
sion (SEC) filings website, and includes material
contracts (Exhibit-10), such as shareholder agree-
ments, employment agreements, etc. The dataset
contains 12,608 clause types and 846,274 clauses
from around 60,000 contracts. Further details on
the dataset are provided in the appendix.
Since this dataset can not be used for our work
readily, we preprocess it to create proxy datasets
for clause type relevance prediction and clause rec-
ommendation tasks. For the former, for a target
clause type t, we consider the labels relevant and
not relevant for binary classification. For relevant
class, we obtain contracts that contain a clause cor-
responding to t, and remove this clause; given such
a contract as input in which t is not present, the
classifier is trained to predict t as relevant to be
added to the contract. For the not relevant class,
we randomly sample an equal number of contracts
that do not contain t in them. For recommenda-
tion, we use the contracts that contain t (i.e., the
relevant class contracts); the inputs consist of the
contract with the specific clause removed and t,
and the output is the clause that is removed. For
both the tasks, we partition these proxy datasets
into train (60%), validation (20%) and test (20%)
sets. These ground truth labels ({relevant, not rel-
CLAUSE TYPE METHOD R-1 R-2 R-L
Governing Sim-based (w/o cls_rep) 0.441 0.213 0.327
Laws Sim-based (with cls_rep) 0.499 0.280 0.399
Generation-based 0.567 0.395 0.506
Severability Sim-based (w/o cls_rep) 0.419 0.142 0.269
Sim-based (with cls_rep) 0.444 0.155 0.288
Generation-based 0.521 0.264 0.432
Notices Sim-based (w/o cls_rep) 0.341 0.085 0.207
Sim-based (with cls_rep) 0.430 0.144 0.309
Generation-based 0.514 0.271 0.422
Counterparts Sim-based (w/o cls_rep) 0.466 0.214 0.406
Sim-based (with cls_rep) 0.530 0.279 0.474
Generation-based 0.666 0.495 0.667
Entire Sim-based (w/o cls_rep) 0.433 0.183 0.306
Agreements Sim-based (with cls_rep) 0.474 0.201 0.331
Generation-based 0.535 0.312 0.485
Table 2: Clause content recommendation results.
evant} for the first task and the clause content for
the second task) that we removed are used for eval-
uation. The implementation details are provided in
appendix.
4 Results and Discussion
Table 1 summarizes the results of the three methods
(CF-based, document similarity-based and binary
classification) for the clause type relevance predic-
tion task. For the tasks, we report results on the
thresholds, k and learning rate which gave best re-
sults on the validation set (the ablation results are
reported in the appendix).
The CF-based method gives the best recall val-
ues for all the clause types, while the precision,
accuracy and F1 scores are worse compared to
the other two methods. This method does not in-
corporate any contextual information of the con-
tract clause content and relies only on the presence
or absence of clause types to predict if a target
type is relevant, thus resulting in high recall and
low precision and F1 scores. While the results of
document similarity-based and classification meth-
ods are comparable, both have merits and demer-
its. While the document similarity-based method
is simpler and more extensible than classification
which requires training a new classifier for each
new clause type, the former requires a large collec-
tion of possible contracts to obtain decent results
(particularly the recall values), which may not be
available always. Further, the performance of docu-
ment similarity method is dependent on k. This can
be seen in the lower recall values for the document
similarity method compared to those of classifica-
tion. The storage costs associated with the contract
collection can also become a bottleneck for the doc-
ument similarity method. Also, currently there is
no way to rank the clauses in the similar contracts,
and hence its recommendations cannot be scoped,
while in classification, the probability scores can
be used to rank the clause types for relevance. On
an average, the F1 scores for binary classification
are highest compared to the other methods, while
the accuracies are comparable with the document
similarity method.
Table 2 shows the results for clause content
recommendation using similarity and generation-
based methods. For the sim-based method, we use
the clause with the highest similarity to compute
ROUGE. The scores using only ct_rep are lower
than those with trgt_cls_rep. This is expected as
trgt_cls_rep adds further information on the clause
type for which the appropriate clauses are to be
retrieved. Finally, the generation-based method re-
sults in the best scores for clause recommendation,
thus indicating the usefulness of our proposed ap-
proach for this task. Some qualitative examples
using both the methods are provided in appendix.
For clause content recommendation, we focused
primarily on relevance (in terms of ROUGE). In
general, retrieval-based frameworks, like the one
we proposed, are mostly extractive in nature, and
hence might be perceived as “safer” (or factual) to
avoid any noise and vocabulary change in clauses
that may be incorporated by generation methods,
particularly in domains like legal. However, they
can also end up retrieving clauses irrelevant to the
contract context at times, as we note from their
lower ROUGE scores, as retrieval is based on sim-
ilarity heuristics which may not always capture
relevance, while generation is trained to generate
the specific missing clause in each contract.
We also notice that generated clauses have lower
linguistic variations in them, i.e., generated clauses
belonging to one type often look alike. However,
this is expected as most clauses look very simi-
lar with only a few linguistic and content varia-
tions. We believe because clauses have this repet-
itive nature, there is a large untapped opportunity
to leverage NLP methods for legal text generation
while accounting for the nuances and factuality in
them, to build more accurate clause recommenda-
tion frameworks. We believe our work can provide
a starting point for future works to build powerful
models to capture the essence of legal text and aid
in authoring them. In the future, we aim to focus
on balancing the relevance and factuality of clauses
recommended by our system.
5 Conclusions
We addressed AI-assisted authoring of con-
tracts via clause recommendation. We proposed
CLAUSEREC pipeline to predict clause types rele-
vant to a contract and generate appropriate content
for them based on the contract content. The results
we get on comparing our approach with similarity-
based heuristics and traditional filtering-based tech-
niques are promising, indicating the viability of AI
solutions to automate tasks for legal domain. Ef-
forts in generating long contracts are still in their
infancy and we hope our work can pave way for
more research in this area.
References
Chandra Bhagavatula, Sergey Feldman, Russell Power,
and Waleed Ammar. 2018. Content-based citation
recommendation. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
238–251, New Orleans, Louisiana. Association for
Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
G. Linden, B. Smith, and J. York. 2003. Amazon.com
recommendations: item-to-item collaborative filter-
ing. IEEE Internet Computing, 7(1):76–80.
Yang Liu and Mirella Lapata. 2019. Text summariza-
tion with pretrained encoders.
Itzik Malkiel, Oren Barkan, Avi Caciularu, Noam
Razin, Ori Katz, and Noam Koenigstein. 2020. Re-
coBERT: A catalog language model for text-based
recommendations. In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages
1704–1714, Online. Association for Computational
Linguistics.
Dan Simonson, Daniel Broderick, and Jonathan Herr.
2019. The extent of repetition in contract language.
In Proceedings of the Natural Legal Language Pro-
cessing Workshop 2019, pages 21–30, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.
Figure 4: Some clause types in the LEDGAR dataset.
Don Tuggener, Pius von Däniken, Thomas Peetz, and
Mark Cieliebak. 2020. LEDGAR: A large-scale
multi-label corpus for text classification of legal pro-
visions in contracts. In Proceedings of the 12th Lan-
guage Resources and Evaluation Conference, pages
1235–1241, Marseille, France. European Language
Resources Association.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need.
Tian Wang and Yuyangzi Fu. 2020. Item-based col-
laborative filtering with BERT. In Proceedings of
The 3rd Workshop on e-Commerce and NLP, pages
54–58, Seattle, WA, USA. Association for Computa-
tional Linguistics.
Appendix
A Data
Figure 4 shows some of the clause types present in
the LEDGAR dataset.
B Implementation Details
To train CONTRACTBERT, we crawl and use a
larger collection of 250k contracts and train it till
the losses converge.
B.1 Binary Classifiers
We use a small 7-layered fully connected neural
network with ReLU activation and dropout of 0.3
as binary classifiers. The input is 768 dimensional
contract representation and output is a probability
score between [0, 1]. We use a batch-size of 64 and
train them for 5000 epochs. We experiment with
4 learning rates: [1e−5, 5e−6, 1e−6, 5e−7].
Adam optimizer is used with Binary Cross Entropy
Loss as criterion. The model with highest accu-
racy on validation set is stored and the results are
reported on a held out test set. The training takes
around 150 minutes for each clause type.
For document similarity method, we experi-
mented with k = [1, 5] and for CF based method,
we evaluated F-scores and accuracies for different
threshold values and report the best results we got.
Clause Label k-value threshold learning_rate
Governing Law 1 0.27 5e−07
Counterparts 2 0.18 1e−06
Notices 2 0.15 5e−06
Entire Agreements 1 0.20 1e−05
Severability 3 0.13 1e−06
Table 3: Implementation Details for Clause Type Pre-
diction
Table 3 summarizes the corresponding k values,
thresholds and learningrates corresponding to
the best results.
B.2 Transformer Decoder
The clause text is pre processed by removing punc-
tuation, single letter words, and multiple spaces
then using nltk’s word tokenizer 1 to tokenize the
clause text. We keep the maximum generation
length to be 400 including <SOS> and <EOS> to-
kens. All the clauses with more than 398 tokens
are discarded. The vocabulary is 7185 token long
which is the output dimension. We use 3 decoder
layers. The hidden dimension is 768 i.e., the length
of input embedding. A dropout of 0.1 is used. A
constant learning rate of 1e−05 is used with a
batch size of 16 and the training takes place for 300
epochs. Validation split of 0.2 is used. The results
are reported on a held out test.
C Qualitative Results
Table 4 shows the qualitative results for a few
clause types comparing the similarity-based re-
trieval with generation-based methods. The
ROUGE-1 F-scores are mentioned in the brack-
ets to compare the results quantitatively as well.
1https://www.nltk.org/
CLAUSE TYPE CLAUSE
Governing Laws
Original Sim-based Generated This agreement and the obligations of the parties here under shall be governed by and construed and enforced in
accordance with the substantive and procedural laws of the state of delaware without regard to rules on choice of law.
This agreement shall be governed by and construed in accordance with the laws of the state of illinois without giving
effect to the principles of conflicts of law rules the parties unconditionally and irrevocably consent to the exclusive
jurisdiction of the courts located in the state of illinois and waive any objection with respect thereto for the purpose of
any action suit or proceeding arising out of or relating to this agreement or the transactions contemplated hereby. (R1:
0.456)
This agreement shall be governed by and construed in accordance with the laws of the state of delaware without regard
to the conflicts of law principles thereof. (R1: 0.718)
Original Notices
Sim-based Generated Any notices required or permitted to be given under this agreement shall be sufficient if in writing and if personally
delivered or when sent by first class certified or registered mail postage prepaid return receipt requested in the case
of the executive to his residence address as set forth in the books and records of the company and in the case of the
company to the address of its principal place of business to such person or at such other address with respect to each
party as such party shall notify the other in writing.
Any notice required or permitted by this agreement shall be in writing and shall be delivered as follows with notice
deemed given as indicated by personal delivery when delivered personally ii by overnight courier upon written verifica-
tion of receipt iii by telecopy or facsimile transmission upon acknowledgment of receipt of electronic transmission or
iv by certified or registered mail return receipt requested upon verification of receipt notice shall be sent to executive at
the address listed on the company personnel records and to the company at its principal place of business or such other
address as either party may specify in writing. (R1: 0.588)
Any notice required or permitted to be given under this agreement shall be sufficient if in writing and if sent by
registered or certified mail return receipt requested to executive at the last address the executive has filed in writing
with the company or in the case of the company to its main offices attention of the board. (R1: 0.640)
Original Severability
Sim-based Generated If any term provision covenant or restriction of this agreement is held by court of competent jurisdiction to be invalid
illegal void or unenforceable the remainder of the terms provisions covenants and restrictions set forth herein shall
remain in full force and effect and shall in no way be affected impaired or invalidated and the parties hereto shall use
their commercially reasonable efforts to find and employ an alternative means to achieve the same or substantially
the same result as that contemplated bit/s]y such term provision covenant or restriction it is hereby stipulated and
declared to be the intention of the parties that they would have executed the remaining terms provisions covenants and
restrictions without including any of such that may be hereafter declared invalid illegal void or unenforceable.
Each party hereto agrees that should any court or other competent authority hold any provision of this agreement or part
of this agreement to be null void or unenforceablen the or order any party to take any action inconsistent herewith or
not to take an action consistent with the terms of or required by this agreement the validity legality and or enforceability
of the remaining provisions and obligations contained or set forth in this agreement shall not in any way be affected or
impaired unless the foregoing inconsistent action or the failure to take an action constitutes material breach of this
agreement or makes this agreement impossible to perform in which case this agreement shall terminate. (R1: 0.396)
If any provision of this agreement is held to be invalid or unenforceable under present or future laws effective during
the term hereof the term hereof the term or provisiagenton shall be fully severable and this agreement shall be construed
and enforced as if such illegal invalid or unenforceable term or provision had never comprised part hereohim hf and
the remaining terms hereof shall remain in full force and effect and shall not be affected by the illegal invalid or
unenforceable term or provision or by the parties hereto shall negotiate in good faith substitute valid and enforceable
provision that most nearly reflects the original intent of the parties and all other provisions hereof shall remain in full
force and effect and shall be enforceable. (R1: 0.504)
Original Counterparts
Sim-based Generated This agreement may be executed in one or more counterparts each of which will constitute an original but all of which
when taken together will constitute but one contract and will become effective as provided in section hereof delivery of
an executed counterpart to this agreement by facsimile or other electronic transmission will be as effective as delivery
of manually signed original.
This agreement may be executed by the parties hereto in separate counterparts each of which when so executed and
delivered shall be an original but all such counterparts shall together constitute one and the same agreement and all
signatures need not appear on any one counterpart executed copies of the signature pages of this agreement sent by
facsimile or transmitted electronically in portable document format or any similar format shall be treated as originals
fully binding and with full legal force and effect and the parties waive any rights they may have to object to such
treatment. (R1: 0.427)
This agreement may be executed in any number of counterparts and by different parties hereto in separate counterparts
each of which when so executed shall be deemed to be an original and all of which taken together shall constitute
one and the same agreement delivery of an executed counterpart of signature page to this agreement by facsimile or
other electronic imaging means shall be effective as delivery of manually executed counterpart of this agreement. (R1:
0.603)
Original Entire
Agreements Sim-based Generated This agreement constitutes the entire agreement of the signing parties with respect to the subject matter hereof and
supersedes all other understandings oral or written with respect to the subject matter hereof there are no oral or implied
obligations of the control agent or the other lenders to any third party in connection with this agreement.
This agreement consisting of sections through with schedules and the technology license agreement which is expressly
incorporated by reference herein constitutes the entire understanding between the parties concerning the subject matter
hereof and supersedes all prior discussions agreements and representations whether oral or written this agreement may
be amended altered or modified only by an instrument in writing duly executed by the authorized representations of
both parties. (R1: 0.435)
This agreement and the other transaction documents constitute the entire agreement among the parties hereto with
respect to the subject matter hereof and thereof and supersede all other prior agreements and understandings both
written and oral among the parties or any of them with respect to the subject matter hereof. (R1: 0.626)
Table 4: Qualitative comparison of retrieved and generated clauses

</paper>
<original_plan>
Project Path: contractrec

Source Tree:

contractrec
├── all.txt
├── complete-code.py
├── collaborative-filtering.py
├── 2110.15794v1 (2).pdf
├── complete-evaluation.py
├── implementation-manual.md
├── contract-bert-pretraining.py
├── missing-components.py
├── clause-clustering.py
└── config-template.json


/Users/arthrod/temp/contractrec/all.txt:

txt
[i] Token count: 11035, Model info: ChatGPT models, text-embedding-ada-002
[✓] Copied to clipboard successfully.


/Users/arthrod/temp/contractrec/complete-code.py:

py
[i] Token count: 13338, Model info: ChatGPT models, text-embedding-ada-002
[✓] Copied to clipboard successfully.


/Users/arthrod/temp/contractrec/collaborative-filtering.py:

py
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from typing import Dict, List, Tuple

class ContractClauseRecommender:
    """
    Collaborative Filtering based recommender for contract clauses
    Implementation of the adjusted cosine similarity approach from the paper
    """
    
    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
        self.contract_clause_matrix = None
        self.similarity_matrix = None
        self.clause_means = None
        self.contract_means = None
        self.clause_index = None
        self.contract_index = None

    def create_contract_clause_matrix(
        self,
        contracts: List[str],
        clause_types: List[str],
        clause_presence: List[Tuple[str, str]]
    ) -> csr_matrix:
        """
        Create the contract-clause type matrix
        
        Args:
            contracts: List of contract IDs
            clause_types: List of clause types
            clause_presence: List of (contract_id, clause_type) tuples indicating presence
        
        Returns:
            Sparse matrix of contract-clause relationships
        """
        # Create indices for contracts and clauses
        self.contract_index = {c: i for i, c in enumerate(contracts)}
        self.clause_index = {c: i for i, c in enumerate(clause_types)}
        
        # Create sparse matrix
        rows = [self.contract_index[c] for c, _ in clause_presence]
        cols = [self.clause_index[t] for _, t in clause_presence]
        data = np.ones(len(rows))
        
        return csr_matrix(
            (data, (rows, cols)),
            shape=(len(contracts), len(clause_types))
        )

    def compute_adjusted_cosine_similarity(self) -> np.ndarray:
        """
        Compute adjusted cosine similarity between clause types
        Following the formula from the paper:
        sim(i,j) = Σu(r(u,i) - r̄u)(r(u,j) - r̄j) / √(Σu r²(u,i))(Σu r²(u,j))
        """
        # Get dense matrix for calculations
        matrix = self.contract_clause_matrix.toarray()
        
        # Calculate means
        self.contract_means = matrix.mean(axis=1)
        self.clause_means = matrix.mean(axis=0)
        
        # Adjust ratings by subtracting means
        adjusted_matrix = matrix - self.contract_means[:, np.newaxis]
        
        # Compute similarity matrix
        norm = np.sqrt((adjusted_matrix ** 2).sum(axis=0))
        similarity = np.zeros((matrix.shape[1], matrix.shape[1]))
        
        for i in range(matrix.shape[1]):
            for j in range(matrix.shape[1]):
                if i != j:
                    numerator = (adjusted_matrix[:, i] * adjusted_matrix[:, j]).sum()
                    denominator = norm[i] * norm[j]
                    if denominator != 0:
                        similarity[i, j] = numerator / denominator
        
        return similarity

    def compute_recommendation_score(
        self,
        contract_id: str,
        clause_type: str
    ) -> float:
        """
        Compute recommendation score for a clause type
        Following the formula from the paper:
        score(u,t) = Σj(sim(t,j)(r(u,j) - r̄j)) / Σj(sim(t,j)) + r̄t
        """
        contract_idx = self.contract_index[contract_id]
        clause_idx = self.clause_index[clause_type]
        
        # Get contract's ratings and clause similarities
        contract_ratings = self.contract_clause_matrix[contract_idx].toarray().flatten()
        clause_similarities = self.


/Users/arthrod/temp/contractrec/complete-evaluation.py:

py
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from sklearn.manifold import TSNE
from rouge_score import rouge_scorer
import matplotlib.pyplot as plt
import seaborn as sns

class ClauseRECEvaluator:
    """
    Comprehensive evaluation of the ClauseREC system
    Implements all metrics from the paper
    """
    
    def __init__(self, models, test_data):
        self.models = models
        self.test_data = test_data
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'],
            use_stemmer=True
        )

    def evaluate_relevance_prediction(self):
        """
        Evaluate clause type relevance prediction
        Returns precision, recall, accuracy, and F1-score
        """
        predictions = []
        ground_truth = []
        
        for sample in self.test_data['relevance']:
            pred = self.models['classifier'].predict(
                sample['contract'],
                sample['clause_type']
            )
            predictions.append(pred)
            ground_truth.append(sample['label'])
            
        # Calculate all metrics
        precision, recall, f1, _ = precision_recall_fscore_support(
            ground_truth,
            predictions,
            average='binary'
        )
        
        accuracy = accuracy_score(ground_truth, predictions)
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy
        }

    def evaluate_clause_generation(self):
        """
        Evaluate clause content generation using ROUGE metrics
        Returns ROUGE-1, ROUGE-2, and ROUGE-L scores
        """
        rouge_scores = {
            'rouge1': [],
            'rouge2': [],
            'rougeL': []
        }
        
        for sample in self.test_data['generation']:
            generated_clause = self.models['generator'].generate(
                sample['contract'],
                sample['clause_type']
            )
            
            scores = self.rouge_scorer.score(
                sample['target_clause'],
                generated_clause
            )
            
            for metric in rouge_scores:
                rouge_scores[metric].append(
                    scores[metric.lower()].fmeasure
                )
        
        # Calculate average scores
        return {
            metric: np.mean(scores) 
            for metric, scores in rouge_scores.items()
        }

    def analyze_embeddings(self, embeddings, labels, model_name='ContractBERT'):
        """
        Analyze embeddings using t-SNE visualization
        As shown in Figures 2 and 3 of the paper
        """
        # Dimensionality reduction using t-SNE
        tsne = TSNE(
            n_components=2,
            perplexity=30,
            n_iter=1000,
            random_state=42
        )
        
        embeddings_2d = tsne.fit_transform(embeddings)
        
        # Create visualization
        plt.figure(figsize=(12, 8))
        unique_labels = np.unique(labels)
        colors = sns.color_palette('husl', n_colors=len(unique_labels))
        
        for label, color in zip(unique_labels, colors):
            mask = labels == label
            plt.scatter(
                embeddings_2d[mask, 0],
                embeddings_2d[mask, 1],
                c=[color],
                label=label,
                alpha=0.6
            )
        
        plt.title(f'Clause Embeddings using {model_name}')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        return plt

    def compare_models(self):
        """
        Compare BERT and ContractBERT embeddings
        Replicates the comparison shown in Figures 2 and 3
        """
        # Get embeddings from both models
        bert_embeddings = self.get_bert_embeddings()
        contractbert_embeddings = self.get_contractbert_embeddings()
        
        # Visualize both
        bert_plot = self.analyze_embeddings(
            bert_embeddings,
            self.test_data['labels'],
            'BERT'
        )
        bert_plot.savefig('bert_clusters.png')
        
        contractbert_plot = self.analyze_embeddings(
            contractbert_embeddings,
            self.test_data['labels'],
            'ContractBERT'
        )
        contractbert_plot.savefig('contractbert_clusters.png')
        
        # Calculate clustering metrics
        bert_metrics = self.calculate_clustering_metrics(bert_embeddings)
        contractbert_metrics = self.calculate_clustering_metrics(
            contractbert_embeddings
        )
        
        return {
            'BERT': bert_metrics,
            'ContractBERT': contractbert_metrics
        }

    def calculate_clustering_metrics(self, embeddings):
        """
        Calculate metrics to evaluate clustering quality
        """
        from sklearn.metrics import (
            silhouette_score,
            calinski_harabasz_score
        )
        
        labels = self.test_data['labels']
        
        return {
            'silhouette_score': silhouette_score(embeddings, labels),
            'calinski_harabasz_score': calinski_harabasz_score(
                embeddings, labels
            )
        }

    def generate_evaluation_report(self):
        """
        Generate comprehensive evaluation report
        """
        relevance_metrics = self.evaluate_relevance_prediction()
        generation_metrics = self.evaluate_clause_generation()
        clustering_comparison = self.compare_models()
        
        report = {
            'relevance_prediction': relevance_metrics,
            'clause_generation': generation_metrics,
            'embedding_analysis': clustering_comparison
        }
        
        return report

def main():
    """
    Example usage of the evaluator
    """
    # Initialize evaluator
    evaluator = ClauseRECEvaluator(models, test_data)
    
    # Generate evaluation report
    report = evaluator.generate_evaluation_report()
    
    # Print results
    print("\nClause Type Relevance Prediction Results:")
    for metric, value in report['relevance_prediction'].items():
        print(f"{metric}: {value:.4f}")
    
    print("\nClause Generation Results:")
    for metric, value in report['clause_generation'].items():
        print(f"{metric}: {value:.4f}")
    
    print("\nEmbedding Analysis Results:")
    for model, metrics in report['embedding_analysis'].items():
        print(f"\n{model} Clustering Metrics:")
        for metric, value in metrics.items():
            print(f"{metric}: {value:.4f}")

if __name__ == "__main__":
    main()


/Users/arthrod/temp/contractrec/implementation-manual.md:

md
# CLAUSEREC Implementation Manual

## Table of Contents
1. Environment Setup
2. Project Structure
3. Data Preparation
4. Model Implementation
5. Training Pipeline
6. Evaluation
7. Troubleshooting
8. Performance Optimization

## 1. Environment Setup

### Prerequisites
bash
# Create a new conda environment
conda create -n clauserec python=3.8
conda activate clauserec

# Install required packages
pip install torch==1.9.0
pip install transformers==4.11.0
pip install pandas numpy scipy sklearn
pip install spacy
python -m spacy download en_core_web_sm
pip install rouge-score umap-learn
### Directory Structure

clauserec/
├── data/               # Data storage
├── models/            # Model implementations
├── training/          # Training scripts
├── evaluation/        # Evaluation scripts
├── utils/            # Utility functions
└── configs/          # Configuration files
## 2. Project Structure

### Create necessary directories and files:
bash
mkdir -p clauserec/{data,models,training,evaluation,utils,configs}
touch clauserec/requirements.txt
touch clauserec/README.md
### Initialize git repository:
bash
git init
echo "*.pyc" >> .gitignore
echo "__pycache__" >> .gitignore
echo "data/" >> .gitignore
## 3. Data Preparation

### 3.1 LEDGAR Dataset Processing
1. Download LEDGAR dataset from the official source
2. Place it in `data/raw/`
3. Run preprocessing:
python
# Run data preprocessing
python scripts/preprocess_ledgar.py \
    --input_path data/raw/ledgar.json \
    --output_path data/processed/
### 3.2 Create Proxy Datasets
python
# Create training datasets
python scripts/create_proxy_datasets.py \
    --input_path data/processed/ledgar_processed.json \
    --output_dir data/proxy_datasets/ \
    --train_ratio 0.6 \
    --val_ratio 0.2
## 4. Model Implementation

### 4.1 ContractBERT Setup
1. Initialize ContractBERT:
python
from models.contract_bert import ContractBertPreTraining

model = ContractBertPreTraining(config)
2. Configure pre-training tasks:
python
# Configure additional tasks in config.json
{
    "clause_prediction": true,
    "sentence_pair_prediction": true,
    "num_clause_types": 12608,  # As per paper
    "max_seq_length": 512
}
### 4.2 Binary Classifier Implementation
python
from models.binary_classifier import ClauseRelevanceClassifier

classifier = ClauseRelevanceClassifier(
    input_size=768,
    hidden_sizes=[512, 256, 128, 64, 32, 16]
)
### 4.3 Generator Setup
python
from models.generator import ClauseGenerator

generator = ClauseGenerator(config, contract_bert_path)
## 5. Training Pipeline

### 5.1 ContractBERT Pre-training
python
# Run pre-training
python training/train_contractbert.py \
    --data_dir data/processed/ \
    --output_dir models/contractbert/ \
    --num_epochs 3 \
    --batch_size 16 \
    --learning_rate 2e-5
### 5.2 Clause Relevance Training
python
# Train binary classifier
python training/train_classifier.py \
    --data_dir data/proxy_datasets/relevance/ \
    --model_dir models/contractbert/ \
    --output_dir models/classifier/ \
    --num_epochs 5000 \
    --batch_size 64 \
    --learning_rate 1e-5
### 5.3 Generator Training
python
# Train clause generator
python training/train_generator.py \
    --data_dir data/proxy_datasets/recommendation/ \
    --model_dir models/contractbert/ \
    --output_dir models/generator/ \
    --num_epochs 300 \
    --batch_size 8 \
    --learning_rate 1e-4
## 6. Evaluation

### 6.1 Relevance Prediction Evaluation
python
# Evaluate clause relevance prediction
python evaluation/evaluate_relevance.py \
    --test_data data/proxy_datasets/relevance/test/ \
    --model_path models/classifier/ \
    --output_path evaluation/results/relevance/
### 6.2 Clause Generation Evaluation
python
# Evaluate clause generation
python evaluation/evaluate_generation.py \
    --test_data data/proxy_datasets/recommendation/test/ \
    --model_path models/generator/ \
    --output_path evaluation/results/generation/
## 7. Troubleshooting

### Common Issues and Solutions

1. Memory Issues
- Use gradient accumulation for large models
- Reduce batch size
- Use mixed precision training

2. Training Stability
- Monitor loss curves
- Implement gradient clipping
- Use learning rate warmup

3. Performance Issues
- Check for data leakage
- Validate preprocessing steps
- Ensure proper model initialization

## 8. Performance Optimization

### 8.1 Model Optimization
- Use mixed precision training
- Implement gradient checkpointing
- Optimize batch size and learning rate

### 8.2 Data Pipeline Optimization
- Use data caching
- Implement efficient data loading
- Use multiprocessing for preprocessing

### 8.3 Hardware Utilization
- Monitor GPU memory usage
- Use appropriate number of workers
- Optimize CPU/GPU synchronization

## Important Notes

1. Model Checkpointing
python
# Save checkpoints regularly
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}, f'checkpoints/checkpoint_epoch_{epoch}.pt')
2. Logging
python
# Use proper logging
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
3. Configuration Management
python
# Use configuration files
with open('configs/model_config.json', 'r') as f:
    config = json.load(f)
4. Error Handling
python
try:
    # Your code here
except Exception as e:
    logging.error(f"Error occurred: {str(e)}")
    raise
Follow this manual step by step to implement the complete CLAUSEREC system. Make sure to validate each step before proceeding to the next one.


/Users/arthrod/temp/contractrec/contract-bert-pretraining.py:

py
import torch
import torch.nn as nn
from transformers import BertPreTrainedModel, BertModel
from typing import Optional, Tuple, Union
from dataclasses import dataclass

@dataclass
class ContractBertPreTrainingOutput:
    """
    Output type for ContractBERT pre-training
    """
    loss: Optional[torch.FloatTensor] = None
    mlm_loss: Optional[torch.FloatTensor] = None
    nsp_loss: Optional[torch.FloatTensor] = None
    clause_word_loss: Optional[torch.FloatTensor] = None
    clause_sentence_loss: Optional[torch.FloatTensor] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None

class ContractBertPreTraining(BertPreTrainedModel):
    """
    ContractBERT model with additional pre-training tasks:
    1. Masked Language Modeling (from BERT)
    2. Next Sentence Prediction (from BERT)
    3. Clause Word Prediction (new)
    4. Clause Sentence Pair Prediction (new)
    """
    
    def __init__(self, config):
        super().__init__(config)
        self.bert = BertModel(config)
        self.cls = nn.ModuleDict({
            'mlm': nn.Linear(config.hidden_size, config.vocab_size),
            'nsp': nn.Linear(config.hidden_size, 2),
            'clause_word': nn.Linear(config.hidden_size, 1),
            'clause_sentence': nn.Linear(config.hidden_size, 1)
        })
        
        # Loss functions
        self.loss_fcts = {
            'mlm': nn.CrossEntropyLoss(),
            'nsp': nn.CrossEntropyLoss(),
            'clause_word': nn.BCEWithLogitsLoss(),
            'clause_sentence': nn.BCEWithLogitsLoss()
        }
        
        self.init_weights()

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        masked_lm_labels: Optional[torch.Tensor] = None,
        next_sentence_label: Optional[torch.Tensor] = None,
        clause_word_labels: Optional[torch.Tensor] = None,
        clause_sentence_labels: Optional[torch.Tensor] = None,
    ) -> Union[Tuple[torch.Tensor], ContractBertPreTrainingOutput]:
        
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            return_dict=True
        )

        sequence_output = outputs.last_hidden_state
        pooled_output = outputs.pooler_output

        losses = {}
        
        # 1. Masked Language Modeling
        if masked_lm_labels is not None:
            prediction_scores = self.cls['mlm'](sequence_output)
            losses['mlm'] = self.loss_fcts['mlm'](
                prediction_scores.view(-1, self.config.vocab_size),
                masked_lm_labels.view(-1)
            )

        # 2. Next Sentence Prediction
        if next_sentence_label is not None:
            seq_relationship_score = self.cls['nsp'](pooled_output)
            losses['nsp'] = self.loss_fcts['nsp'](
                seq_relationship_score.view(-1, 2),
                next_sentence_label.view(-1)
            )

        # 3. Clause Word Prediction
        if clause_word_labels is not None:
            clause_word_score = self.cls['clause_word'](sequence_output)
            losses['clause_word'] = self.loss_fcts['clause_word'](
                clause_word_score.squeeze(-1),
                clause_word_labels.float()
            )

        # 4. Clause Sentence Pair Prediction
        if clause_sentence_labels is not None:
            clause_sentence_score = self.cls['clause_sentence'](pooled_output)
            losses['clause_sentence'] = self.loss_fcts['clause_sentence'](
                clause_sentence_score.squeeze(-1),
                clause_sentence_labels.float()
            )

        # Calculate total loss
        total_loss = sum(losses.values()) if losses else None

        return ContractBertPreTrainingOutput(
            loss=total_loss,
            mlm_loss=losses.get('mlm'),
            nsp_loss=losses.get('nsp'),
            clause_word_loss=losses.get('clause_word'),
            clause_sentence_loss=losses.get('clause_sentence'),
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions
        )

class ContractClauseDataset(torch.utils.data.Dataset):
    """
    Dataset for ContractBERT pre-training with clause-specific tasks
    """
    def __init__(
        self,
        contracts: list,
        clauses: list,
        tokenizer,
        max_length: int = 512
    ):
        self.contracts = contracts
        self.clauses = clauses
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.contracts)

    def create_clause_word_labels(self, text: str, clause_text: str) -> list:
        """Create binary labels for words belonging to clause"""
        words = text.split()
        clause_words = set(clause_text.split())
        return [1 if word in clause_words else 0 for word in words]

    def get_sentence_pairs(self, text: str) -> list:
        """Get pairs of sentences with labels indicating if from same clause"""
        import nltk
        sentences = nltk.sent_tokenize(text)
        pairs = []
        labels = []
        
        # Create positive and negative pairs
        for i, sent1 in enumerate(sentences):
            # Positive pair (same clause)
            if i < len(sentences) - 1:
                pairs.append((sent1, sentences[i + 1]))
                labels.append(1)
            
            # Negative pair (different clauses)
            if i < len(sentences) - 2:
                pairs.append((sent1, sentences[i + 2]))
                labels.append(0)
        
        return pairs, labels

    def __getitem__(self, idx):
        contract = self.contracts[idx]
        clause = self.clauses[idx]

        # Tokenize main input
        encoding = self.tokenizer(
            contract,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # Create clause word labels
        clause_word_labels = self.create_clause_word_labels(contract, clause)
        
        # Create sentence pair samples
        sentence_pairs, sentence_labels = self.get_sentence_pairs(contract)
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'clause_word_labels': torch.tensor(clause_word_labels),
            'clause_sentence_labels': torch.tensor(sentence_labels)
        }

def pretrain_contractbert(
    model: ContractBertPreTraining,
    train_dataset: ContractClauseDataset,
    val_dataset: ContractClauseDataset,
    batch_size: int = 16,
    num_epochs: int = 3,
    learning_rate: float = 2e-5,
):
    """Pre-train ContractBERT with the additional tasks"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size
    )
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        total_train_loss = 0
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            
            total_train_loss += loss.item()
        
        avg_train_loss = total_train_loss / len(train_loader)
        
        # Validation
        model.eval()
        total_val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                inputs = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**inputs)
                total_val_loss += outputs.loss.item()
        
        avg_val_loss = total_val_loss / len(val_loader)
        
        print(f'Epoch {epoch + 1}:')
        print(f'Average training loss: {avg_train_loss}')
        print(f'Average validation loss: {avg_val_loss}')


/Users/arthrod/temp/contractrec/missing-components.py:

py
# ledgar_dataset.py
class LEDGARDataset:
    """
    Handler for LEDGAR dataset processing
    """
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.contracts = []
        self.clauses = []
        
    def load_data(self):
        """Load LEDGAR dataset"""
        import json
        
        with open(self.data_path, 'r') as f:
            data = json.load(f)
            
        # Process 250k contracts as mentioned in paper
        for contract in data:
            self.contracts.append({
                'id': contract['id'],
                'text': contract['text'],
                'clauses': contract['clauses']
            })
            
            # Extract individual clauses
            for clause in contract['clauses']:
                self.clauses.append({
                    'contract_id': contract['id'],
                    'type': clause['type'],
                    'text': clause['text']
                })
                
    def create_proxy_datasets(self, train_ratio=0.6, val_ratio=0.2):
        """
        Create proxy datasets for training as described in paper
        """
        # Shuffle data
        np.random.shuffle(self.contracts)
        
        # Calculate split indices
        train_idx = int(len(self.contracts) * train_ratio)
        val_idx = int(len(self.contracts) * (train_ratio + val_ratio))
        
        # Split data
        train_contracts = self.contracts[:train_idx]
        val_contracts = self.contracts[train_idx:val_idx]
        test_contracts = self.contracts[val_idx:]
        
        # Create proxy datasets for relevance prediction
        relevance_datasets = self._create_relevance_datasets(
            train_contracts, val_contracts, test_contracts
        )
        
        # Create proxy datasets for recommendation
        recommendation_datasets = self._create_recommendation_datasets(
            train_contracts, val_contracts, test_contracts
        )
        
        return relevance_datasets, recommendation_datasets
    
    def _create_relevance_datasets(self, train, val, test):
        """Create datasets for relevance prediction"""
        def process_split(contracts):
            positive = []
            negative = []
            
            for contract in contracts:
                # Create positive samples by removing clauses
                for clause in contract['clauses']:
                    modified_contract = self._remove_clause(
                        contract, clause['type']
                    )
                    positive.append({
                        'contract': modified_contract,
                        'clause_type': clause['type'],
                        'label': 1
                    })
                
                # Create negative samples
                all_types = set(c['type'] for c in self.clauses)
                present_types = set(c['type'] for c in contract['clauses'])
                absent_types = all_types - present_types
                
                # Sample equal number of negative examples
                neg_samples = np.random.choice(
                    list(absent_types),
                    size=len(contract['clauses'])
                )
                
                for clause_type in neg_samples:
                    negative.append({
                        'contract': contract['text'],
                        'clause_type': clause_type,
                        'label': 0
                    })
            
            return positive + negative
        
        return {
            'train': process_split(train),
            'val': process_split(val),
            'test': process_split(test)
        }
    
    def _create_recommendation_datasets(self, train, val, test):
        """Create datasets for clause recommendation"""
        def process_split(contracts):
            samples = []
            
            for contract in contracts:
                for clause in contract['clauses']:
                    # Remove the clause and use it as target
                    modified_contract = self._remove_clause(
                        contract, clause['type']
                    )
                    
                    samples.append({
                        'contract': modified_contract,
                        'clause_type': clause['type'],
                        'target_clause': clause['text']
                    })
            
            return samples
        
        return {
            'train': process_split(train),
            'val': process_split(val),
            'test': process_split(test)
        }

# evaluation.py
class ClauserecEvaluator:
    """
    Comprehensive evaluation of CLAUSEREC models
    """
    def __init__(
        self,
        clause_predictor,
        clause_generator,
        test_data,
        k_values=[1, 3, 5],
        thresholds=[0.3, 0.5, 0.7]
    ):
        self.clause_predictor = clause_predictor
        self.clause_generator = clause_generator
        self.test_data = test_data
        self.k_values = k_values
        self.thresholds = thresholds
        
    def evaluate_relevance_prediction(self):
        """Evaluate clause type relevance prediction"""
        results = {}
        
        for threshold in self.thresholds:
            predictions = []
            labels = []
            
            for sample in self.test_data['relevance']:
                pred = self.clause_predictor.predict(
                    sample['contract'],
                    sample['clause_type'],
                    threshold=threshold
                )
                predictions.append(pred)
                labels.append(sample['label'])
            
            # Calculate metrics
            metrics = self._calculate_metrics(predictions, labels)
            results[f'threshold_{threshold}'] = metrics
            
        return results
    
    def evaluate_recommendation(self):
        """Evaluate clause recommendation"""
        results = {
            'rouge1': [],
            'rouge2': [],
            'rougeL': []
        }
        
        scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'],
            use_stemmer=True
        )
        
        for sample in self.test_data['recommendation']:
            generated = self.clause_generator.generate(
                sample['contract'],
                sample['clause_type']
            )
            
            scores = scorer.score(sample['target_clause'], generated)
            
            for metric in results:
                results[metric].append(
                    getattr(scores, metric.lower()).fmeasure
                )
        
        return {
            metric: np.mean(scores)
            for metric, scores in results.items()
        }

# train.py
def train_clauserec():
    """Main training script"""
    # Load and process data
    dataset = LEDGARDataset('path/to/ledgar/data')
    dataset.load_data()
    relevance_data, recommendation_data = dataset.create_proxy_datasets()
    
    # Initialize models
    contract_bert = ContractBertPreTraining(config)
    clause_predictor = ClauseRelevanceClassifier(config)
    clause_generator = ClauseGenerator(config)
    
    # Train ContractBERT
    pretrain_contractbert(
        model=contract_bert,
        train_dataset=create_pretraining_dataset(relevance_data['train']),
        val_dataset=create_pretraining_dataset(relevance_data['val']),
        num_epochs=3
    )
    
    # Train relevance predictor
    train_clause_predictor(
        model=clause_predictor,
        train_dataset=create_relevance_dataset(relevance_data['train']),
        val_dataset=create_relevance_dataset(relevance_data['val']),
        num_epochs=5000
    )
    
    # Train generator
    train_clause_generator(
        model=clause_generator,
        train_dataset=create_generation_dataset(recommendation_data['train']),
        val_dataset=create_generation_dataset(recommendation_data['val']),
        num_epochs=300
    )
    
    # Evaluate
    evaluator = ClauserecEvaluator(
        clause_predictor=clause_predictor,
        clause_generator=clause_generator,
        test_data={
            'relevance': relevance_data['test'],
            'recommendation': recommendation_data['test']
        }
    )
    
    relevance_results = evaluator.evaluate_relevance_prediction()
    recommendation_results = evaluator.evaluate_recommendation()
    
    return {
        'relevance': relevance_results,
        'recommendation': recommendation_results
    }


/Users/arthrod/temp/contractrec/clause-clustering.py:

py
import torch
import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import BertTokenizer, BertModel
from torch.utils.data import DataLoader, Dataset
import umap

class ClauseClusteringAnalysis:
    """
    Handles the clustering analysis and comparison between BERT and ContractBERT
    embeddings for contract clauses.
    """
    def __init__(self, base_bert_path='bert-base-uncased', contract_bert_path='./contract_bert'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize models
        self.base_bert = BertModel.from_pretrained(base_bert_path).to(self.device)
        self.contract_bert = BertModel.from_pretrained(contract_bert_path).to(self.device)
        self.tokenizer = BertTokenizer.from_pretrained(base_bert_path)
        
        # Mapping of clause types to colors for visualization
        self.clause_colors = {
            'governing_laws': '#1f77b4',
            'severability': '#ff7f0e',
            'notices': '#2ca02c',
            'counterparts': '#d62728',
            'entire_agreements': '#9467bd'
        }

    def label_clauses(self, texts):
        """
        Label clauses using keyword and pattern matching.
        Args:
            texts (list): List of clause texts
        Returns:
            list: Predicted clause types
        """
        clause_patterns = {
            'governing_laws': [
                r'govern\w+\s+by\s+.{0,50}laws?',
                r'laws?\s+of\s+the\s+state',
                r'jurisdiction'
            ],
            'severability': [
                r'sever\w+',
                r'invalid\w*\s+.{0,30}\s+unenforc\w+',
                r'void\w*\s+.{0,30}\s+unenforc\w+'
            ],
            'notices': [
                r'notice\w*',
                r'notif\w+',
                r'communicate\w*\s+in\s+writing'
            ],
            'counterparts': [
                r'counterpart\w*',
                r'execute\w*\s+.{0,30}\s+separate',
                r'facsimile\s+.{0,30}\s+original'
            ],
            'entire_agreements': [
                r'entire\s+agreement',
                r'complete\s+understanding',
                r'supersede\w*'
            ]
        }
        
        def match_patterns(text, patterns):
            import re
            return any(re.search(pattern, text.lower()) for pattern in patterns)
        
        labels = []
        for text in texts:
            matched_type = None
            for clause_type, patterns in clause_patterns.items():
                if match_patterns(text, patterns):
                    matched_type = clause_type
                    break
            labels.append(matched_type or 'other')
            
        return labels

    def get_embeddings(self, texts, model):
        """
        Get embeddings for texts using specified model.
        """
        embeddings = []
        
        model.eval()
        with torch.no_grad():
            for text in texts:
                # Tokenize and prepare input
                inputs = self.tokenizer(
                    text,
                    return_tensors='pt',
                    padding=True,
                    truncation=True,
                    max_length=512
                ).to(self.device)
                
                # Get model outputs
                outputs = model(**inputs)
                
                # Use CLS token embedding
                cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(cls_embedding[0])
                
        return np.array(embeddings)

    def reduce_dimensions(self, embeddings, method='tsne'):
        """
        Reduce embedding dimensions for visualization.
        """
        if method == 'tsne':
            reducer = TSNE(n_components=2, random_state=42)
        elif method == 'umap':
            reducer = umap.UMAP(random_state=42)
        else:
            raise ValueError(f"Unknown reduction method: {method}")
            
        return reducer.fit_transform(embeddings)

    def visualize_clusters(self, embeddings_2d, labels, model_name, save_path=None):
        """
        Create scatter plot of clause embeddings.
        """
        plt.figure(figsize=(12, 8))
        
        # Plot points for each clause type
        for clause_type in self.clause_colors:
            mask = np.array(labels) == clause_type
            plt.scatter(
                embeddings_2d[mask, 0],
                embeddings_2d[mask, 1],
                c=self.clause_colors[clause_type],
                label=clause_type,
                alpha=0.6
            )
        
        plt.title(f'Clause Clustering using {model_name}')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path)
        plt.show()

    def compare_models(self, texts):
        """
        Compare clustering results between BERT and ContractBERT.
        """
        # Get labels for clauses
        labels = self.label_clauses(texts)
        
        # Get embeddings from both models
        bert_embeddings = self.get_embeddings(texts, self.base_bert)
        contract_bert_embeddings = self.get_embeddings(texts, self.contract_bert)
        
        # Reduce dimensions for visualization
        bert_2d = self.reduce_dimensions(bert_embeddings)
        contract_bert_2d = self.reduce_dimensions(contract_bert_embeddings)
        
        # Visualize both
        self.visualize_clusters(bert_2d, labels, 'BERT', 'bert_clusters.png')
        self.visualize_clusters(contract_bert_2d, labels, 'ContractBERT', 'contractbert_clusters.png')
        
        # Calculate clustering metrics
        return self.calculate_clustering_metrics(
            bert_embeddings, contract_bert_embeddings, labels
        )

    def calculate_clustering_metrics(self, bert_emb, contract_bert_emb, labels):
        """
        Calculate metrics to compare clustering quality.
        """
        from sklearn.metrics import silhouette_score, calinski_harabasz_score
        
        def get_metrics(embeddings):
            # Convert labels to numeric
            label_encoder = {label: i for i, label in enumerate(set(labels))}
            numeric_labels = [label_encoder[label] for label in labels]
            
            # Calculate clustering metrics
            silhouette = silhouette_score(embeddings, numeric_labels)
            calinski = calinski_harabasz_score(embeddings, numeric_labels)
            
            return {
                'silhouette_score': silhouette,
                'calinski_harabasz_score': calinski
            }
        
        bert_metrics = get_metrics(bert_emb)
        contract_bert_metrics = get_metrics(contract_bert_emb)
        
        return {
            'BERT': bert_metrics,
            'ContractBERT': contract_bert_metrics
        }

# Usage example
def main():
    # Load your contract data
    # This is a placeholder - replace with actual data loading
    contract_data = pd.read_csv('contract_clauses.csv')
    texts = contract_data['clause_text'].tolist()
    
    # Initialize analysis
    analyzer = ClauseClusteringAnalysis()
    
    # Perform comparison
    metrics = analyzer.compare_models(texts)
    
    # Print metrics
    print("\nClustering Metrics Comparison:")
    print("\nBERT metrics:")
    print(metrics['BERT'])
    print("\nContractBERT metrics:")
    print(metrics['ContractBERT'])

if __name__ == "__main__":
    main()


/Users/arthrod/temp/contractrec/config-template.json:

json
{
    "data_config": {
        "ledgar_path": "data/raw/ledgar.json",
        "processed_data_path": "data/processed/",
        "proxy_datasets_path": "data/proxy_datasets/",
        "train_ratio": 0.6,
        "val_ratio": 0.2,
        "test_ratio": 0.2,
        "max_seq_length": 512,
        "min_clause_length": 20,
        "max_clause_length": 400
    },
    
    "model_config": {
        "contractbert": {
            "pretrained_model": "bert-base-uncased",
            "num_hidden_layers": 12,
            "hidden_size": 768,
            "num_attention_heads": 12,
            "intermediate_size": 3072,
            "hidden_dropout_prob": 0.1,
            "attention_probs_dropout_prob": 0.1,
            "max_position_embeddings": 512,
            "num_labels": 2,
            "num_clause_types": 12608
        },
        
        "binary_classifier": {
            "input_size": 768,
            "hidden_sizes": [512, 256, 128, 64, 32, 16],
            "dropout_prob": 0.3,
            "activation": "relu"
        },
        
        "generator": {
            "num_decoder_layers": 6,
            "nhead": 8,
            "dim_feedforward": 2048,
            "dropout": 0.1,
            "activation": "gelu",
            "max_length": 400,
            "num_beams": 4,
            "no_repeat_ngram_size": 3
        }
    },
    
    "training_config": {
        "contractbert": {
            "num_epochs": 3,
            "batch_size": 16,
            "learning_rate": 2e-5,
            "warmup_steps": 10000,
            "gradient_accumulation_steps": 1,
            "max_grad_norm": 1.0,
            "weight_decay": 0.01
        },
        
        "binary_classifier": {
            "num_epochs": 5000,
            "batch_size": 64,
            "learning_rate": 1e-5,
            "early_stopping_patience": 5
        },
        
        "generator": {
            "num_epochs": 300,
            "batch_size": 8,
            "learning_rate": 1e-4,
            "teacher_forcing_ratio": 0.5
        }
    },
    
    "evaluation_config": {
        "relevance": {
            "thresholds": [0.3, 0.5, 0.7],
            "k_values": [1, 3, 5]
        },
        "generation": {
            "metrics": ["rouge1", "rouge2", "rougeL"],
            "use_stemmer": true
        }
    }
}


Thought about CLAUSEREC system evaluation for 14 seconds
Below is some constructive feedback based on how your code and folder structure compare to the CLAUSEREC pipeline described in the paper “CLAUSEREC: A Clause Recommendation Framework for AI-aided Contract Authoring.” I’ve highlighted key gaps and points of divergence that, if addressed, would bring your replication closer to the original work.

1. Project Structure & Code Organization
Multiple “entry point” scripts
You have several separate scripts (complete-code.py, collaborative-filtering.py, missing-components.py, etc.) referencing each other. This spreads out the code for (1) data loading, (2) model training, and (3) evaluation in a way that makes the final pipeline hard to follow.

Suggestion: Provide a single driver script/notebook (e.g., train.py or main.py) that:
Loads/preprocesses data.
Trains the model (both ContractBERT and the smaller classifiers).
Runs your evaluation metrics (clause type relevance + clause generation).
This will let you replicate the paper’s tables and results more easily in one shot.
Missing definitions or references

In complete-evaluation.py and missing-components.py, there are references to a ClauseRelevanceClassifier and a ClauseGenerator class, but the actual implementations are not fully visible in your repo (or appear only as placeholders).
Similarly, references to models in ClauseRECEvaluator (e.g., self.models['classifier'], self.models['generator']) assume a certain code structure that you haven’t fully provided.
Suggestion: Either provide the actual implementations or remove references to them so the code runs end-to-end. You can see in the original paper that the classification approach and the generation approach are both tied strongly to the ContractBERT embeddings. Make sure everything is defined in code (even if minimally).
Integration of CF + Document Similarity + Classification + Generation
In the paper, the authors systematically compare:

CF-based: Item-to-item collaborative filtering with adjusted cosine similarity.
Document-sim-based: Using nearest-neighbor approach with k-similar contracts.
Binary classification: A trainable classifier using contract embeddings.
Clause generation vs. retrieval.
Your code has partial coverage of these steps but does not unify them in a single pipeline.
Suggestion: Provide a top-level function or script that:
Runs CF for each contract to predict clause relevance, calculates metrics (precision, recall, etc.).
Runs doc-sim for each contract with various k to predict relevance, calculates the same metrics.
Trains and tests a binary classifier to replicate the “Binary classification + generation” results.
Compares the final metrics (Tables 1 and 2 from the paper).
2. Data Handling & Preprocessing
Long Contract Handling
In the paper, the authors mention “the number of tokens in contracts are usually very large,” and they handle this by taking an average of the [CLS] embeddings of each clause in the contract.

In your code (contract-bert-pretraining.py, missing-components.py), it’s not fully clear whether you are chunking each contract into clauses, or if you simply truncate at 512 tokens, or if you do something else.
Suggestion: Mirror the approach in the paper more explicitly:
Split the contract into clauses.
Encode each clause with BERT, store the [CLS], then average those embeddings to get the final contract representation.
This aligns with how the authors overcame the 512-token limit.
Negative Sampling & “Removing Clause” Logic
The paper’s training data creation is subtle: for each contract that does contain a target clause type 
t
t, they remove the text of that clause from the contract to create a “contract that does not contain 
t
t.” Then they train a classifier that should predict 
t
t is relevant.

Your code in LEDGARDataset (within missing-components.py) hints at removing clauses, but it’s not entirely clear if you replicate that logic exactly (particularly for the recommendation proxy dataset).
Suggestion: Ensure your “create_proxy_datasets” logic truly matches the approach:
For “relevant” examples: a contract originally has clause type 
t
t; remove the text of 
t
t. The label is that 
t
t is relevant.
For “non-relevant” examples: a contract that truly never had 
t
t.
3. Collaborative Filtering & Document Similarity
Adjusting the CF threshold
Your collaborative-filtering.py script defines compute_adjusted_cosine_similarity() and a compute_recommendation_score(), but it’s partially incomplete (the code cuts off in the snippet). Even if complete, it’s not integrated into a final “predict” function that chooses which clause types to recommend.

Suggestion: Provide a short “CF Recommender” function:
def recommend_clause_types(contract_id, top_n=5):
    # Uses compute_recommendation_score for each clause type
    # Filters out ones the contract already has
    # Sorts them by the recommendation score
    # Possibly applies threshold if relevant
    # Returns top_n or all above threshold
Document similarity approach
The code references DocumentSimilarity in comments or in docstrings, but there is no top-level function or snippet showing how you pick the top-k neighbor documents.
Suggestion: Similarly, wrap it up in a function:
def recommend_using_doc_similarity(contract_embedding, all_contract_embeddings, k=5):
    # 1. find k nearest neighbors
    # 2. see which clause types appear in those neighbors
    # 3. if they are not in the current contract, recommend them
Ranking & ties
In the paper, the doc-sim method’s performance depends on the choice of 
k
=
1
k=1 or 
k
=
5
k=5. Make sure you replicate that exactly and store the doc-sim approach’s recommended clause types (and compare to ground truth).

4. Clause Generation vs. Retrieval
Generation approach

The code in contract-bert-pretraining.py shows how you extended BERT for additional tasks like “clause_word” and “clause_sentence.” However, the sequence-to-sequence generation described in the paper uses a Transformer encoder–decoder style.
In the paper’s final approach, the encoder is a frozen ContractBERT, and the decoder is a new Transformer. The user’s code references a “ClauseGenerator” but that class is missing from the snippets (the placeholders in missing-components.py or complete-evaluation.py just show how you might call it, not how it’s implemented).
Suggestion: Provide that “ClauseGenerator” code with a dedicated forward pass that takes the (contract representation + clause-type representation) as input for the decoder. This is crucial to replicate the higher ROUGE scores shown in Table 2.
Retrieval approach

In the paper, they run similarity-based retrieval in two ways:
Similarity between 
ct_rep
ct_rep and each candidate clause in the library.
Similarity between 
(
ct_rep
+
trgt_cls_rep
)
/
2
(ct_rep+trgt_cls_rep)/2 and each candidate clause.
In your code, you partially show how to compute embeddings and do “visualization” or “clustering,” but it’s not fully integrated for the top-k retrieval approach.
Suggestion: Show a short snippet (similar to CF) that returns the top 
n
n candidate clauses from a given library for a given contract, sorted by the appropriate similarity score.
Comparison vs. Paper
The paper’s results show that generation-based approach yields higher ROUGE-1, ROUGE-2, ROUGE-L on average than retrieval. In your codebase, make sure you replicate the same metric computations. The complete-evaluation.py does have a evaluate_clause_generation() function referencing ROUGE, so you’re on the right track. You just need to unify it all.

5. Practical Details & Reproducibility
Hyperparameters & Logging
The paper mentions training the classifier for 5000 epochs with a batch size of 64, etc. In your config-template.json, you have placeholders for these values (e.g. binary_classifier.num_epochs=5000). Make sure your actual calls respect that config.

Suggestion: Confirm you are truly training for enough epochs, with the same batch size, and that your learning rates match.
Provide or enable a reproducible random seed, so that your results are stable.
Large-Scale ContractBERT Pretraining
The paper states that they “crawl and use ~250k contracts” to continue pretraining BERT. If you truly want to replicate their ContractBERT, you need a corpus of that scale, plus the code that implements the two extra tasks (clause-word and clause-sentence).

You do have placeholders for these in ContractBertPreTraining, which is good. Just clarify whether you are doing a quick smaller-scale run or replicating the large-scale approach.
Evaluation & Final Metrics

The paper’s main results appear in Table 1 (precision, recall, accuracy, F1) for clause type relevance, and Table 2 (ROUGE-1, ROUGE-2, ROUGE-L) for clause content recommendation.
In your code, it would be helpful to output exactly these same metrics, side by side, for each approach: CF-based, doc-sim-based, and binary classifier + generation.
Right now, complete-evaluation.py mostly lumps them all in “evaluate_relevance_prediction” or “evaluate_clause_generation.” That is good, but be sure you also store or print out the results as a table or dictionary with the same headings.
6. Summary of Recommended Next Steps
Create a single orchestrating script (e.g. main.py) that:

Loads the data (LEDGAR or a subset).
Runs your CF approach, doc-sim approach, and classification approach (trained or with pre-trained ContractBERT).
Evaluates them on your test set using precision, recall, accuracy, F1 for relevance, plus ROUGE for generation.
Prints out the final table(s).
Fully implement and integrate the “ClauseGenerator.”

Show the end-to-end usage of (ct_rep + trgt_cls_rep)/2 as the decoder input.
Evaluate its outputs with ROUGE and compare them to your retrieval approach.
Harmonize references to missing or partial code, so everything runs.

E.g., unify clause_predictor and ClauseRelevanceClassifier; unify or remove leftover references that aren’t actually used.
Include the same hyperparameters (epochs, batch sizes, etc.) as in the paper, or at least note divergences.

E.g., if you do fewer epochs for practical reasons, specify that you expect lower absolute numbers, but the relative ranking of approaches should still hold.
Document thoroughly how to replicate the final results.

This includes how you handle chunking or splitting the contract into sub-clauses (or 512-token splits) when embedding with ContractBERT.
Provide the exact negative sampling and “clause removal” logic so that your proxy dataset creation matches the original approach.
By making these adjustments—especially centralizing the pipeline logic, clarifying your references to (and implementations of) each sub-module (classifier, generator, CF, doc-sim), and verifying that you’re replicating the original paper’s method of turning large contracts into smaller clause embeddings—you will get closer to a faithful re-creation of the CLAUSEREC system.

Good luck refining your codebase! If you unify these steps and hyperparameters, you’ll have a clear, end-to-end replication that matches the paper’s methodology and reported results.




You said:
what about now, whaat am i missing? # main.py
import torch
import logging
from pathlib import Path
from datetime import datetime
import json
import numpy as np
from typing import Dict, List, Tuple, Optional

from src.data.ledgar_dataset import LEDGARDataset
from src.models.contract_bert import ContractBERTModel
from src.models.clause_classifier import ClauseRelevanceClassifier
from src.models.clause_generator import ClauseGenerator
from src.approaches.collaborative_filtering import CFRecommender
from src.approaches.document_similarity import DocSimRecommender
from src.evaluation.metrics import evaluate_relevance, evaluate_generation
from src.utils.contract_utils import split_contract_into_clauses

class CLAUSERECPipeline:
    """
    Main pipeline for CLAUSEREC implementation, following the paper exactly.
    """
    
    def __init__(self, config_path: str):
        """Initialize pipeline with configuration"""
        self.config = self._load_config(config_path)
        self.setup_logging()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize components
        self._initialize_components()
        
    def _load_config(self, config_path: str) -> Dict:
        """Load configuration file"""
        with open(config_path) as f:
            return json.load(f)
            
    def setup_logging(self):
        """Setup logging with timestamp"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'logs/clauserec_{timestamp}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('CLAUSEREC')

    def _initialize_components(self):
        """Initialize all components of the pipeline"""
        # Initialize dataset
        self.dataset = LEDGARDataset(
            data_path=self.config['data']['ledgar_path'],
            max_length=self.config['model']['max_seq_length']
        )
        
        # Initialize ContractBERT
        self.contract_bert = ContractBERTModel(self.config['model'])
        
        # Initialize different approaches
        self.cf_recommender = CFRecommender(self.config['cf'])
        self.doc_sim_recommender = DocSimRecommender(self.config['doc_sim'])
        self.clause_classifier = ClauseRelevanceClassifier(self.config['classifier'])
        self.clause_generator = ClauseGenerator(
            self.config['generator'],
            self.contract_bert
        )

    def prepare_data(self):
        """
        Prepare data following paper's methodology:
        1. Load LEDGAR dataset
        2. Create proxy datasets for both tasks
        3. Handle long contracts by splitting into clauses
        """
        self.logger.info("Preparing datasets...")
        
        # Load and preprocess LEDGAR data
        self.dataset.load_data()
        
        # Create proxy datasets exactly as in paper
        train_data, val_data, test_data = self.dataset.create_proxy_datasets(
            train_ratio=self.config['data']['train_ratio'],
            val_ratio=self.config['data']['val_ratio']
        )
        
        # Process long contracts by splitting into clauses and averaging embeddings
        def process_contract(contract_text: str) -> torch.Tensor:
            clauses = split_contract_into_clauses(contract_text)
            clause_embeddings = []
            
            for clause in clauses:
                # Get [CLS] embedding for each clause
                embedding = self.contract_bert.get_clause_embedding(clause)
                clause_embeddings.append(embedding)
                
            # Average clause embeddings as per paper
            return torch.stack(clause_embeddings).mean(dim=0)
        
        # Process all contracts
        self.processed_contracts = {
            'train': {id_: process_contract(text) for id_, text in train_data.items()},
            'val': {id_: process_contract(text) for id_, text in val_data.items()},
            'test': {id_: process_contract(text) for id_, text in test_data.items()}
        }
        
        return train_data, val_data, test_data

    def train_contract_bert(self):
        """Train ContractBERT with additional tasks"""
        self.logger.info("Training ContractBERT...")
        
        self.contract_bert.train_model(
            train_data=self.processed_contracts['train'],
            val_data=self.processed_contracts['val'],
            num_epochs=self.config['training']['contractbert']['num_epochs'],
            batch_size=self.config['training']['contractbert']['batch_size'],
            learning_rate=self.config['training']['contractbert']['learning_rate']
        )

    def evaluate_all_approaches(self, test_data: Dict):
        """
        Evaluate all approaches as in paper:
        1. CF-based
        2. Document similarity-based
        3. Binary classification + generation
        """
        self.logger.info("Evaluating all approaches...")
        
        results = {
            'relevance_prediction': {},
            'clause_generation': {}
        }
        
        # 1. CF-based approach
        cf_preds = self.cf_recommender.predict_all(test_data)
        results['relevance_prediction']['cf'] = evaluate_relevance(
            cf_preds,
            test_data['labels']
        )
        
        # 2. Document similarity approach
        for k in self.config['doc_sim']['k_values']:
            doc_sim_preds = self.doc_sim_recommender.predict_all(
                test_data,
                k=k
            )
            results['relevance_prediction'][f'doc_sim_k{k}'] = evaluate_relevance(
                doc_sim_preds,
                test_data['labels']
            )
        
        # 3. Binary classification + generation
        classifier_preds = self.clause_classifier.predict_all(test_data)
        results['relevance_prediction']['binary_clf'] = evaluate_relevance(
            classifier_preds,
            test_data['labels']
        )
        
        # Compare generation vs retrieval
        generated_clauses = self.clause_generator.generate_all(test_data)
        retrieved_clauses = self.doc_sim_recommender.retrieve_clauses(test_data)
        
        results['clause_generation']['generation'] = evaluate_generation(
            generated_clauses,
            test_data['target_clauses']
        )
        results['clause_generation']['retrieval'] = evaluate_generation(
            retrieved_clauses,
            test_data['target_clauses']
        )
        
        return results

    def run_pipeline(self):
        """Run complete pipeline"""
        # 1. Prepare data
        train_data, val_data, test_data = self.prepare_data()
        
        # 2. Train ContractBERT
        self.train_contract_bert()
        
        # 3. Train classifiers and generator
        self.clause_classifier.train(
            train_data=train_data,
            val_data=val_data,
            num_epochs=self.config['training']['classifier']['num_epochs']
        )
        
        self.clause_generator.train(
            train_data=train_data,
            val_data=val_data,
            num_epochs=self.config['training']['generator']['num_epochs']
        )
        
        # 4. Evaluate all approaches
        results = self.evaluate_all_approaches(test_data)
        
        # 5. Save and display results
        self._save_results(results)
        self._display_results(results)
        
        return results
    
    def _save_results(self, results: Dict):
        """Save results to file"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = Path('results')
        output_dir.mkdir(exist_ok=True)
        
        with open(output_dir / f'results_{timestamp}.json', 'w') as f:
            json.dump(results, f, indent=2)
    
    def _display_results(self, results: Dict):
        """Display results in same format as paper tables"""
        # Display Table 1: Clause type relevance prediction
        print("\nTable 1: Clause Type Relevance Prediction Results")
        print("=" * 80)
        print(f"{'Method':<15} {'Precision':<10} {'Recall':<10} {'Accuracy':<10} {'F1':<10}")
        print("-" * 80)
        
        for method, metrics in results['relevance_prediction'].items():
            print(
                f"{method:<15} "
                f"{metrics['precision']:<10.4f} "
                f"{metrics['recall']:<10.4f} "
                f"{metrics['accuracy']:<10.4f} "
                f"{metrics['f1']:<10.4f}"
            )
        
        # Display Table 2: Clause content recommendation
        print("\nTable 2: Clause Content Recommendation Results")
        print("=" * 60)
        print(f"{'Method':<15} {'R-1':<10} {'R-2':<10} {'R-L':<10}")
        print("-" * 60)
        
        for method, metrics in results['clause_generation'].items():
            print(
                f"{method:<15} "
                f"{metrics['rouge1']:<10.4f} "
                f"{metrics['rouge2']:<10.4f} "
                f"{metrics['rougeL']:<10.4f}"
            )

if __name__ == "__main__":
    # Set random seed for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    
    # Initialize and run pipeline
    pipeline = CLAUSERECPipeline("configs/clauserec_config.json")
    results = pipeline.run_pipeline()
ChatGPT

Thought about CLAUSEREC pipeline refinement for 13 seconds
Below is a checklist of the most common remaining gaps you’ll want to fill so your main.py orchestrates the entire pipeline exactly as the paper describes, with minimal “missing piece” errors. Most often, the issues center around (1) ensuring that your dataset creation matches what the pipeline expects, (2) adding the internal code for all references you’ve imported (particularly from src/), and (3) making sure each step really returns the fields needed at evaluation time.

1. Verify All Referenced Modules Exist
Your main.py references the following:

from src.data.ledgar_dataset import LEDGARDataset
from src.models.contract_bert import ContractBERTModel
from src.models.clause_classifier import ClauseRelevanceClassifier
from src.models.clause_generator import ClauseGenerator
from src.approaches.collaborative_filtering import CFRecommender
from src.approaches.document_similarity import DocSimRecommender
from src.evaluation.metrics import evaluate_relevance, evaluate_generation
from src.utils.contract_utils import split_contract_into_clauses
Make sure you actually have all these implemented:

LEDGARDataset

Has load_data() and create_proxy_datasets() that return train/val/test partitions as dicts or lists (consistent with your usage).
If create_proxy_datasets() is supposed to produce (train_data, val_data, test_data), each of these needs to be in the format you expect:
Right now, your code calls process_contract(text) for id_, text in train_data.items().
So train_data must be a dict, where each key is an ID, and each value is the entire contract text.
If you need labels or target clauses for your relevance/generation evaluation, you also need them in your train_data, val_data, test_data structures or stored elsewhere.
ContractBERTModel

Must provide:
A train_model(...) method called by self.contract_bert.train_model(...).
A get_clause_embedding(text: str) -> torch.Tensor method called by embedding = self.contract_bert.get_clause_embedding(clause).
If you want to replicate the original paper’s approach (pretraining BERT on extra tasks), this model must do that under the hood.
ClauseRelevanceClassifier

Must have:
A train(...) method (as you call self.clause_classifier.train(train_data=..., val_data=..., ...)).
A predict_all(...) method (as you call self.clause_classifier.predict_all(test_data)).
ClauseGenerator

Must have:
A train(...) method.
A generate_all(...) method (you call generated_clauses = self.clause_generator.generate_all(test_data)).
CFRecommender

Must have:
A predict_all(...) method returning predictions for each test sample.
DocSimRecommender

Must have:
A predict_all(test_data, k=...) for relevance prediction.
A retrieve_clauses(test_data) for clause retrieval used in your generation vs. retrieval comparison.
evaluate_relevance, evaluate_generation

Must handle the way you’re passing predictions (cf_preds, doc_sim_preds, classifier_preds) vs. the ground-truth (test_data['labels'], test_data['target_clauses'], etc.).
split_contract_into_clauses

Must contain the actual logic to chunk a contract text into individual clauses.
In short, confirm those modules exist and do exactly what your pipeline is calling.

2. Confirm Data Structures for Train / Test Splits
2.1. The Return Format of create_proxy_datasets()
In your code snippet:

train_data, val_data, test_data = self.dataset.create_proxy_datasets(
    train_ratio=self.config['data']['train_ratio'],
    val_ratio=self.config['data']['val_ratio']
)
Then you do:

for id_, text in train_data.items():
    embedding = process_contract(text)
Hence train_data must be:

{
  "contract_id_1": "full text of contract 1",
  "contract_id_2": "full text of contract 2",
  ...
}
But later on, in your evaluation code, you do something like:

cf_preds = self.cf_recommender.predict_all(test_data)
evaluate_relevance(cf_preds, test_data['labels'])
This implies that test_data also has a ['labels'] field, or you have a separate structure for labels. If you want to replicate the paper’s approach for relevance classification, you need either:

test_data = {
  'contracts': { contract_id: contract_text, ... },
  'labels': { contract_id: label_for_that_contract, ... },
  'target_clauses': { contract_id: the_clause_text_removed, ... },
  ...
}
Or you keep them in parallel arrays. Either way, your CFRecommender.predict_all and evaluate_relevance must know how to interpret “test_data.”

Double-check your shapes and keys. If predict_all expects something like test_data['contracts'], but your pipeline only has test_data as a dict of contract_text, you’ll get KeyError’s.

2.2. Negative Sampling & “Removed Clause” Logic
The paper’s proxy dataset creation for relevance classification says:

If a contract originally had clause type t, remove that clause from the text → that sample is labeled “t is relevant.”
If a contract never had t, that sample is labeled “not relevant.”
And for generation, they keep that missing clause as the ground truth. So you’ll want something like:

test_data['labels'][contract_id] = 1 or 0
test_data['target_clauses'][contract_id] = "the text of the missing clause"
test_data['contracts'][contract_id] = "contract text after removing clause"
Make sure your code abides by that logic; otherwise you’ll get mismatched predictions (the classifier sees no difference between samples that never had the clause vs. those from which you forcibly removed it).

3. Ensure the Approaches Return the Right Data
3.1. CF & Document Similarity: Predicting Clause Relevance
When you do:

cf_preds = self.cf_recommender.predict_all(test_data)
evaluate_relevance(cf_preds, test_data['labels'])
cf_preds should be a dict: {contract_id: 0_or_1, ...} if you want to compute precision, recall, etc.
Similarly, for doc-sim predict_all(test_data, k=k), you must return that same structure.
3.2. Clause Generation vs. Retrieval
generated_clauses = self.clause_generator.generate_all(test_data)
retrieved_clauses = self.doc_sim_recommender.retrieve_clauses(test_data)
evaluate_generation(generated_clauses, test_data['target_clauses'])
evaluate_generation(retrieved_clauses, test_data['target_clauses'])
generate_all and retrieve_clauses should each return a dict: {contract_id: "clause text string"}, so you can run a ROUGE-based metric against test_data['target_clauses'][contract_id].
4. Double-Check Hyperparameters
The original paper uses certain epochs, batch sizes, LR schedules, etc. Your snippet references:

"training": {
  "contractbert": {
      "num_epochs": ...,
      "batch_size": ...,
      "learning_rate": ...
  },
  "classifier": {
      "num_epochs": ...
  },
  "generator": {
      "num_epochs": ...
  }
}
Just confirm you’re consistent with the paper if you want the same performance. If not, mention which parts deviate from the paper.

5. Summary
You’ve done the biggest part by creating a main driver pipeline that calls prepare_data(), trains ContractBERT, trains the classifier/generator, and evaluates.
The primary “what’s missing” is the body of each src/... module and ensuring the data structure lines up with your calls. In particular, confirm your LEDGARDataset actually produces the right dictionary-based splits, along with labels and target_clauses that your evaluation code demands.
Also confirm that all the references—like predict_all(), retrieve_clauses(), generate_all(), etc.—are actually implemented and return the correct format.
Once you add those details and confirm your dataset format matches the pipeline’s usage, you’ll have a fully integrated replication of the CLAUSEREC paper.




You said:
# src/data/ledgar_dataset.py

import json
import torch
from typing import Dict, List, Tuple
import numpy as np
from pathlib import Path
from transformers import BertTokenizer
import re

class LEDGARDataset:
    """
    LEDGAR dataset handler that creates proper proxy datasets
    following the paper's methodology exactly
    """
    def __init__(
        self,
        data_path: str,
        max_length: int = 512,
        cache_dir: str = "data/cache"
    ):
        self.data_path = Path(data_path)
        self.max_length = max_length
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.contracts = {}
        self.clause_types = set()
        
    def load_data(self) -> None:
        """Load LEDGAR dataset and initialize structures"""
        self.contracts = {}
        cache_file = self.cache_dir / "processed_contracts.json"
        
        if cache_file.exists():
            with open(cache_file) as f:
                self.contracts = json.load(f)
        else:
            with open(self.data_path) as f:
                raw_data = json.load(f)
                
            for contract in raw_data:
                contract_id = contract['id']
                self.contracts[contract_id] = {
                    'text': contract['text'],
                    'clauses': []
                }
                
                # Process each clause
                for clause in contract['clauses']:
                    self.clause_types.add(clause['type'])
                    self.contracts[contract_id]['clauses'].append({
                        'type': clause['type'],
                        'text': clause['text'],
                        'start_idx': contract['text'].find(clause['text']),
                        'end_idx': contract['text'].find(clause['text']) + len(clause['text'])
                    })
            
            # Cache processed data
            with open(cache_file, 'w') as f:
                json.dump(self.contracts, f)
                
        self.clause_types = sorted(list(self.clause_types))

    def create_proxy_datasets(
        self,
        train_ratio: float = 0.6,
        val_ratio: float = 0.2,
        seed: int = 42
    ) -> Tuple[Dict, Dict, Dict]:
        """
        Create proxy datasets exactly as described in the paper:
        1. For relevance: Remove clause from contract, label as relevant
        2. For non-relevance: Use contracts that never had the clause
        """
        np.random.seed(seed)
        contract_ids = list(self.contracts.keys())
        np.random.shuffle(contract_ids)
        
        # Split indices
        n_contracts = len(contract_ids)
        train_idx = int(n_contracts * train_ratio)
        val_idx = int(n_contracts * (train_ratio + val_ratio))
        
        # Create split IDs
        train_ids = contract_ids[:train_idx]
        val_ids = contract_ids[train_idx:val_idx]
        test_ids = contract_ids[val_idx:]
        
        # Initialize data structures for each split
        splits = {}
        for split_name, split_ids in [
            ('train', train_ids),
            ('val', val_ids),
            ('test', test_ids)
        ]:
            splits[split_name] = {
                'contracts': {},
                'labels': {},
                'target_clauses': {},
                'clause_types': {}
            }
            
            for contract_id in split_ids:
                contract = self.contracts[contract_id]
                
                # For each clause type in the contract
                for clause in contract['clauses']:
                    # Create positive example by removing the clause
                    modified_contract = self._remove_clause(
                        contract['text'],
                        clause['start_idx'],
                        clause['end_idx']
                    )
                    
                    example_id = f"{contract_id}_{clause['type']}"
                    splits[split_name]['contracts'][example_id] = modified_contract
                    splits[split_name]['labels'][example_id] = 1
                    splits[split_name]['target_clauses'][example_id] = clause['text']
                    splits[split_name]['clause_types'][example_id] = clause['type']
                    
                # Create negative examples from other clause types
                present_types = {c['type'] for c in contract['clauses']}
                absent_types = self.clause_types - present_types
                
                # Sample equal number of negative examples
                for clause_type in np.random.choice(
                    list(absent_types),
                    size=len(contract['clauses'])
                ):
                    example_id = f"{contract_id}_{clause_type}"
                    splits[split_name]['contracts'][example_id] = contract['text']
                    splits[split_name]['labels'][example_id] = 0
                    splits[split_name]['target_clauses'][example_id] = ""
                    splits[split_name]['clause_types'][example_id] = clause_type
        
        return splits['train'], splits['val'], splits['test']

    def _remove_clause(
        self,
        contract_text: str,
        start_idx: int,
        end_idx: int
    ) -> str:
        """Carefully remove clause while preserving document structure"""
        # Remove the clause text
        modified_text = (
            contract_text[:start_idx].rstrip() +
            " " +
            contract_text[end_idx:].lstrip()
        )
        
        # Clean up any double spaces
        modified_text = re.sub(r'\s+', ' ', modified_text)
        
        return modified_text.strip()

    def get_clause_type_vocab(self) -> Dict[str, int]:
        """Get mapping of clause types to indices"""
        return {
            clause_type: idx
            for idx, clause_type in enumerate(self.clause_types)
        }

    def __len__(self) -> int:
        return len(self.contracts)

if __name__ == "__main__":
    # Test dataset creation
    dataset = LEDGARDataset("data/ledgar/ledgar.json")
    dataset.load_data()
    train_data, val_data, test_data = dataset.create_proxy_datasets()
    
    # Print statistics
    print(f"Number of contracts: {len(dataset)}")
    print(f"Number of clause types: {len(dataset.clause_types)}")
    print("\nSplit sizes:")
    print(f"Train: {len(train_data['contracts'])}")
    print(f"Val: {len(val_data['contracts'])}")
    print(f"Test: {len(test_data['contracts'])}")
    
    # Verify structure
    example_id = next(iter(test_data['contracts']))
    print("\nExample data structure:")
    print(f"Contract text length: {len(test_data['contracts'][example_id])}")
    print(f"Label: {test_data['labels'][example_id]}")
    print(f"Clause type: {test_data['clause_types'][example_id]}")
    if test_data['target_clauses'][example_id]:
        print(f"Target clause length: {len(test_data['target_clauses'][example_id])}")# src/approaches/collaborative_filtering.py

import numpy as np
from scipy.sparse import csr_matrix
from typing import Dict, List, Tuple
import torch

class CFRecommender:
    """
    Collaborative Filtering based recommender implementing the paper's
    adjusted cosine similarity approach
    """
    
    def __init__(self, config: Dict):
        self.threshold = config.get('threshold', 0.5)
        self.contract_clause_matrix = None
        self.similarity_matrix = None
        self.clause_means = None
        self.contract_means = None
        self.clause_index = None
        self.contract_index = None

    def fit(self, train_data: Dict) -> None:
        """
        Build contract-clause matrix and compute similarities
        
        Args:
            train_data: Dictionary containing training data with structure:
                {
                    'contracts': {id: text, ...},
                    'clause_types': {id: type, ...},
                    'labels': {id: label, ...}
                }
        """
        # Create indices for contracts and clauses
        unique_contracts = {
            '_'.join(id_.split('_')[:-1])  # Get original contract ID
            for id_ in train_data['contracts']
        }
        unique_clause_types = set(train_data['clause_types'].values())
        
        self.contract_index = {c: i for i, c in enumerate(unique_contracts)}
        self.clause_index = {c: i for i, c in enumerate(unique_clause_types)}
        
        # Initialize matrix
        self.contract_clause_matrix = np.zeros(
            (len(self.contract_index), len(self.clause_index))
        )
        
        # Fill matrix based on positive examples
        for example_id, label in train_data['labels'].items():
            if label == 1:  # Only use positive examples
                contract_id = '_'.join(example_id.split('_')[:-1])
                clause_type = train_data['clause_types'][example_id]
                
                self.contract_clause_matrix[
                    self.contract_index[contract_id],
                    self.clause_index[clause_type]
                ] = 1
        
        # Compute means
        self.contract_means = np.mean(self.contract_clause_matrix, axis=1)
        self.clause_means = np.mean(self.contract_clause_matrix, axis=0)
        
        # Compute similarity matrix using adjusted cosine
        self._compute_similarity_matrix()

    def _compute_similarity_matrix(self) -> None:
        """
        Compute adjusted cosine similarity matrix between clause types
        Following the paper's formula:
        sim(i,j) = Σu(r(u,i) - r̄u)(r(u,j) - r̄j) / √(Σu r²(u,i))(Σu r²(u,j))
        """
        n_clauses = len(self.clause_index)
        self.similarity_matrix = np.zeros((n_clauses, n_clauses))
        
        # Adjust ratings by subtracting means
        adjusted_matrix = (
            self.contract_clause_matrix - 
            self.contract_means[:, np.newaxis]
        )
        
        # Compute similarities
        for i in range(n_clauses):
            for j in range(n_clauses):
                if i != j:
                    numerator = np.sum(
                        adjusted_matrix[:, i] * adjusted_matrix[:, j]
                    )
                    denominator = np.sqrt(
                        np.sum(adjusted_matrix[:, i] ** 2) *
                        np.sum(adjusted_matrix[:, j] ** 2)
                    )
                    
                    if denominator != 0:
                        self.similarity_matrix[i, j] = numerator / denominator

    def predict(self, contract_id: str, clause_type: str) -> float:
        """
        Predict if a clause type is relevant for a contract
        
        Args:
            contract_id: Contract identifier
            clause_type: Target clause type
            
        Returns:
            float: Prediction score between 0 and 1
        """
        if clause_type not in self.clause_index:
            return 0
            
        clause_idx = self.clause_index[clause_type]
        contract_idx = self.contract_index.get(contract_id)
        
        if contract_idx is None:
            return 0
        
        # Get contract's ratings and clause similarities
        contract_ratings = self.contract_clause_matrix[contract_idx]
        clause_similarities = self.similarity_matrix[clause_idx]
        
        # Calculate score components
        rating_diff = contract_ratings - self.clause_means
        numerator = np.sum(clause_similarities * rating_diff)
        denominator = np.sum(clause_similarities)
        
        if denominator == 0:
            return 0
            
        score = (numerator / denominator) + self.clause_means[clause_idx]
        return float(score > self.threshold)

    def predict_all(self, test_data: Dict) -> Dict[str, int]:
        """
        Predict relevance for all examples in test set
        
        Args:
            test_data: Dictionary with same structure as training data
            
        Returns:
            Dictionary mapping example IDs to predictions (0 or 1)
        """
        predictions = {}
        
        for example_id in test_data['contracts']:
            contract_id = '_'.join(example_id.split('_')[:-1])
            clause_type = test_data['clause_types'][example_id]
            
            pred = self.predict(contract_id, clause_type)
            predictions[example_id] = pred
            
        return predictions

if __name__ == "__main__":
    # Test CFRecommender
    from src.data.ledgar_dataset import LEDGARDataset
    
    # Load data
    dataset = LEDGARDataset("data/ledgar/ledgar.json")
    dataset.load_data()
    train_data, val_data, test_data = dataset.create_proxy_datasets()
    
    # Initialize and fit recommender
    config = {'threshold': 0.5}
    recommender = CFRecommender(config)
    recommender.fit(train_data)
    
    # Make predictions
    predictions = recommender.predict_all(test_data)
    
    # Print some statistics
    n_positive = sum(1 for p in predictions.values() if p == 1)
    print(f"Total predictions: {len(predictions)}")
    print(f"Positive predictions: {n_positive}")
    print(f"Negative predictions: {len(predictions) - n_positive}")# src/models/contract_bert.py

import torch
import torch.nn as nn
from transformers import BertModel, BertPreTrainedModel, BertConfig
from typing import Dict, Optional, Tuple
import logging
from torch.utils.data import DataLoader, Dataset

class ContractBERTPreTrainingDataset(Dataset):
    """Dataset for ContractBERT's additional pretraining tasks"""
    
    def __init__(
        self,
        contracts: Dict[str, str],
        tokenizer,
        max_length: int = 512
    ):
        self.contracts = contracts
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # Process all contracts into training examples
        self.examples = self._create_examples()
        
    def _create_examples(self):
        """Create training examples with labels for additional tasks"""
        examples = []
        
        for contract_id, contract_text in self.contracts.items():
            # Split into clauses for clause-word prediction
            clauses = contract_text.split('\n')
            
            for i, clause in enumerate(clauses):
                if not clause.strip():
                    continue
                    
                # Create input for clause-word prediction
                clause_encoding = self.tokenizer(
                    clause,
                    max_length=self.max_length,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )
                
                # Create labels for clause-word prediction
                # 1 for tokens in the clause, 0 for others
                clause_word_labels = torch.ones_like(
                    clause_encoding['input_ids']
                )
                
                # Create sentence pair examples
                if i < len(clauses) - 1:
                    # Positive example (consecutive sentences)
                    next_clause = clauses[i + 1]
                    pair_encoding = self.tokenizer(
                        clause,
                        next_clause,
                        max_length=self.max_length,
                        padding='max_length',
                        truncation=True,
                        return_tensors='pt'
                    )
                    
                    examples.append({
                        'input_ids': clause_encoding['input_ids'],
                        'attention_mask': clause_encoding['attention_mask'],
                        'clause_word_labels': clause_word_labels,
                        'pair_input_ids': pair_encoding['input_ids'],
                        'pair_attention_mask': pair_encoding['attention_mask'],
                        'sentence_pair_label': torch.tensor([1])
                    })
                    
                    # Negative example (non-consecutive sentences)
                    if i < len(clauses) - 2:
                        non_consecutive = clauses[i + 2]
                        neg_pair_encoding = self.tokenizer(
                            clause,
                            non_consecutive,
                            max_length=self.max_length,
                            padding='max_length',
                            truncation=True,
                            return_tensors='pt'
                        )
                        
                        examples.append({
                            'input_ids': clause_encoding['input_ids'],
                            'attention_mask': clause_encoding['attention_mask'],
                            'clause_word_labels': clause_word_labels,
                            'pair_input_ids': neg_pair_encoding['input_ids'],
                            'pair_attention_mask': neg_pair_encoding['attention_mask'],
                            'sentence_pair_label': torch.tensor([0])
                        })
        
        return examples
    
    def __len__(self):
        return len(self.examples)
        
    def __getitem__(self, idx):
        return self.examples[idx]

class ContractBERTModel(BertPreTrainedModel):
    """
    ContractBERT with additional pretraining tasks:
    1. Clause word prediction
    2. Sentence pair prediction
    """
    
    def __init__(self, config):
        super().__init__(config)
        
        # Base BERT model
        self.bert = BertModel(config)
        
        # Additional task heads
        self.clause_word_classifier = nn.Linear(config.hidden_size, 2)
        self.sentence_pair_classifier = nn.Linear(config.hidden_size, 2)
        
        # Loss functions
        self.clause_word_loss_fct = nn.CrossEntropyLoss()
        self.sentence_pair_loss_fct = nn.CrossEntropyLoss()
        
        # Initialize weights
        self.init_weights()
        
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        clause_word_labels: Optional[torch.Tensor] = None,
        pair_input_ids: Optional[torch.Tensor] = None,
        pair_attention_mask: Optional[torch.Tensor] = None,
        sentence_pair_label: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """Forward pass with both additional tasks"""
        
        # Get BERT outputs
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        sequence_output = outputs.last_hidden_state
        pooled_output = outputs.pooler_output
        
        loss = 0.0
        results = {}
        
        # Clause word prediction
        if clause_word_labels is not None:
            clause_word_logits = self.clause_word_classifier(sequence_output)
            clause_word_loss = self.clause_word_loss_fct(
                clause_word_logits.view(-1, 2),
                clause_word_labels.view(-1)
            )
            loss += clause_word_loss
            results['clause_word_loss'] = clause_word_loss
            results['clause_word_logits'] = clause_word_logits
        
        # Sentence pair prediction
        if pair_input_ids is not None:
            # Get embeddings for pair
            pair_outputs = self.bert(
                input_ids=pair_input_ids,
                attention_mask=pair_attention_mask,
                return_dict=True
            )
            pair_pooled_output = pair_outputs.pooler_output
            
            # Combine embeddings
            combined = torch.cat([pooled_output, pair_pooled_output], dim=1)
            sentence_pair_logits = self.sentence_pair_classifier(combined)
            
            if sentence_pair_label is not None:
                sentence_pair_loss = self.sentence_pair_loss_fct(
                    sentence_pair_logits,
                    sentence_pair_label
                )
                loss += sentence_pair_loss
                results['sentence_pair_loss'] = sentence_pair_loss
            
            results['sentence_pair_logits'] = sentence_pair_logits
        
        results['loss'] = loss
        return results
    
    def get_clause_embedding(self, clause_text: str) -> torch.Tensor:
        """Get embedding for a single clause"""
        # Tokenize
        inputs = self.tokenizer(
            clause_text,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.config.max_length
        ).to(self.device)
        
        # Get embedding
        with torch.no_grad():
            outputs = self.bert(**inputs)
            return outputs.pooler_output[0]
    
    def train_model(
        self,
        train_data: Dict,
        val_data: Dict,
        num_epochs: int,
        batch_size: int,
        learning_rate: float
    ) -> None:
        """Train ContractBERT with additional tasks"""
        # Create datasets
        train_dataset = ContractBERTPreTrainingDataset(
            train_data['contracts'],
            self.tokenizer,
            self.config.max_length
        )
        val_dataset = ContractBERTPreTrainingDataset(
            val_data['contracts'],
            self.tokenizer,
            self.config.max_length
        )
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size
        )
        
        # Optimizer
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=learning_rate
        )
        
        # Training loop
        best_val_loss = float('inf')
        patience = 3
        patience_counter = 0
        
        for epoch in range(num_epochs):
            # Training
            self.train()
            total_train_loss = 0
            
            for batch in train_loader:
                # Move to device
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # Forward pass
                outputs = self(**batch)
                loss = outputs['loss']
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_train_loss += loss.item()
            
            avg_train_loss = total_train_loss / len(train_loader)
            
            # Validation
            self.eval()
            total_val_loss = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    outputs = self(**batch)
                    total_val_loss += outputs['loss'].item()
            
            avg_val_loss = total_val_loss / len(val_loader)
            
            # Early stopping
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                logging.info(f"Early stopping at epoch {epoch + 1}")
                break
            
            logging.info(
                f"Epoch {epoch + 1}/{num_epochs} - "
                f"Train loss: {avg_train_loss:.4f}, "
                f"Val loss: {avg_val_loss:.4f}"
            )

if __name__ == "__main__":
    # Test ContractBERT
    from transformers import BertTokenizer
    
    # Load tokenizer and config
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    config = BertConfig.from_pretrained('bert-base-uncased')
    
    # Add custom config parameters
    config.max_length = 512
    
    # Initialize model
    model = ContractBERTModel(config)
    
    # Test forward pass
    batch_size = 2
    seq_length = 10
    
    dummy_input = {
        'input_ids': torch.randint(0, 1000, (batch_size, seq_length)),
        'attention_mask': torch.ones(batch_size, seq_length),
        'clause_word_labels': torch.randint(0, 2, (batch_size, seq_length)),
        'pair_input_ids': torch.randint(0, 1000, (batch_size, seq_length)),
        'pair_attention_mask': torch.ones(batch_size, seq_length),
        'sentence_pair_label': torch.randint(0, 2, (batch_size, 1))
    }
    
    outputs = model(**dummy_input)
    print("Forward pass successful!")
    print(f"Loss: {outputs['loss'].item():.4f}")# src/models/clause_classifier.py

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Optional
import logging
import numpy as np

class ClauseClassifierDataset(Dataset):
    """Dataset for clause relevance classification"""
    
    def __init__(
        self,
        data: Dict,
        contract_bert_model,
        device: str = 'cuda'
    ):
        self.data = data
        self.contract_bert = contract_bert_model
        self.device = device
        
        # Pre-compute embeddings for efficiency
        self.embeddings = {}
        self._compute_embeddings()
        
    def _compute_embeddings(self):
        """Pre-compute contract embeddings using ContractBERT"""
        self.contract_bert.eval()
        with torch.no_grad():
            for example_id, contract_text in self.data['contracts'].items():
                # Split contract into clauses
                clauses = contract_text.split('\n')
                clause_embeddings = []
                
                # Get embedding for each clause
                for clause in clauses:
                    if clause.strip():
                        embedding = self.contract_bert.get_clause_embedding(
                            clause
                        )
                        clause_embeddings.append(embedding)
                
                # Average clause embeddings
                if clause_embeddings:
                    self.embeddings[example_id] = torch.stack(
                        clause_embeddings
                    ).mean(dim=0)
        
    def __len__(self):
        return len(self.data['contracts'])
    
    def __getitem__(self, idx):
        example_id = list(self.data['contracts'].keys())[idx]
        
        return {
            'embedding': self.embeddings[example_id],
            'label': torch.tensor(
                self.data['labels'][example_id],
                dtype=torch.float32
            )
        }

class ClauseRelevanceClassifier(nn.Module):
    """
    7-layer neural network for clause type relevance prediction
    as described in the paper
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        
        # Layer sizes from paper
        self.input_size = config['input_size']  # 768 (BERT hidden size)
        self.hidden_sizes = config['hidden_sizes']  # [512, 256, 128, 64, 32, 16]
        
        # Build layers
        layers = []
        prev_size = self.input_size
        
        for hidden_size in self.hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(config['dropout_prob'])
            ])
            prev_size = hidden_size
        
        # Output layer
        layers.append(nn.Linear(self.hidden_sizes[-1], 1))
        layers.append(nn.Sigmoid())
        
        self.classifier = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.classifier(x).squeeze()
    
    def train_model(
        self,# src/models/clause_classifier.py (continued)

    def train_model(
        self,
        train_data: Dict,
        val_data: Dict,
        contract_bert_model,
        num_epochs: int = 5000,  # As specified in paper
        batch_size: int = 64,    # As specified in paper
        learning_rate: float = 1e-5,
        device: str = 'cuda'
    ) -> None:
        """Train the classifier for 5000 epochs as specified in paper"""
        self.to(device)
        
        # Create datasets
        train_dataset = ClauseClassifierDataset(
            train_data,
            contract_bert_model,
            device
        )
        val_dataset = ClauseClassifierDataset(
            val_data,
            contract_bert_model,
            device
        )
        
        # Create dataloaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size
        )
        
        # Loss and optimizer
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
        
        # Training loop
        best_val_loss = float('inf')
        patience = 5
        patience_counter = 0
        
        for epoch in range(num_epochs):
            # Training
            self.train()
            total_train_loss = 0
            correct_train = 0
            total_train = 0
            
            for batch in train_loader:
                embeddings = batch['embedding'].to(device)
                labels = batch['label'].to(device)
                
                # Forward pass
                outputs = self(embeddings)
                loss = criterion(outputs, labels)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_train_loss += loss.item()
                
                # Calculate accuracy
                predictions = (outputs > 0.5).float()
                correct_train += (predictions == labels).sum().item()
                total_train += labels.size(0)
            
            avg_train_loss = total_train_loss / len(train_loader)
            train_accuracy = correct_train / total_train
            
            # Validation
            self.eval()
            total_val_loss = 0
            correct_val = 0
            total_val = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    embeddings = batch['embedding'].to(device)
                    labels = batch['label'].to(device)
                    
                    outputs = self(embeddings)
                    loss = criterion(outputs, labels)
                    
                    total_val_loss += loss.item()
                    
                    predictions = (outputs > 0.5).float()
                    correct_val += (predictions == labels).sum().item()
                    total_val += labels.size(0)
            
            avg_val_loss = total_val_loss / len(val_loader)
            val_accuracy = correct_val / total_val
            
            # Early stopping
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.state_dict(), 'best_classifier.pt')
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                logging.info(f"Early stopping triggered at epoch {epoch + 1}")
                break
            
            if (epoch + 1) % 100 == 0:  # Log every 100 epochs
                logging.info(
                    f"Epoch [{epoch+1}/{num_epochs}], "
                    f"Train Loss: {avg_train_loss:.4f}, "
                    f"Train Acc: {train_accuracy:.4f}, "
                    f"Val Loss: {avg_val_loss:.4f}, "
                    f"Val Acc: {val_accuracy:.4f}"
                )
    
    def predict(self, contract_id: str, inputs: Dict) -> float:
        """Predict relevance for a single contract"""
        self.eval()
        with torch.no_grad():
            embedding = inputs['embedding'].unsqueeze(0)
            output = self(embedding)
            return output.item()
    
    def predict_all(self, test_data: Dict) -> Dict[str, int]:
        """
        Predict relevance for all examples in test set
        Returns:
            Dict mapping example IDs to binary predictions (0 or 1)
        """
        self.eval()
        predictions = {}
        
        # Create test dataset
        test_dataset = ClauseClassifierDataset(
            test_data,
            self.contract_bert,
            self.device
        )
        test_loader = DataLoader(test_dataset, batch_size=64)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                embeddings = batch['embedding'].to(self.device)
                outputs = self(embeddings)
                
                # Convert to binary predictions
                batch_preds = (outputs > 0.5).float().cpu().numpy()
                
                # Get corresponding example IDs
                start_idx = batch_idx * test_loader.batch_size
                batch_ids = list(test_data['contracts'].keys())[
                    start_idx:start_idx + len(batch_preds)
                ]
                
                # Store predictions
                for example_id, pred in zip(batch_ids, batch_preds):
                    predictions[example_id] = int(pred)
        
        return predictions

if __name__ == "__main__":
    # Test classifier
    from src.data.ledgar_dataset import LEDGARDataset
    from src.models.contract_bert import ContractBERTModel
    
    # Load data
    dataset = LEDGARDataset("data/ledgar/ledgar.json")
    dataset.load_data()
    train_data, val_data, test_data = dataset.create_proxy_datasets()
    
    # Initialize ContractBERT
    contract_bert = ContractBERTModel.from_pretrained('bert-base-uncased')
    
    # Initialize classifier
    config = {
        'input_size': 768,
        'hidden_sizes': [512, 256, 128, 64, 32, 16],
        'dropout_prob': 0.3
    }
    classifier = ClauseRelevanceClassifier(config)
    
    # Train classifier
    classifier.train_model(
        train_data=train_data,
        val_data=val_data,
        contract_bert_model=contract_bert
    )
    
    # Test predictions
    predictions = classifier.predict_all(test_data)
    print(f"Total predictions: {len(predictions)}")
    print(f"Positive predictions: {sum(predictions.values())}")
    print(f"Negative predictions: {len(predictions) - sum(predictions.values())}")# src/models/clause_generator.py

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import PreTrainedModel
import logging
from typing import Dict, List, Optional

class ClauseGeneratorDataset(Dataset):
    """Dataset for clause generation training"""
    
    def __init__(
        self,
        data: Dict,
        tokenizer,
        contract_bert_model,
        max_length: int = 512,
        device: str = 'cuda'
    ):
        self.data = data
        self.tokenizer = tokenizer
        self.contract_bert = contract_bert_model
        self.max_length = max_length
        self.device = device
        
        # Pre-compute embeddings
        self.contract_embeddings = {}
        self._compute_embeddings()
        
    def _compute_embeddings(self):
        """Pre-compute contract and clause type embeddings"""
        self.contract_bert.eval()
        with torch.no_grad():
            for example_id in self.data['contracts']:
                # Get contract embedding
                contract_text = self.data['contracts'][example_id]
                clauses = contract_text.split('\n')
                clause_embeddings = []
                
                for clause in clauses:
                    if clause.strip():
                        embedding = self.contract_bert.get_clause_embedding(
                            clause
                        )
                        clause_embeddings.append(embedding)
                
                if clause_embeddings:
                    contract_embedding = torch.stack(
                        clause_embeddings
                    ).mean(dim=0)
                    
                    # Get clause type embedding
                    clause_type = self.data['clause_types'][example_id]
                    clause_type_embedding = self.contract_bert.get_clause_type_embedding(
                        clause_type
                    )
                    
                    # Average contract and clause type embeddings as per paper
                    self.contract_embeddings[example_id] = (
                        contract_embedding + clause_type_embedding
                    ) / 2
    
    def __len__(self):
        return len(self.data['contracts'])
    
    def __getitem__(self, idx):
        example_id = list(self.data['contracts'].keys())[idx]
        
        # Get target clause text
        target_clause = self.data['target_clauses'][example_id]
        target_encoding = self.tokenizer(
            target_clause,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'encoder_embedding': self.contract_embeddings[example_id],
            'decoder_input_ids': target_encoding['input_ids'].squeeze(0),
            'decoder_attention_mask': target_encoding['attention_mask'].squeeze(0),
            'labels': target_encoding['input_ids'].squeeze(0)
        }

class ClauseGenerator(PreTrainedModel):
    """
    Transformer-based generator for clause content
    as described in the paper
    """
    
    def __init__(self, config, contract_bert):
        super().__init__(config)
        
        # Keep reference to ContractBERT (frozen)
        self.contract_bert = contract_bert
        for param in self.contract_bert.parameters():
            param.requires_grad = False
        
        # Decoder
        self.decoder = nn.TransformerDecoder(
            num_layers=config.num_decoder_layers,
            d_model=config.hidden_size,
            nhead=config.num_attention_heads,
            dim_feedforward=config.intermediate_size,
            dropout=config.hidden_dropout_prob,
            activation=config.hidden_act
        )
        
        # Output projection
        self.output_projection = nn.Linear(
            config.hidden_size,
            config.vocab_size
        )
        
        # Initialize weights
        self.init_weights()
    
    def forward(
        self,
        encoder_embedding: torch.Tensor,
        decoder_input_ids: torch.Tensor,
        decoder_attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """Forward pass"""
        # Create causal mask for decoder
        tgt_len = decoder_input_ids.size(1)
        tgt_mask = self.generate_square_subsequent_mask(tgt_len)
        tgt_mask = tgt_mask.to(decoder_input_ids.device)
        
        # Decoder forward pass
        decoder_outputs = self.decoder(
            tgt=self.embed_tokens(decoder_input_ids),
            memory=encoder_embedding.unsqueeze(1),
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=~decoder_attention_mask if decoder_attention_mask is not None else None
        )
        
        # Project to vocabulary
        logits = self.output_projection(decoder_outputs)
        
        outputs = {'logits': logits}
        
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(
                logits.view(-1, self.config.vocab_size),
                labels.view(-1)
            )
            outputs['loss'] = loss
        
        return outputs
    
    @staticmethod
    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:
        """Generate causal mask for decoder"""
        mask = torch.triu(
            torch.ones(sz, sz),
            diagonal=1
        )
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask
    
    def generate_all(
        self,
        test_data: Dict,
        max_length: int = 512,
        num_beams: int = 4,
        temperature: float = 1.0
    ) -> Dict[str, str]:
        """
        Generate clauses for all test examples
        Returns:
            Dict mapping example IDs to generated clause text
        """
        self.eval()
        generations = {}
        
        # Create test dataset
        test_dataset = ClauseGeneratorDataset(
            test_data,
            self.tokenizer,
            self.contract_bert,
            max_length=max_length,
            device=self.device
        )
        
        test_loader = DataLoader(test_dataset, batch_size=1)
        
        with torch.no_grad():
            for batch in test_loader:
                encoder_embedding = batch['encoder_embedding'].to(self.device)
                
                # Initialize decoder input
                decoder_input = torch.tensor(
                    [[self.tokenizer.cls_token_id]],
                    device=self.device
                )
                
                for _ in range(max_length):
                    outputs = self(
                        encoder_embedding=encoder_embedding,
                        decoder_input_ids=decoder_input
                    )
                    
                    # Get next token probabilities
                    next_token_logits = outputs['logits'][:, -1, :] / temperature
                    next_token = torch.argmax(next_token_logits, dim=-1)
                    
                    # Break if we hit [SEP] token
                    if next_token.item() == self.tokenizer.sep_token_id:
                        break
                    
                    # Append to decoder input
                    decoder_input = torch.cat([
                        decoder_input,
                        next_token.unsqueeze(0)
                    ], dim=1)
                
                # Convert to text
                generated_text = self.tokenizer.decode(
                    decoder_input[0],
                    skip_special_tokens=True
                )
                
                # Store generation
                example_id = list(test_data['contracts'].keys())[0]
                generations[example_i# src/models/clause_generator.py (continued)

                # Store generation
                example_id = list(test_data['contracts'].keys())[
                    len(generations)
                ]
                generations[example_id] = generated_text
        
        return generations
    
    def train_model(
        self,
        train_data: Dict,
        val_data: Dict,
        num_epochs: int = 300,  # As mentioned in paper
        batch_size: int = 8,
        learning_rate: float = 1e-4
    ) -> None:
        """Train the generator"""
        
        # Create datasets
        train_dataset = ClauseGeneratorDataset(
            train_data,
            self.tokenizer,
            self.contract_bert
        )
        val_dataset = ClauseGeneratorDataset(
            val_data,
            self.tokenizer,
            self.contract_bert
        )
        
        # Create dataloaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size
        )
        
        # Optimizer
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=learning_rate
        )
        
        # Training loop
        best_val_loss = float('inf')
        patience = 5
        patience_counter = 0
        
        for epoch in range(num_epochs):
            # Training
            self.train()
            total_train_loss = 0
            
            for batch in train_loader:
                # Move to device
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # Forward pass
                outputs = self(**batch)
                loss = outputs['loss']
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_train_loss += loss.item()
            
            avg_train_loss = total_train_loss / len(train_loader)
            
            # Validation
            self.eval()
            total_val_loss = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    outputs = self(**batch)
                    total_val_loss += outputs['loss'].item()
            
            avg_val_loss = total_val_loss / len(val_loader)
            
            # Early stopping
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.state_dict(), 'best_generator.pt')
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                logging.info(f"Early stopping triggered at epoch {epoch + 1}")
                break
            
            logging.info(
                f"Epoch [{epoch+1}/{num_epochs}], "
                f"Train Loss: {avg_train_loss:.4f}, "
                f"Val Loss: {avg_val_loss:.4f}"
            )
    
    def _prepare_encoder_input(
        self,
        contract_text: str,
        clause_type: str
    ) -> torch.Tensor:
        """
        Prepare encoder input by averaging contract and clause type embeddings
        as described in the paper
        """
        with torch.no_grad():
            # Get contract embedding
            contract_embedding = self.contract_bert.get_clause_embedding(
                contract_text
            )
            
            # Get clause type embedding
            clause_type_embedding = self.contract_bert.get_clause_type_embedding(
                clause_type
            )
            
            # Average them as per paper
            return (contract_embedding + clause_type_embedding) / 2

    def embed_tokens(self, input_ids: torch.Tensor) -> torch.Tensor:
        """Embed input tokens"""
        return self.contract_bert.embeddings(input_ids)

if __name__ == "__main__":
    # Test generator
    from transformers import BertConfig
    from src.data.ledgar_dataset import LEDGARDataset
    from src.models.contract_bert import ContractBERTModel
    
    # Load data
    dataset = LEDGARDataset("data/ledgar/ledgar.json")
    dataset.load_data()
    train_data, val_data, test_data = dataset.create_proxy_datasets()
    
    # Initialize ContractBERT
    contract_bert = ContractBERTModel.from_pretrained('bert-base-uncased')
    
    # Initialize generator config
    config = BertConfig.from_pretrained('bert-base-uncased')
    config.num_decoder_layers = 6
    config.num_attention_heads = 8
    config.intermediate_size = 2048
    config.hidden_dropout_prob = 0.1
    config.hidden_act = "gelu"
    
    # Initialize generator
    generator = ClauseGenerator(config, contract_bert)
    
    # Train generator
    generator.train_model(
        train_data=train_data,
        val_data=val_data
    )
    
    # Test generation
    generations = generator.generate_all(test_data)
    print(f"\nGenerated {len(generations)} clauses")
    
    # Print sample generation
    example_id = next(iter(generations))
    print(f"\nSample generation for {example_id}:")
    print(generations[example_id])# src/approaches/document_similarity.py

import torch
import numpy as np
from typing import Dict, List, Tuple
from sklearn.metrics.pairwise import cosine_similarity
import logging

class DocSimRecommender:
    """
    Document similarity-based recommender implementing the k-nearest
    neighbor approach from the paper
    """
    
    def __init__(self, config: Dict):
        self.k_values = config.get('k_values', [1, 3, 5])
        self.threshold = config.get('threshold', 0.5)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Store contract embeddings and metadata
        self.contract_embeddings = {}
        self.contract_clause_types = {}
        self.clause_library = {}
    
    def fit(self, train_data: Dict, contract_bert_model) -> None:
        """
        Compute and store embeddings for all training contracts
        
        Args:
            train_data: Training data dictionary
            contract_bert_model: Trained ContractBERT model
        """
        logging.info("Computing embeddings for document similarity...")
        
        contract_bert_model.eval()
        with torch.no_grad():
            # Compute embeddings for all contracts
            for example_id in train_data['contracts']:
                # Get contract text and clause type
                contract_text = train_data['contracts'][example_id]
                clause_type = train_data['clause_types'][example_id]
                
                # Compute embedding using ContractBERT
                embedding = contract_bert_model.get_clause_embedding(
                    contract_text
                ).cpu().numpy()
                
                # Store embedding and metadata
                self.contract_embeddings[example_id] = embedding
                self.contract_clause_types[example_id] = clause_type
                
                # Store clause text in library if it's a positive example
                if train_data['labels'][example_id] == 1:
                    target_clause = train_data['target_clauses'][example_id]
                    if clause_type not in self.clause_library:
                        self.clause_library[clause_type] = []
                    self.clause_library[clause_type].append(target_clause)
        
        logging.info(f"Processed {len(self.contract_embeddings)} contracts")
    
    def _find_k_nearest(
        self,
        query_embedding: np.ndarray,
        k: int
    ) -> List[Tuple[str, float]]:
        """Find k nearest contracts to query"""
        # Compute similarities with all contracts
        similarities = []
        for contract_id, embedding in self.contract_embeddings.items():
            sim = cosine_similarity(
                query_embedding.reshape(1, -1),
                embedding.reshape(1, -1)
            )[0][0]
            similarities.append((contract_id, sim))
        
        # Sort by similarity and return top k
        return sorted(similarities, key=lambda x: x[1], reverse=True)[:k]
    
    def predict_all(
        self,
        test_data: Dict,
        k: int,
        contract_bert_model
    ) -> Dict[str, int]:
        """
        Predict relevance for all test examples using k-nearest neighbors
        
        Args:
            test_data: Test data dictionary
            k: Number of neighbors to consider
            contract_bert_model: Trained ContractBERT model
            
        Returns:
            Dictionary mapping example IDs to binary predictions
        """
        predictions = {}
        
        contract_bert_model.eval()
        with torch.no_grad():
            for example_id in test_data['contracts']:
                # Get contract text
                contract_text = test_data['contracts'][example_id]
                target_clause_type = test_data['clause_types'][example_id]
                
                # Compute query embedding
                query_embedding = contract_bert_model.get_clause_embedding(
                    contract_text
                ).cpu().numpy()
                
                # Find k nearest neighbors
                neighbors = self._find_k_nearest(query_embedding, k)
                
                # Check if target clause type appears in neighbors
                neighbor_clause_types = [
                    self.contract_clause_types[n_id]
                    for n_id, _ in neighbors
                ]
                
                # Predict 1 if target type appears in neighbors
                predictions[example_id] = int(
                    target_clause_type in neighbor_clause_types
                )
        
        return predictions
    
    def retrieve_clauses(
        self,
        test_data: Dict,
        contract_bert_model,
        top_n: int = 1
    ) -> Dict[str, str]:
        """
        Retrieve most similar clauses from library for each test example
        
        Args:
            test_data: Test data dictionary
            contract_bert_model: Trained ContractBERT model
            top_n: Number of clauses to retrieve (default 1 for comparison)
            
        Returns:
            Dictionary mapping example IDs to retrieved clause text
        """
        retrieved_clauses = {}
        
        contract_bert_model.eval()
        with torch.no_grad():
            for example_id in test_data['contracts']:
                # Get contract text and target type
                contract_text = test_data['contracts'][example_id]
                target_type = test_data['clause_types'][example_id]
                
                # Get candidate clauses of target type
                if target_type not in self.clause_library:
                    retrieved_clauses[example_id] = ""
                    continue
                
                candidates = self.clause_library[target_type]
                
                # Compute query embedding
                query_embedding = contract_bert_model.get_clause_embedding(
                    contract_text
                ).cpu().numpy()
                
                # Compute similarities with candidates
                similarities = []
                for clause in candidates:
                    clause_embedding = contract_bert_model.get_clause_embedding(
                        clause
                    ).cpu().numpy()
                    
                    sim = cosine_similarity(
                        query_embedding.reshape(1, -1),
                        clause_embedding.reshape(1, -1)
                    )[0][0]
                    similarities.append((clause, sim))
                
                # Sort by similarity and get top_n
                similarities.sort(key=lambda x: x[1], reverse=True)
                top_clauses = [c for c, _ in similarities[:top_n]]
                
                # Store most similar clause
                retrieved_clauses[example_id] = top_clauses[0]
        
        return retrieved_clauses

if __name__ == "__main__":
    # Test document similarity recommender
    from src.data.ledgar_dataset import LEDGARDataset
    from src.models.contract_bert import ContractBERTModel
    
    # Load data
    dataset = LEDGARDataset("data/ledgar/ledgar.json")
    dataset.load_data()
    train_data, val_data, test_data = dataset.create_proxy_datasets()
    
    # Initialize ContractBERT
    contract_bert = ContractBERTModel.from_pretrained('bert-base-uncased')
    
    # Initialize recommender
    config = {'k_values': [1, 3, 5]}
    recommender = DocSimRecommender(config)
    
    # Fit recommender
    recommender.fit(train_data, contract_bert)
    
    # Test predictions
    for k in config['k_values']:
        predictions = recommender.predict_all(test_data, k, contract_bert)
        print(f"\nResults for k={k}:")
        print(f"Total predictions: {len(predictions)}")
        print(f"Positive predictions: {sum(predictions.values())}")
    
    # Test retrieval
    retrieved = recommender.retrieve_clauses(test_data, contract_bert)
    print(f"\nRetrieved {len(retrieved)} clauses")
ChatGPT

Thought about CLAUSEREC pipeline and modules for 7 seconds
Below is a checklist of the most common missing pieces or inconsistencies still lurking in your code, especially around ClauseGenerator, ContractBERTModel, and how you handle clause-type embeddings.

1. get_clause_type_embedding() Is Not Defined
In ClauseGeneratorDataset and ClauseGenerator, you call:

clause_type_embedding = self.contract_bert.get_clause_type_embedding(clause_type)
But in ContractBERTModel, you only have:

def get_clause_embedding(self, clause_text: str) -> torch.Tensor: ...
There is no method named get_clause_type_embedding(). This will throw an error at runtime. To fix:

Either add a get_clause_type_embedding() method in ContractBERTModel, something like:
def get_clause_type_embedding(self, clause_type: str) -> torch.Tensor:
    # Possibly just tokenize the clause_type string
    inputs = self.tokenizer(
        clause_type,
        return_tensors='pt',
        truncation=True,
        max_length=32
    ).to(self.device)
    
    with torch.no_grad():
        outputs = self.bert(**inputs)
        return outputs.pooler_output[0]
Or remove calls to it and just embed the clause type the same way you embed a clause.
2. self.tokenizer Missing Inside ClauseGenerator
Inside ClauseGenerator, you do:

decoder_input = torch.tensor([[self.tokenizer.cls_token_id]], device=self.device)
...
next_token.item() == self.tokenizer.sep_token_id
...
generated_text = self.tokenizer.decode(...)
But we don’t see where self.tokenizer is assigned (like self.tokenizer = ???). In ClauseGeneratorDataset, there is a self.tokenizer, but that belongs only to the dataset class, not automatically to the ClauseGenerator model class.

Solution: pass the tokenizer into ClauseGenerator.__init__ and store it:

def __init__(self, config, contract_bert, tokenizer):
    super().__init__(config)
    self.contract_bert = contract_bert
    self.tokenizer = tokenizer  # store it here
    ...
Then you can do self.tokenizer.cls_token_id safely in your generation loop.

3. self.contract_bert.embeddings(...) in embed_tokens()
In ClauseGenerator.embed_tokens(), you do:

def embed_tokens(self, input_ids: torch.Tensor) -> torch.Tensor:
    return self.contract_bert.embeddings(input_ids)
But ContractBERTModel does not define a property named .embeddings. It defines self.bert = BertModel(config). If you want to do the same token-embedding that BERT does, you typically call self.bert.embeddings(...). For instance:

def embed_tokens(self, input_ids: torch.Tensor) -> torch.Tensor:
    # If you want the raw token embeddings from BERT:
    return self.contract_bert.bert.embeddings.word_embeddings(input_ids)
Alternatively, you can manually run your entire BERT with no attention, just to get embeddings. But as written, self.contract_bert.embeddings does not exist, so you must fix it.

4. Data/Example Mismatch in ClauseGenerator.generate_all()
Inside generate_all(), you have:

example_id = list(test_data['contracts'].keys())[0]
generations[example_id] = generated_text
...
example_id = list(test_data['contracts'].keys())[ len(generations) ]
Two issues here:

You always use [0] or an index based on len(generations) to pick the next key in test_data['contracts']. But you are already iterating over the test_loader in a for-loop, which yields batches in a certain order.
If the order of keys in test_data['contracts'] is not guaranteed, you might mismatch example IDs.
Fix:
Grab the example ID from the dataset or batch instead of re-indexing. For instance, in your loop:

for i, batch in enumerate(test_loader):
    ...
    # We know each batch is size=1 in your code, so we can do:
    example_id = list(test_data['contracts'].keys())[i]
    ...
Or better, store the example IDs in the dataset so that ClauseGeneratorDataset.__getitem__ can return the example_id. Then you can do something like:

for batch in test_loader:
    example_ids = batch['example_ids']  # Suppose your dataset returns it
    ...
    generations[example_ids[0]] = generated_text
5. Ensuring the Clause Generation Actually Uses “(contract + clause_type) / 2” Embeddings
Your code in ClauseGeneratorDataset:

clause_type_embedding = self.contract_bert.get_clause_type_embedding(clause_type)
self.contract_embeddings[example_id] = (contract_embedding + clause_type_embedding) / 2
That’s good, but confirm that your get_clause_embedding and get_clause_type_embedding are consistent. If get_clause_type_embedding doesn’t exist, it’s a bug.

6. Pipeline Fit Calls for Document Similarity
In your main.py or pipeline, be sure you do:

self.doc_sim_recommender.fit(train_data, self.contract_bert)
before you call predict_all(test_data). Otherwise the doc-sim approach never gets the library or embeddings it needs.

7. Final Usage: Confirm the Flow
dataset = LEDGARDataset(...)
dataset.load_data()
train_data, val_data, test_data = dataset.create_proxy_datasets()
Train your ContractBERT with contract_bert.train_model(train_data['contracts'], ...) or the format you require.
Train your classifier with classifier.train_model(train_data, val_data, contract_bert).
Train your generator with generator.train_model(train_data, val_data).
Fit CF and doc-sim:
cf_recommender.fit(train_data)
doc_sim_recommender.fit(train_data, contract_bert)
Evaluate each approach with predict_all(...).
Double-check the shapes & data structures.

Summary
The biggest items you’re missing or need to fix:

Add a method (or remove references) to get_clause_type_embedding(...) in ContractBERTModel.
Assign self.tokenizer inside ClauseGenerator so your generation loop can use cls_token_id, sep_token_id, etc.
Fix references to self.contract_bert.embeddings in ClauseGenerator.embed_tokens() (should probably be self.contract_bert.bert.embeddings.word_embeddings(...)).
Correct how you index example_id in generate_all(). You can’t rely on [0] or [len(generations)]; you need a robust approach tied to the dataset.
Once you address those, the pipeline should run from start to finish without “missing method” or “attribute not found” errors, and your final results will be consistent with the paper’s design.




You said:
# src/models/contract_bert.py

class ContractBERTModel(BertPreTrainedModel):
    def __init__(self, config, tokenizer):
        super().__init__(config)
        self.bert = BertModel(config)
        self.tokenizer = tokenizer  # Store tokenizer
        
        # Task-specific heads remain the same...
        self.clause_word_classifier = nn.Linear(config.hidden_size, 2)
        self.sentence_pair_classifier = nn.Linear(config.hidden_size, 2)
        
        # Initialize weights
        self.init_weights()
    
    def get_clause_embedding(self, clause_text: str) -> torch.Tensor:
        """Get embedding for a clause text"""
        inputs = self.tokenizer(
            clause_text,
            return_tensors='pt',
            truncation=True,
            max_length=self.config.max_length,
            padding='max_length'
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.bert(**inputs)
            return outputs.pooler_output[0]
    
    def get_clause_type_embedding(self, clause_type: str) -> torch.Tensor:
        """Get embedding for a clause type"""
        inputs = self.tokenizer(
            clause_type,
            return_tensors='pt',
            truncation=True,
            max_length=32,  # Shorter max_length for clause types
            padding='max_length'
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.bert(**inputs)
            return outputs.pooler_output[0]
    
    def get_token_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
        """Get raw token embeddings"""
        return self.bert.embeddings.word_embeddings(input_ids)class ClauseGeneratorDataset(Dataset):
    def __init__(self, data: Dict, tokenizer, contract_bert_model, max_length: int = 512):
        self.data = data
        self.tokenizer = tokenizer
        self.contract_bert = contract_bert_model
        self.max_length = max_length
        
        # Add example_ids list for proper ordering
        self.example_ids = list(data['contracts'].keys())
        
        # Pre-compute embeddings
        self.contract_embeddings = {}
        self._compute_embeddings()
    
    def __getitem__(self, idx):
        example_id = self.example_ids[idx]
        
        # Get target clause text
        target_clause = self.data['target_clauses'][example_id]
        target_encoding = self.tokenizer(
            target_clause,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'example_id': example_id,  # Return example_id with batch
            'encoder_embedding': self.contract_embeddings[example_id],
            'decoder_input_ids': target_encoding['input_ids'].squeeze(0),
            'decoder_attention_mask': target_encoding['attention_mask'].squeeze(0),
            'labels': target_encoding['input_ids'].squeeze(0)
        }

class ClauseGenerator(PreTrainedModel):
    def __init__(self, config, contract_bert, tokenizer):
        super().__init__(config)
        
        self.contract_bert = contract_bert
        self.tokenizer = tokenizer  # Store tokenizer
        
        # Freeze ContractBERT
        for param in self.contract_bert.parameters():
            param.requires_grad = False
        
        # Rest of initialization remains same...
        self.decoder = nn.TransformerDecoder(...)
        self.output_projection = nn.Linear(...)
    
    def embed_tokens(self, input_ids: torch.Tensor) -> torch.Tensor:
        """Get token embeddings using BERT's embedding layer"""
        return self.contract_bert.get_token_embeddings(input_ids)
    
    def generate_all(self, test_data: Dict, max_length: int = 512) -> Dict[str, str]:
        """Generate clauses for all test examples"""
        self.eval()
        generations = {}
        
        # Create test dataset
        test_dataset = ClauseGeneratorDataset(
            test_data,
            self.tokenizer,
            self.contract_bert,
            max_length=max_length
        )
        
        test_loader = DataLoader(test_dataset, batch_size=1)
        
        with torch.no_grad():
            for batch in test_loader:
                # Get example_id from batch
                example_id = batch['example_id'][0]
                encoder_embedding = batch['encoder_embedding'].to(self.device)
                
                # Initialize generation with CLS token
                decoder_input = torch.tensor(
                    [[self.tokenizer.cls_token_id]],
                    device=self.device
                )
                
                # Generation loop
                for _ in range(max_length):
                    outputs = self(
                        encoder_embedding=encoder_embedding,
                        decoder_input_ids=decoder_input
                    )
                    
                    next_token_logits = outputs['logits'][:, -1, :]
                    next_token = torch.argmax(next_token_logits, dim=-1)
                    
                    if next_token.item() == self.tokenizer.sep_token_id:
                        break
                    
                    decoder_input = torch.cat([
                        decoder_input,
                        next_token.unsqueeze(0)
                    ], dim=1)
                
                # Decode generated text
                generated_text = self.tokenizer.decode(
                    decoder_input[0],
                    skip_special_tokens=True
                )
                
                # Store generation with correct example_id
                generations[example_id] = generated_text
        
        return generations# src/pipeline.py

class CLAUSERECPipeline:
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.setup_logging()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize tokenizer first
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        
        # Initialize components with proper dependencies
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialize all components with proper dependencies"""
        # Initialize ContractBERT with tokenizer
        self.contract_bert = ContractBERTModel(
            self.config['model'],
            self.tokenizer
        ).to(self.device)
        
        # Initialize classifier
        self.clause_classifier = ClauseRelevanceClassifier(
            self.config['classifier']
        ).to(self.device)
        
        # Initialize generator with tokenizer
        self.clause_generator = ClauseGenerator(
            self.config['generator'],
            self.contract_bert,
            self.tokenizer
        ).to(self.device)
        
        # Initialize recommenders
        self.cf_recommender = CFRecommender(self.config['cf'])
        self.doc_sim_recommender = DocSimRecommender(self.config['doc_sim'])
    
    def run_pipeline(self):
        """Run complete pipeline with correct ordering"""
        # 1. Prepare data
        train_data, val_data, test_data = self.prepare_data()
        
        # 2. Train ContractBERT first
        self.contract_bert.train_model(
            train_data=train_data,
            val_data=val_data,
            num_epochs=self.config['training']['contractbert']['num_epochs']
        )
        
        # 3. Train classifier using trained ContractBERT
        self.clause_classifier.train_model(
            train_data=train_data,
            val_data=val_data,
            contract_bert_model=self.contract_bert
        )
        
        # 4. Train generator
        self.clause_generator.train_model(
            train_data=train_data,
            val_data=val_data
        )
        
        # 5. Fit recommender models
        self.cf_recommender.fit(train_data)
        self.doc_sim_recommender.fit(train_data, self.contract_bert)
        
        # 6. Evaluate all approaches
        results = self.evaluate_all_approaches(test_data)
        
        # 7. Save and display results
        self._save_results(results)
        self._display_results(results)
        
        return results
    
    def evaluate_all_approaches(self, test_data: Dict) -> Dict:
        """Evaluate all approaches with proper data handling"""
        results = {
            'relevance_prediction': {},
            'clause_generation': {}
        }
        
        # 1. CF-based approach
        cf_preds = self.cf_recommender.predict_all(test_data)
        results['relevance_prediction']['cf'] = evaluate_relevance(
            cf_preds,
            test_data['labels']
        )
        
        # 2. Document similarity approach for each k
        for k in self.config['doc_sim']['k_values']:
            doc_sim_preds = self.doc_sim_recommender.predict_all(
                test_data,
                k=k,
                contract_bert_model=self.contract_bert
            )
            results['relevance_prediction'][f'doc_sim_k{k}'] = evaluate_relevance(
                doc_sim_preds,
                test_data['labels']
            )
        
        # 3. Binary classification
        classifier_preds = self.clause_classifier.predict_all(test_data)
        results['relevance_prediction']['binary_clf'] = evaluate_relevance(
            classifier_preds,
            test_data['labels']
        )
        
        # 4. Generation vs Retrieval
        generated_clauses = self.clause_generator.generate_all(test_data)
        retrieved_clauses = self.doc_sim_recommender.retrieve_clauses(
            test_data,
            self.contract_bert
        )
        
        # Evaluate both with ROUGE
        results['clause_generation']['generation'] = evaluate_generation(
            generated_clauses,
            test_data['target_clauses']
        )
        results['clause_generation']['retrieval'] = evaluate_generation(
            retrieved_clauses,
            test_data['target_clauses']
        )
        
        return results

if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    
    # Initialize and run pipeline
    pipeline = CLAUSERECPipeline("configs/clauserec_config.json")
    results = pipeline.run_pipeline()
    
    # Print final results
    print("\nFinal Results:")
    print("\nRelevance Prediction Results:")
    for method, metrics in results['relevance_prediction'].items():
        print(f"\n{method}:")
        for metric_name, value in metrics.items():
            print(f"{metric_name}: {value:.4f}")
    
    print("\nClause Generation Results:")
    for method, metrics in results['clause_generation'].items():
        print(f"\n{method}:")
        for metric_name, value in metrics.items():
            print(f"{metric_name}: {value:.4f}")
ChatGPT

Thought about CLAUSEREC pipeline adjustments for 5 seconds
Below is a final set of items to verify in your pipeline, now that you’ve implemented the recommended changes. Most of these are relatively small or occur “one level deeper” in your references. However, addressing them will help ensure your new code runs end to end without unexpected errors and truly replicates the paper’s workflow.

1. Confirm You Actually Define prepare_data()
In the last snippet, you do:

train_data, val_data, test_data = self.prepare_data()
But we don’t see the prepare_data() function defined. You either need:

def prepare_data(self):
    """
    Should:
    1. Load your LEDGARDataset
    2. dataset.load_data()
    3. train_data, val_data, test_data = dataset.create_proxy_datasets(...)
    4. Return train_data, val_data, test_data
    """
    # Example:
    dataset = LEDGARDataset(self.config['data']['ledgar_path'])
    dataset.load_data()
    train_data, val_data, test_data = dataset.create_proxy_datasets(
        train_ratio=self.config['data']['train_ratio'],
        val_ratio=self.config['data']['val_ratio']
    )
    return train_data, val_data, test_data
Or you can inline that logic inside run_pipeline() if you prefer. But as shown, you must define prepare_data() in the pipeline or you’ll get an AttributeError.

2. Ensure ContractBERTModel.train_model() Actually Accepts All Arguments
Your pipeline.run_pipeline() calls:

self.contract_bert.train_model(
    train_data=train_data,
    val_data=val_data,
    num_epochs=self.config['training']['contractbert']['num_epochs']
)
But in your ContractBERTModel, the train_model() method from earlier snippets typically wants batch_size, learning_rate, etc.

If you want to replicate the original paper’s hyperparameters, make sure you pass them in.
Example:
self.contract_bert.train_model(
    train_data=train_data,
    val_data=val_data,
    num_epochs=self.config['training']['contractbert']['num_epochs'],
    batch_size=self.config['training']['contractbert']['batch_size'],
    learning_rate=self.config['training']['contractbert']['learning_rate']
)
Similarly for the classifier and generator. Otherwise, the model might default to some hard-coded values (like batch_size = 2 or learning_rate = 5e-5), which could produce unexpected results.

3. Config Keys Must Match Your Code
You reference items like:

self.config['training']['contractbert']['num_epochs']
So in your clauserec_config.json, you must have:

{
  "training": {
    "contractbert": {
      "num_epochs": 3,
      "batch_size": 16,
      "learning_rate": 2e-5
    },
    "classifier": {
      "num_epochs": 5000,
      ...
    },
    "generator": {
      "num_epochs": 300,
      ...
    }
  },
  "data": {
    "ledgar_path": "data/ledgar/ledgar.json",
    "train_ratio": 0.6,
    "val_ratio": 0.2
  },
  ...
}
Double-check that your keys match exactly. Otherwise you’ll get KeyErrors or default to zero/None.

4. Confirm the evaluate_relevance and evaluate_generation Functions
Inside evaluate_all_approaches(), you do:

results['relevance_prediction']['cf'] = evaluate_relevance(
    cf_preds,
    test_data['labels']
)
...
results['clause_generation']['generation'] = evaluate_generation(
    generated_clauses,
    test_data['target_clauses']
)
Just make sure you’ve defined evaluate_relevance(predictions, true_labels) and evaluate_generation(predicted_clauses, true_clauses). They must:

Accept the dictionaries you pass in, e.g. {'example_id': 0_or_1} for relevance or {'example_id': "clause text"} for generation.
Return a dict that includes precision, recall, accuracy, f1, or rouge1, rouge2, rougeL, etc.
Otherwise you might get an import error or a mismatch between how your code calls them and how they’re defined.

5. Data Structures in test_data
Your code consistently references:

test_data['contracts']
test_data['labels']
test_data['target_clauses']
test_data['clause_types']
from the snippet in your create_proxy_datasets(). So confirm your final dataset keys look like this:

test_data = {
  'contracts': {...},       # {example_id: "contract text", ...}
  'labels': {...},          # {example_id: 0/1, ...}
  'target_clauses': {...},  # {example_id: "clause text that was removed", ...}
  'clause_types': {...}     # {example_id: "the clause type", ...}
}
This is crucial so that your pipeline’s calls to predict_all(), generate_all(), evaluate_relevance() and evaluate_generation() all have the data they need. If e.g. test_data is missing target_clauses, your generation metrics will fail.

6. Everything Else Looks Good!
You now define get_clause_type_embedding(), you store self.tokenizer in both ContractBERTModel and ClauseGenerator, you pass example_id from dataset to the generator’s loop—so the biggest “missing method” errors are resolved.
You freeze ContractBERT in ClauseGenerator—which matches the original paper’s statement that they do not retrain the BERT encoder for generation, only the new decoder.
Once you fix/verify the items above, your pipeline should finally run end to end without the typical “missing method,” “KeyError,” or “undefined function” issues. You’ll be able to produce the final results for “clause type relevance” (precision, recall, F1, etc.) and “clause generation” (ROUGE).

Good luck finishing the last details—you’re nearly there!




You said:
# src/pipeline.py

class CLAUSERECPipeline:
    def prepare_data(self):
        """
        Prepare datasets following paper's methodology
        """
        self.logger.info("Preparing datasets...")
        
        # Initialize dataset
        dataset = LEDGARDataset(
            data_path=self.config['data']['ledgar_path'],
            max_length=self.config['model']['max_seq_length']
        )
        
        # Load data
        dataset.load_data()
        self.logger.info(f"Loaded {len(dataset)} contracts")
        
        # Create proxy datasets
        train_data, val_data, test_data = dataset.create_proxy_datasets(
            train_ratio=self.config['data']['train_ratio'],
            val_ratio=self.config['data']['val_ratio'],
            seed=self.config['training']['seed']
        )
        
        # Log dataset statistics
        for split_name, split_data in [
            ('Train', train_data),
            ('Validation', val_data),
            ('Test', test_data)
        ]:
            self.logger.info(
                f"{split_name} set: "
                f"{len(split_data['contracts'])} examples, "
                f"{sum(split_data['labels'].values())} positive"
            )
        
        return train_data, val_data, test_data

# src/evaluation/metrics.py

def evaluate_relevance(
    predictions: Dict[str, int],
    true_labels: Dict[str, int]
) -> Dict[str, float]:
    """
    Evaluate clause type relevance predictions
    
    Args:
        predictions: Dict mapping example_ids to binary predictions (0/1)
        true_labels: Dict mapping example_ids to true labels (0/1)
    
    Returns:
        Dict with precision, recall, accuracy, and F1 score
    """
    if not predictions or not true_labels:
        return {
            'precision': 0.0,
            'recall': 0.0,
            'accuracy': 0.0,
            'f1': 0.0
        }
    
    # Convert to lists in same order
    example_ids = list(true_labels.keys())
    y_true = [true_labels[id_] for id_ in example_ids]
    y_pred = [predictions[id_] for id_ in example_ids]
    
    # Calculate metrics
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true,
        y_pred,
        average='binary',
        zero_division=0
    )
    
    accuracy = accuracy_score(y_true, y_pred)
    
    return {
        'precision': float(precision),
        'recall': float(recall),
        'accuracy': float(accuracy),
        'f1': float(f1)
    }

def evaluate_generation(
    generated_clauses: Dict[str, str],
    target_clauses: Dict[str, str]
) -> Dict[str, float]:
    """
    Evaluate clause generation using ROUGE metrics
    
    Args:
        generated_clauses: Dict mapping example_ids to generated clause text
        target_clauses: Dict mapping example_ids to target clause text
    
    Returns:
        Dict with ROUGE-1, ROUGE-2, and ROUGE-L F1 scores
    """
    if not generated_clauses or not target_clauses:
        return {
            'rouge1': 0.0,
            'rouge2': 0.0,
            'rougeL': 0.0
        }
    
    # Initialize ROUGE scorer
    scorer = rouge_scorer.RougeScorer(
        ['rouge1', 'rouge2', 'rougeL'],
        use_stemmer=True
    )
    
    # Calculate scores for each example
    scores = {
        'rouge1': [],
        'rouge2': [],
        'rougeL': []
    }
    
    for example_id in target_clauses:
        if example_id not in generated_clauses:
            continue
            
        target = target_clauses[example_id]
        generated = generated_clauses[example_id]
        
        # Skip empty clauses
        if not target.strip() or not generated.strip():
            continue
        
        # Calculate ROUGE scores
        rouge_scores = scorer.score(target, generated)
        
        # Store F1 scores
        scores['rouge1'].append(rouge_scores['rouge1'].fmeasure)
        scores['rouge2'].append(rouge_scores['rouge2'].fmeasure)
        scores['rougeL'].append(rouge_scores['rougeL'].fmeasure)
    
    # Average scores
    return {
        metric: float(np.mean(values)) if values else 0.0
        for metric, values in scores.items()
    }

# configs/clauserec_config.json
{
    "training": {
        "seed": 42,
        "contractbert": {
            "num_epochs": 3,
            "batch_size": 16,
            "learning_rate": 2e-5,
            "warmup_steps": 10000,
            "weight_decay": 0.01,
            "gradient_accumulation_steps": 1
        },
        "classifier": {
            "num_epochs": 5000,
            "batch_size": 64,
            "learning_rate": 1e-5,
            "early_stopping_patience": 5
        },
        "generator": {
            "num_epochs": 300,
            "batch_size": 8,
            "learning_rate": 1e-4,
            "teacher_forcing_ratio": 0.5
        }
    },
    "model": {
        "max_seq_length": 512,
        "hidden_size": 768,
        "num_attention_heads": 12,
        "num_hidden_layers": 12,
        "intermediate_size": 3072
    },
    "data": {
        "ledgar_path": "data/ledgar/ledgar.json",
        "train_ratio": 0.6,
        "val_ratio": 0.2,
        "cache_dir": "data/cache"
    },
    "doc_sim": {
        "k_values": [1, 3, 5]
    },
    "cf": {
        "threshold": 0.5
    }
}# src/models/contract_bert.py (train_model update)

def train_model(
    self,
    train_data: Dict,
    val_data: Dict,
    num_epochs: int,
    batch_size: int,
    learning_rate: float,
    warmup_steps: int = 10000,
    weight_decay: float = 0.01,
    gradient_accumulation_steps: int = 1
) -> None:
    """
    Train ContractBERT with additional tasks
    Using hyperparameters as specified in paper
    """
    # Create datasets
    train_dataset = ContractBERTPreTrainingDataset(
        train_data['contracts'],
        self.tokenizer,
        self.config.max_length
    )
    val_dataset = ContractBERTPreTrainingDataset(
        val_data['contracts'],
        self.tokenizer,
        self.config.max_length
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        num_workers=4
    )
    
    # Calculate total steps for warmup
    num_training_steps = len(train_loader) * num_epochs
    
    # Initialize optimizer with weight decay
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [
                p for n, p in self.named_parameters()
                if not any(nd in n for nd in no_decay)
            ],
            'weight_decay': weight_decay
        },
        {
            'params': [
                p for n, p in self.named_parameters()
                if any(nd in n for nd in no_decay)
            ],
            'weight_decay': 0.0
        }
    ]
    
    optimizer = AdamW(
        optimizer_grouped_parameters,
        lr=learning_rate
    )
    
    # Linear warmup scheduler
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=num_training_steps
    )
    
    # Training loop
    best_val_loss = float('inf')
    patience = 3
    patience_counter = 0
    
    for epoch in range(num_epochs):
        # Training
        self.train()
        total_train_loss = 0
        optimizer.zero_grad()
        
        for step, batch in enumerate(train_loader):
            # Move batch to device
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # Forward pass
            outputs = self(**batch)
            loss = outputs['loss']
            
            # Scale loss for gradient accumulation
            loss = loss / gradient_accumulation_steps
            loss.backward()
            
            # Update weights after accumulation steps
            if (step + 1) % gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(
                    self.parameters(),
                    max_norm=1.0
                )
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
            
            total_train_loss += loss.item() * gradient_accumulation_steps
        
        avg_train_loss = total_train_loss / len(train_loader)
        
        # Validation
        self.eval()
        total_val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}
                outputs = self(**batch)
                total_val_loss += outputs['loss'].item()
        
        avg_val_loss = total_val_loss / len(val_loader)
        
        # Early stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # Save best model
            self.save_pretrained('best_contractbert')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            self.logger.info(f"Early stopping triggered at epoch {epoch + 1}")
            break
        
        self.logger.info(
            f"Epoch {epoch + 1}/{num_epochs} - "
            f"Train loss: {avg_train_loss:.4f}, "
            f"Val loss: {avg_val_loss:.4f}, "
            f"LR: {scheduler.get_last_lr()[0]:.2e}"
        )
</original_plan>
<criticism>

Thought about CLAUSEREC pipeline update for a couple of seconds
Everything looks very close to being a fully functional pipeline. Below is a final audit of likely remaining minor details:

1. Confirm Imports for the New Code in train_model()
In the snippet you shared, you’re using:

optimizer = AdamW(...)
scheduler = get_linear_schedule_with_warmup(...)
Check that you have:

from transformers import AdamW, get_linear_schedule_with_warmup
imported somewhere in the file. Without these imports, you’ll get a NameError.

2. Ensure self.logger Is Defined in ContractBERTModel
You’re calling:

self.logger.info(f"Early stopping triggered at epoch {epoch + 1}")
But ContractBERTModel does not show a logger field in the snippet. In your pipeline, you do self.logger = logging.getLogger(...) in CLAUSERECPipeline. However, that pipeline logger is not automatically available to ContractBERTModel unless you pass it in or define it there, too.

Two Options
Pass logger from the pipeline into the model’s constructor:

class ContractBERTModel(BertPreTrainedModel):
    def __init__(self, config, tokenizer, logger=None):
        super().__init__(config)
        self.logger = logger or logging.getLogger(__name__)
        ...
Then in your pipeline:

self.contract_bert = ContractBERTModel(
    self.config['model'],
    self.tokenizer,
    logger=self.logger
).to(self.device)
Use a module-level logger in contract_bert.py. For example:

import logging
logger = logging.getLogger(__name__)

class ContractBERTModel(BertPreTrainedModel):
    ...
    def train_model(...):
        ...
        logger.info("Some info")
Either approach works; just be consistent so you don’t get AttributeError: 'ContractBERTModel' object has no attribute 'logger'.

3. Make Sure ContractBERTPreTrainingDataset Is Imported or Defined
The snippet references:

train_dataset = ContractBERTPreTrainingDataset(
    train_data['contracts'],
    self.tokenizer,
    self.config.max_length
)
You’ll want:

class ContractBERTPreTrainingDataset(Dataset):
    ...
Check that the dataset class is in the same file (or properly imported). If it’s in a separate file, ensure you do something like:

from .datasets import ContractBERTPreTrainingDataset
or similar. Otherwise, you’ll get NameError / ModuleNotFoundError.

4. Confirm self.logger.info(...) in prepare_data()
You’ve got:

self.logger.info("Preparing datasets...")
in prepare_data(), which implies self.logger is set up in your pipeline’s setup_logging(). Just confirm that you do something like:

def setup_logging(self):
    ...
    self.logger = logging.getLogger("CLAUSEREC")
inside the pipeline so self.logger definitely exists.

5. Final Verifications of Data Structures
You’ve got your final structure:

# train_data, val_data, test_data
{
  'contracts': {
    example_id: "<contract text>",
    ...
  },
  'labels': {
    example_id: 0|1,
    ...
  },
  'target_clauses': {
    example_id: "<clause text>",
    ...
  },
  'clause_types': {
    example_id: "<type>",
    ...
  }
}
Check that each function consistently references train_data['contracts'], train_data['labels'], etc. Make sure you never do something like train_data['examples'] by accident. It looks correct in your snippet.

6. Summary
Once you:

Add the missing imports (AdamW, get_linear_schedule_with_warmup).
Either define or remove references to self.logger in ContractBERTModel.
Ensure your datasets are properly imported (like ContractBERTPreTrainingDataset).
Confirm the rest of your pipeline references the correct dictionary keys.
…you should have a fully working end-to-end pipeline matching the paper’s approach and hyperparameters. At that point, you can run python pipeline.py (or your main script) and (hopefully!) watch everything train, evaluate, and print out those sweet final precision/recall/F1 and ROUGE metrics.

Congratulations on pulling all the pieces together!




You said:
thanks! can you please prepare an implementation on a jupytewr notebook with beautiful explanations and drawings to present it
ChatGPT

Thought about pipeline final checks for a few seconds
Below is a sample Jupyter notebook that brings together the CLAUSEREC pipeline with clear explanations and visuals (using either ASCII diagrams or mermaid syntax for flowcharts). It walks through:

Project Overview
Data Preparation
Model Architecture (ContractBERT, Classifier, Generator)
Recommenders (CF, Doc-Sim)
Training & Evaluation
Note: Some file paths and references may need to be adjusted based on your environment. Also, if you are running in a “pure” Jupyter environment, you’ll need to install the libraries used (transformers, torch, etc.) prior to running.

Optional: If you want to embed advanced visuals (e.g., mermaid diagrams) directly in Jupyter, you can install a JupyterLab extension that supports mermaid. Or you can just use ASCII diagrams.

################################################################################
# Jupyter Notebook for Demonstrating CLAUSEREC Pipeline
################################################################################

# If you're running in a fresh environment, you'll need:
# !pip install torch transformers scikit-learn rouge-score seaborn matplotlib

################################################################################
# 1. INTRO: EXPLANATION & OVERVIEW
################################################################################

"""
# CLAUSEREC: A Clause Recommendation Framework

Welcome to this notebook showcasing an end-to-end pipeline for the **CLAUSEREC** 
approach, as inspired by the paper:
> **CLAUSEREC: A Clause Recommendation Framework for AI-aided Contract Authoring**

We will demonstrate:
- How to load & preprocess the LEDGAR dataset (or a mock dataset).
- How to train a custom **ContractBERT** model (continued pretraining on extra tasks).
- How to train:
  1. A binary clause-relevance classifier,
  2. A generator for clause content,
  3. Collaborative filtering and document-similarity approaches for baseline comparisons.

Finally, we will **evaluate** the pipeline on:
- Clause type relevance (precision/recall/accuracy/F1),
- Clause content generation or retrieval (ROUGE metrics).

## High-Level Diagram

Below is a simple flowchart describing the pipeline:

```mermaid
flowchart LR
    A[Load Data / LEDGAR] --> B(Create Proxy Datasets)
    B --> C[Train ContractBERT]
    C --> D[Train Classifier + Generator]
    D --> E[CF & Doc-Sim Recommenders]
    E --> F[Evaluate 
    1. Clause Type Relevance 
    2. Clause Generation vs. Retrieval]
Key Steps:

Load/Preprocess: Convert the LEDGAR dataset into train/val/test splits, remove clauses to create “relevant” or “non-relevant” examples.
Train ContractBERT: Additional tasks: (1) Clause-word prediction, (2) Sentence-pair prediction.
Train Classifier & Generator: Classifier to predict relevance, generator to generate actual clause text.
Collaborative Filtering & Doc-Sim: Baseline approaches to recommend clauses.
Evaluation: Compare all approaches on the same test set.
"""

################################################################################

2. IMPORTS & SETUP
################################################################################

import json import logging import numpy as np import torch

For data manipulation & metrics
from typing import Dict, List from sklearn.metrics import ( precision_recall_fscore_support, accuracy_score ) from rouge_score import rouge_scorer

If you plan to produce real visuals inline:
import matplotlib.pyplot as plt import seaborn as sns

We'll assume these exist in "src" subdirectory or as local python modules
If not, you can paste them directly here in the notebook.
from src.data.ledgar_dataset import LEDGARDataset from src.models.contract_bert import ContractBERTModel from src.models.clause_classifier import ClauseRelevanceClassifier from src.models.clause_generator import ClauseGenerator from src.approaches.collaborative_filtering import CFRecommender from src.approaches.document_similarity import DocSimRecommender

We'll define or import the evaluation metrics
from src.evaluation.metrics import evaluate_relevance, evaluate_generation

For scheduling (if used in ContractBERT training)
from transformers import AdamW, get_linear_schedule_with_warmup
################################################################################

3. CONFIGURATION
################################################################################

""" We'll load a config JSON or define a dictionary inline for demo. Adjust paths to your local environment as needed. """

config = { "training": { "seed": 42, "contractbert": { "num_epochs": 3, "batch_size": 16, "learning_rate": 2e-5, "warmup_steps": 10000, "weight_decay": 0.01, "gradient_accumulation_steps": 1 }, "classifier": { "num_epochs": 5000, "batch_size": 64, "learning_rate": 1e-5, "early_stopping_patience": 5 }, "generator": { "num_epochs": 300, "batch_size": 8, "learning_rate": 1e-4, "teacher_forcing_ratio": 0.5 } }, "model": { "max_seq_length": 512, "hidden_size": 768, "num_attention_heads": 12, "num_hidden_layers": 12, "intermediate_size": 3072 }, "data": { "ledgar_path": "data/ledgar/ledgar.json", "train_ratio": 0.6, "val_ratio": 0.2, "cache_dir": "data/cache" }, "doc_sim": { "k_values": [1, 3, 5] }, "cf": { "threshold": 0.5 } }

Set random seeds
torch.manual_seed(config["training"]["seed"]) np.random.seed(config["training"]["seed"])

Logging setup
logger = logging.getLogger("CLAUSEREC_Notebook") logger.setLevel(logging.INFO) logger.addHandler(logging.StreamHandler())

logger.info("Configuration loaded and random seed set.")

################################################################################

4. DATA PREPARATION & PROXY DATASETS
################################################################################

We'll demonstrate the LEDGARDataset usage
dataset = LEDGARDataset( data_path=config['data']['ledgar_path'], max_length=config['model']['max_seq_length'], cache_dir=config['data']['cache_dir'] )

logger.info("Loading LEDGAR data...") dataset.load_data() logger.info(f"Loaded a total of {len(dataset)} contracts.")

Create train/val/test splits
train_data, val_data, test_data = dataset.create_proxy_datasets( train_ratio=config['data']['train_ratio'], val_ratio=config['data']['val_ratio'], seed=config["training"]["seed"] )

logger.info(f"Train size: {len(train_data['contracts'])}, positives={sum(train_data['labels'].values())}") logger.info(f"Val size: {len(val_data['contracts'])}, positives={sum(val_data['labels'].values())}") logger.info(f"Test size: {len(test_data['contracts'])}, positives={sum(test_data['labels'].values())}")

""" At this point, each data dictionary has keys:

'contracts': {example_id: "contract text w/ clause removed for +ve, or full for -ve"},
'labels': {example_id: 0 or 1},
'target_clauses': {example_id: "the missing clause (if +ve)"},
'clause_types': {example_id: "the type of the missing clause or negative candidate"} """
################################################################################

5. CONTRACTBERT TRAINING
################################################################################

from transformers import BertConfig, BertTokenizer

We assume ContractBERTModel is a subclass of BertPreTrainedModel
that also includes the train_model method with additional tasks (clause word, etc.)
bert_config = BertConfig( vocab_size=30522, # typical for bert-base-uncased hidden_size=config['model']['hidden_size'], num_hidden_layers=config['model']['num_hidden_layers'], num_attention_heads=config['model']['num_attention_heads'], intermediate_size=config['model']['intermediate_size'] ) bert_config.max_length = config['model']['max_seq_length']

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

contract_bert = ContractBERTModel( bert_config, tokenizer=tokenizer ).cuda() # or .to(device)

logger.info("Training ContractBERT on proxy data...")

contract_bert.train_model( train_data=train_data, val_data=val_data, num_epochs=config['training']['contractbert']['num_epochs'], batch_size=config['training']['contractbert']['batch_size'], learning_rate=config['training']['contractbert']['learning_rate'], warmup_steps=config['training']['contractbert']['warmup_steps'], weight_decay=config['training']['contractbert']['weight_decay'], gradient_accumulation_steps=config['training']['contractbert']['gradient_accumulation_steps'] )

logger.info("ContractBERT training completed.")

################################################################################

6. CLAUSE RELEVANCE CLASSIFIER
################################################################################

classifier_config = { 'input_size': 768, 'hidden_sizes': [512, 256, 128, 64, 32, 16], 'dropout_prob': 0.3 }

clause_classifier = ClauseRelevanceClassifier(classifier_config).cuda()

logger.info("Training Clause Relevance Classifier...")

clause_classifier.train_model( train_data=train_data, val_data=val_data, contract_bert_model=contract_bert, num_epochs=config['training']['classifier']['num_epochs'], batch_size=config['training']['classifier']['batch_size'], learning_rate=config['training']['classifier']['learning_rate'], device='cuda' )

logger.info("Clause Relevance Classifier training completed.")

################################################################################

7. CLAUSE GENERATOR
################################################################################

A typical approach is to create a BertConfig for the generator, or re-use.
We'll re-use the "bert_config" but add generator-specific fields.
generator_config = bert_config generator_config.num_decoder_layers = 6 generator_config.num_attention_heads = 8 generator_config.intermediate_size = 2048 generator_config.hidden_dropout_prob = 0.1 generator_config.hidden_act = "gelu"

clause_generator = ClauseGenerator( generator_config, contract_bert, tokenizer ).cuda()

logger.info("Training Clause Generator...")

clause_generator.train_model( train_data=train_data, val_data=val_data, num_epochs=config['training']['generator']['num_epochs'], batch_size=config['training']['generator']['batch_size'], learning_rate=config['training']['generator']['learning_rate'] )

logger.info("Clause Generator training completed.")

################################################################################

8. BASELINE RECOMMENDERS: CF & DOC-SIM
################################################################################

1. Collaborative Filtering
cf_recommender = CFRecommender(config['cf']) logger.info("Fitting CF recommender on train_data...") cf_recommender.fit(train_data)

2. Document Similarity
doc_sim_recommender = DocSimRecommender(config['doc_sim']) logger.info("Fitting DocSim recommender on train_data...") doc_sim_recommender.fit(train_data, contract_bert)

################################################################################

9. EVALUATION
################################################################################

logger.info("Evaluating all approaches...")

9.1 CF-based
cf_preds = cf_recommender.predict_all(test_data) cf_metrics = evaluate_relevance(cf_preds, test_data['labels'])

9.2 Doc-Sim for each k
doc_sim_results = {} for k in config['doc_sim']['k_values']: preds_k = doc_sim_recommender.predict_all(test_data, k, contract_bert) doc_sim_results[k] = evaluate_relevance(preds_k, test_data['labels'])

9.3 Binary Classification
clf_preds = clause_classifier.predict_all(test_data) clf_metrics = evaluate_relevance(clf_preds, test_data['labels'])

9.4 Clause Generation vs. Retrieval
generated = clause_generator.generate_all(test_data) retrieved = doc_sim_recommender.retrieve_clauses(test_data, contract_bert)

gen_metrics = evaluate_generation(generated, test_data['target_clauses']) ret_metrics = evaluate_generation(retrieved, test_data['target_clauses'])

logger.info("Evaluation completed.")

################################################################################

10. PRESENT RESULTS
################################################################################

print("="*80) print("CLAUSE TYPE RELEVANCE PREDICTION RESULTS") print("="*80)

print("\nCollaborative Filtering (threshold={})".format(config['cf']['threshold'])) print(cf_metrics)

for k in doc_sim_results: print(f"\nDocument Similarity (k={k})") print(doc_sim_results[k])

print("\nBinary Classifier:") print(clf_metrics)

print("\n\nCLAUSE GENERATION RESULTS (ROUGE metrics)") print("===========================================") print("\nGeneration-based approach:") print(gen_metrics)

print("\nRetrieval-based approach (DocSim):") print(ret_metrics)

""" We can also create a nice table or bar chart from these results if desired. For instance: """

import pandas as pd

relv_df = pd.DataFrame({ 'CF': cf_metrics, 'Binary_Classifier': clf_metrics, **{f'DocSim_k={k}': doc_sim_results[k] for k in doc_sim_results} }) relv_df = relv_df.transpose() relv_df

We can display it as a table in the notebook:
relv_df.style.background_gradient(cmap='Blues')

""" For generation, we only have 2 sets of metrics: Generation vs Retrieval: """

gen_df = pd.DataFrame({ 'Generation': gen_metrics, 'Retrieval': ret_metrics }) gen_df = gen_df.transpose() gen_df

That should produce a table of rouge scores in the notebook.
################################################################################

FINISHED
################################################################################

"""

Conclusion
We've now demonstrated the entire CLAUSEREC pipeline end-to-end. You should see results for:

Clause Type Relevance (Table 1 in the original paper):
CF, DocSim (k=1,3,5), Binary Classifier
Clause Generation (Table 2 in the original paper):
Generation approach vs. doc-sim retrieval
Feel free to modify the hyperparameters in the config or explore the partial generation outputs to see actual contract clauses.

Thank you for checking out this demonstration notebook! """


---

### Final Tips

1. **Adjust local file paths** (e.g., `data/ledgar/ledgar.json`) as needed.  
2. **Install** any missing dependencies:
   - `torch`, `transformers`, `scikit-learn`, `rouge-score`, `pandas`, `numpy`, `matplotlib`, `seaborn`.
3. If you want fancy **mermaid** or **Graphviz** diagrams in Jupyter, install the relevant JupyterLab extensions or use ASCII diagrams.

Enjoy your **CLAUSEREC** pipeline demonstration notebook!



You said:
prepare the explanations and maybe additional code to provide additioonal visualisations, such as view the set, the clusters etc we can use seaborn or altairs or vega
ChatGPT

Thought for a couple of seconds
Below is a sample Jupyter notebook that presents the entire CLAUSEREC pipeline with extended explanations, visualizations, and additional code snippets. We illustrate how to:

Load and preprocess the LEDGAR dataset, with a quick look at the data distribution (e.g. distribution of positive vs. negative samples).
Visualize high-dimensional embeddings (e.g. ContractBERT or Clause embeddings) using t-SNE/UMAP plus seaborn scatterplots (or Altair/vega).
Train the different components of the pipeline (ContractBERT, classifier, generator, CF & doc-sim).
Evaluate all approaches and generate multiple tables/figures to compare results.
Note: You can pick whichever visualization library you prefer—Altair, vega, or seaborn. This sample uses seaborn for embedding scatterplots and bar charts. You can adapt it for Altair or Vega by changing the relevant code.

################################################################################
# Jupyter Notebook for Demonstrating CLAUSEREC Pipeline with Advanced Visuals
################################################################################

# If running in a fresh environment, make sure to install:
# !pip install torch transformers scikit-learn rouge-score seaborn matplotlib altair

################################################################################
# 1. INTRO: EXPLANATION & OVERVIEW
################################################################################

"""
# CLAUSEREC: A Clause Recommendation Framework (Extended Demo)

This notebook demonstrates the CLAUSEREC pipeline with extra commentary and 
visualizations. We will:

1. **Load & explore** the LEDGAR dataset (or a mock dataset if you don't have LEDGAR).
2. **Create proxy datasets** for the tasks: 
   - Clause Relevance (positive/negative).
   - Clause Generation (which clause text is missing).
3. **Train**:
   - A custom ContractBERT (continued pretraining).
   - A Clause Relevance Classifier (predicting if a clause type is relevant).
   - A Clause Generator (sequence-to-sequence approach).
   - Collaborative Filtering & Document Similarity recommenders.
4. **Visualize**:
   - A small sample of embeddings from ContractBERT or from data distribution 
     using t-SNE/UMAP + seaborn scatterplots.
   - Data distribution (positive vs. negative).
5. **Evaluate** all approaches with standard metrics:
   - **Precision, recall, F1, accuracy** for relevance.
   - **ROUGE** for generation.
"""

################################################################################
# 2. IMPORTS & SETUP
################################################################################

import json
import logging
import numpy as np
import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from typing import Dict, List
from sklearn.metrics import (
    precision_recall_fscore_support, 
    accuracy_score
)
from rouge_score import rouge_scorer

# Dimensionality reduction
from sklearn.manifold import TSNE
# or from umap import UMAP  # If you prefer UMAP

# Altair if you prefer an interactive approach:
# import altair as alt

# We'll assume local modules:
from src.data.ledgar_dataset import LEDGARDataset
from src.models.contract_bert import ContractBERTModel
from src.models.clause_classifier import ClauseRelevanceClassifier
from src.models.clause_generator import ClauseGenerator
from src.approaches.collaborative_filtering import CFRecommender
from src.approaches.document_similarity import DocSimRecommender
from src.evaluation.metrics import evaluate_relevance, evaluate_generation

# For advanced scheduling (used in ContractBERT)
# from transformers import AdamW, get_linear_schedule_with_warmup

# Inline plotting
%matplotlib inline

################################################################################
# 3. CONFIGURATION
################################################################################

config = {
    "training": {
        "seed": 42,
        "contractbert": {
            "num_epochs": 3,
            "batch_size": 16,
            "learning_rate": 2e-5,
            "warmup_steps": 10000,
            "weight_decay": 0.01,
            "gradient_accumulation_steps": 1
        },
        "classifier": {
            "num_epochs": 5000,
            "batch_size": 64,
            "learning_rate": 1e-5,
            "early_stopping_patience": 5
        },
        "generator": {
            "num_epochs": 300,
            "batch_size": 8,
            "learning_rate": 1e-4,
            "teacher_forcing_ratio": 0.5
        }
    },
    "model": {
        "max_seq_length": 512,
        "hidden_size": 768,
        "num_attention_heads": 12,
        "num_hidden_layers": 12,
        "intermediate_size": 3072
    },
    "data": {
        "ledgar_path": "data/ledgar/ledgar.json",
        "train_ratio": 0.6,
        "val_ratio": 0.2,
        "cache_dir": "data/cache"
    },
    "doc_sim": {
        "k_values": [1, 3, 5]
    },
    "cf": {
        "threshold": 0.5
    }
}

# Logging
logger = logging.getLogger("CLAUSEREC_Extended")
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())

logger.info("Configuration loaded. Seeds will be set next.")

# Set random seeds
torch.manual_seed(config["training"]["seed"])
np.random.seed(config["training"]["seed"])

################################################################################
# 4. DATA PREPARATION & QUICK EXPLORATIONS
################################################################################

logger.info("Initializing LEDGARDataset...")
dataset = LEDGARDataset(
    data_path=config['data']['ledgar_path'],
    max_length=config['model']['max_seq_length'],
    cache_dir=config['data']['cache_dir']
)

logger.info("Loading data...")
dataset.load_data()
logger.info(f"Loaded {len(dataset)} contracts total.")

# Create splits
train_data, val_data, test_data = dataset.create_proxy_datasets(
    train_ratio=config['data']['train_ratio'],
    val_ratio=config['data']['val_ratio'],
    seed=config['training']['seed']
)

# Quick stats
logger.info(f"Train: {len(train_data['contracts'])} examples, "
            f"{sum(train_data['labels'].values())} positives.")
logger.info(f"Val:   {len(val_data['contracts'])} examples, "
            f"{sum(val_data['labels'].values())} positives.")
logger.info(f"Test:  {len(test_data['contracts'])} examples, "
            f"{sum(test_data['labels'].values())} positives.")

"""
We can visualize the distribution of positive vs negative examples.
Let's gather them into a small dataframe to make a simple bar chart.
"""

split_names = ["train", "val", "test"]
split_data_dicts = [train_data, val_data, test_data]
dist_records = []

for split_name, sdata in zip(split_names, split_data_dicts):
    positives = sum(sdata["labels"].values())
    negatives = len(sdata["labels"]) - positives
    dist_records.append({
        "split": split_name,
        "label": "positive",
        "count": positives
    })
    dist_records.append({
        "split": split_name,
        "label": "negative",
        "count": negatives
    })

dist_df = pd.DataFrame(dist_records)

plt.figure(figsize=(6,4))
sns.barplot(x="split", y="count", hue="label", data=dist_df)
plt.title("Distribution of Positive vs. Negative Examples by Split")
plt.show()

################################################################################
# 5. TRAINING CONTRACTBERT
################################################################################

from transformers import BertConfig, BertTokenizer

bert_config = BertConfig(
    vocab_size=30522,  # typical for bert-base-uncased
    hidden_size=config['model']['hidden_size'],
    num_hidden_layers=config['model']['num_hidden_layers'],
    num_attention_heads=config['model']['num_attention_heads'],
    intermediate_size=config['model']['intermediate_size']
)
bert_config.max_length = config['model']['max_seq_length']

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Create ContractBERT
contract_bert = ContractBERTModel(
    bert_config,
    tokenizer=tokenizer
).cuda()

logger.info("Training ContractBERT with additional tasks...")

contract_bert.train_model(
    train_data=train_data,
    val_data=val_data,
    num_epochs=config['training']['contractbert']['num_epochs'],
    batch_size=config['training']['contractbert']['batch_size'],
    learning_rate=config['training']['contractbert']['learning_rate'],
    warmup_steps=config['training']['contractbert']['warmup_steps'],
    weight_decay=config['training']['contractbert']['weight_decay'],
    gradient_accumulation_steps=config['training']['contractbert']['gradient_accumulation_steps']
)

logger.info("ContractBERT training complete.")

################################################################################
# 6. QUICK EMBEDDING VISUALIZATION (TSNE / UMAP)
################################################################################

"""
We can visualize embeddings of a small subset of contract texts or 
some sample clauses. We'll do a t-SNE on contract embeddings to see if 
positive vs negative examples cluster in some manner, or if clause types cluster.
"""

# We'll pick e.g. 200 random examples from the train_data
sample_ids = list(train_data["contracts"].keys())
np.random.shuffle(sample_ids)
sample_ids = sample_ids[:200]

embeddings = []
labels_for_plot = []

contract_bert.eval()
with torch.no_grad():
    for ex_id in sample_ids:
        text = train_data["contracts"][ex_id]
        # label = train_data["labels"][ex_id]  # or clause type
        label = train_data["clause_types"][ex_id]  # If you want to color by clause type
        
        # We can use contract_bert.get_clause_embedding(text) 
        # but note that for a long contract, you might want to average clause embeddings.
        emb = contract_bert.get_clause_embedding(text).cpu().numpy()
        
        embeddings.append(emb)
        labels_for_plot.append(label)

embeddings = np.array(embeddings)  # shape: [N, hidden_size=768 or so]

logger.info("Running TSNE on sample embeddings. This may take a minute...")
tsne = TSNE(n_components=2, random_state=config["training"]["seed"], perplexity=30)
emb_2d = tsne.fit_transform(embeddings)

# Let's visualize using seaborn scatterplot
plt.figure(figsize=(8,6))
sns.scatterplot(
    x=emb_2d[:,0], 
    y=emb_2d[:,1], 
    hue=labels_for_plot,  # if clause_type has many distinct categories, might be busy
    palette="deep", 
    legend=False
)
plt.title("t-SNE of Contract/Clause Embeddings (Colored by Clause Type)")
plt.show()

################################################################################
# 7. CLAUSE RELEVANCE CLASSIFIER TRAINING
################################################################################

classifier_config = {
    "input_size": 768,
    "hidden_sizes": [512, 256, 128, 64, 32, 16],
    "dropout_prob": 0.3
}

clause_classifier = ClauseRelevanceClassifier(classifier_config).cuda()
logger.info("Training ClauseRelevanceClassifier...")

clause_classifier.train_model(
    train_data=train_data,
    val_data=val_data,
    contract_bert_model=contract_bert,
    num_epochs=config['training']['classifier']['num_epochs'],
    batch_size=config['training']['classifier']['batch_size'],
    learning_rate=config['training']['classifier']['learning_rate'],
    device='cuda'
)

logger.info("Classifier training complete.")

################################################################################
# 8. CLAUSE GENERATOR TRAINING
################################################################################

gen_config = bert_config
gen_config.num_decoder_layers = 6
gen_config.num_attention_heads = 8
gen_config.intermediate_size = 2048
gen_config.hidden_dropout_prob = 0.1
gen_config.hidden_act = "gelu"

clause_generator = ClauseGenerator(
    gen_config,
    contract_bert,
    tokenizer
).cuda()

logger.info("Training ClauseGenerator...")

clause_generator.train_model(
    train_data=train_data,
    val_data=val_data,
    num_epochs=config['training']['generator']['num_epochs'],
    batch_size=config['training']['generator']['batch_size'],
    learning_rate=config['training']['generator']['learning_rate']
)

logger.info("Generator training complete.")

################################################################################
# 9. BASELINE RECOMMENDERS: CF & DOC-SIM
################################################################################

# CF
cf_recommender = CFRecommender(config['cf'])
logger.info("Fitting CFRecommender on train_data...")
cf_recommender.fit(train_data)

# Doc-Sim
doc_sim_recommender = DocSimRecommender(config['doc_sim'])
logger.info("Fitting DocSimRecommender on train_data...")
doc_sim_recommender.fit(train_data, contract_bert)

################################################################################
# 10. EVALUATION
################################################################################

logger.info("Evaluating approaches on test_data...")

# 10.1 CF predictions
cf_preds = cf_recommender.predict_all(test_data)
cf_metrics = evaluate_relevance(cf_preds, test_data["labels"])

# 10.2 Doc-Sim predictions (k=1,3,5)
doc_sim_metrics = {}
for k in config["doc_sim"]["k_values"]:
    preds_k = doc_sim_recommender.predict_all(
        test_data, k, contract_bert
    )
    doc_sim_metrics[k] = evaluate_relevance(preds_k, test_data["labels"])

# 10.3 Binary classifier
clf_preds = clause_classifier.predict_all(test_data)
clf_metrics = evaluate_relevance(clf_preds, test_data["labels"])

# 10.4 Clause Generation vs. Retrieval
generated_clauses = clause_generator.generate_all(test_data)
retrieved_clauses = doc_sim_recommender.retrieve_clauses(
    test_data, contract_bert
)

gen_eval = evaluate_generation(generated_clauses, test_data["target_clauses"])
ret_eval = evaluate_generation(retrieved_clauses, test_data["target_clauses"])

logger.info("All evaluations done!")

################################################################################
# 11. DISPLAY RESULTS
################################################################################

print("\n================ CLAUSE TYPE RELEVANCE PREDICTION ================")
print("CF Recommender:", cf_metrics)
for k, metrics_k in doc_sim_metrics.items():
    print(f"Doc-Sim (k={k}):", metrics_k)
print("Binary Classifier:", clf_metrics)

print("\n================ CLAUSE GENERATION (ROUGE) ========================")
print("Generation-based:", gen_eval)
print("Retrieval-based:", ret_eval)

# Create dataframes
relv_df = pd.DataFrame({
    "CF": cf_metrics,
    "Binary_Classifier": clf_metrics,
    **{f"DocSim_k={k}": doc_sim_metrics[k] for k in doc_sim_metrics}
})
relv_df = relv_df.transpose()  # each row is a method, columns are precision, recall, etc.

print("\nRelevance Results as DataFrame:")
display(relv_df)

gen_df = pd.DataFrame({
    "Generation": gen_eval,
    "Retrieval": ret_eval
}).transpose()
print("\nGeneration Results as DataFrame:")
display(gen_df)

"""
Optionally, we can do a little bar chart comparing methods for precision, recall, 
F1, etc. For example:
"""

relv_plot_df = relv_df[["precision", "recall", "f1", "accuracy"]].reset_index().rename(columns={"index": "method"})
melted = relv_plot_df.melt(id_vars=["method"], var_name="metric", value_name="score")

plt.figure(figsize=(8,5))
sns.barplot(x="metric", y="score", hue="method", data=melted)
plt.title("Comparison of Clause Type Relevance Methods")
plt.legend(bbox_to_anchor=(1.05,1), loc="upper left")
plt.show()

"""
Similarly, for generation:
"""

gen_plot_df = gen_df[["rouge1", "rouge2", "rougeL"]].reset_index().rename(columns={"index": "method"})
gen_melted = gen_plot_df.melt(id_vars=["method"], var_name="metric", value_name="score")

plt.figure(figsize=(6,4))
sns.barplot(x="metric", y="score", hue="method", data=gen_melted)
plt.title("Generation vs. Retrieval (ROUGE metrics)")
plt.legend(bbox_to_anchor=(1.05,1), loc="upper left")
plt.show()

"""
And that's it! 
We've shown how to visualize data distribution, produce t-SNE plots for embeddings, 
train all models, and compare final metrics with bar charts. 
Feel free to adapt with altair or vega for interactive plotting.
"""

Summary of the Extended Notebook
Initialization/Explanations: A thorough Markdown introduction.
Data Loading & Preprocessing: We show how to load LEDGAR (or a mock dataset) into train/val/test using the create_proxy_datasets() approach.
Data Distribution Plot: Using a simple bar chart to compare positive vs. negative examples for each split.
t-SNE Embedding Visualization: Create a small sample (e.g. 200 examples), get embeddings from ContractBERTModel.get_clause_embedding(), and do a scatterplot with seaborn.
You can color by label or clause type.
Alternatively, use Altair for interactive plots (you’d convert the emb_2d array to a pandas.DataFrame and call alt.Chart(...)).
Model Training: ContractBERT, classifier, generator, plus CF & doc-sim fitting.
Evaluation:
Relevance: evaluate_relevance(preds, true_labels).
Generation: evaluate_generation(generated_clauses, target_clauses).
Results:
Printed side-by-side.
DataFrames with a bar chart for easy visual comparison.
With these steps, you have a comprehensive, visually rich notebook that demonstrates the entire CLAUSEREC pipeline and stands ready for presentation. Feel free to tailor the number of epochs, batch size, or perplexity for t-SNE so it runs quickly in a demonstration environment. Have fun customizing your visual approach—Altair or vega can produce more interactive plots if you prefer!




You said:
provide additional cells to add the extra altair/vega itneractive drawings
ChatGPT

Thought for a second
Below is a sample Jupyter notebook outline that includes additional cells for Altair (which uses Vega-Lite under the hood) to produce interactive visualizations. You can pick and choose which parts you want to incorporate in your final notebook.

Note: To run Altair in a Jupyter environment, ensure you have installed:

pip install altair vega_datasets
If you use JupyterLab, you may also need the jupyterlab and jupyterlab_voyager extensions or a modern version of JupyterLab that supports Altair automatically.

This example shows how to create interactive scatterplots (for embeddings) and bar charts (for metrics) using Altair. You can adapt it for your environment.

################################################################################
# 0. Setup: You must ensure the following packages are installed:
#    !pip install altair vega_datasets
#    If using JupyterLab, you might need:
#    jupyter labextension install @jupyter-widgets/jupyterlab-manager vega-lite
# 
#    Then "import altair" in your notebook cells.
################################################################################

import altair as alt
import pandas as pd
import numpy as np

################################################################################
# Additional Cell: Interactive t-SNE (or UMAP) Visualization with Altair
################################################################################

"""
Here we'll assume you have a sample of embeddings (in 'embeddings') 
and corresponding labels (in 'labels_for_plot'), each length N=200 (for example).
We'll turn them into a DataFrame and use an Altair scatterplot with 
tooltips, zoom, pan, etc.
"""

# Suppose from earlier, we have emb_2d (shape [N,2]) and labels_for_plot (length N).
# We'll place them in a DataFrame:
emb_data = pd.DataFrame({
    'x': emb_2d[:, 0],
    'y': emb_2d[:, 1],
    'label': labels_for_plot
})

# Build an interactive chart
tsne_chart = alt.Chart(emb_data).mark_circle(size=60).encode(
    x=alt.X('x', axis=alt.Axis(title='t-SNE Dimension 1')),
    y=alt.Y('y', axis=alt.Axis(title='t-SNE Dimension 2')),
    color=alt.Color('label', legend=alt.Legend(title='Clause Type')),
    tooltip=['label', 'x', 'y']
).properties(
    width=600,
    height=400,
    title='t-SNE Plot of Clause/Contract Embeddings (Altair Interactive)'
).interactive()  # This enables zooming and panning

tsne_chart

################################################################################
# Additional Cell: Interactive Bar Chart for Relevance Metrics with Altair
################################################################################

"""
We have a DataFrame 'relv_df' storing metrics for each approach. 
Typically shaped like:
    index = [ 'CF', 'Binary_Classifier', 'DocSim_k=1', 'DocSim_k=3', 'DocSim_k=5', ... ]
    columns = [ 'precision', 'recall', 'f1', 'accuracy' ]

We can melt this DataFrame into a long format for Altair.
"""

relv_df_reset = relv_df.reset_index().rename(columns={'index': 'method'})
relv_long = relv_df_reset.melt(
    id_vars='method',
    var_name='metric',
    value_name='score'
)

# Build interactive bar chart
relevance_chart = alt.Chart(relv_long).mark_bar().encode(
    x=alt.X('metric:N', sort=['precision', 'recall', 'f1', 'accuracy']),
    y=alt.Y('score:Q'),
    color=alt.Color('method:N', legend=alt.Legend(title='Approach')),
    column='method:N',
    tooltip=['method', 'metric', 'score']
).properties(
    width=150,
    height=300,
    title='Clause Type Relevance: Comparison of Approaches'
).interactive()  # Allows zoom/pan on the x/y axes

relevance_chart

################################################################################
# Additional Cell: Interactive Bar Chart for Generation (ROUGE) with Altair
################################################################################

"""
We'll do the same with 'gen_df', shaped like:
     index = [ 'Generation', 'Retrieval' ]
     columns = [ 'rouge1', 'rouge2', 'rougeL' ]
"""

gen_df_reset = gen_df.reset_index().rename(columns={'index': 'method'})
gen_long = gen_df_reset.melt(
    id_vars='method',
    var_name='metric',
    value_name='score'
)

gen_chart = alt.Chart(gen_long).mark_bar().encode(
    x=alt.X('metric:N', sort=['rouge1', 'rouge2', 'rougeL']),
    y=alt.Y('score:Q'),
    color=alt.Color('method:N', legend=alt.Legend(title='Approach')),
    column='method:N',
    tooltip=['method', 'metric', 'score']
).properties(
    width=150,
    height=300,
    title='Clause Generation vs. Retrieval (ROUGE Scores)'
).interactive()

gen_chart
Explanation of the Additional Cells:
t-SNE / UMAP Visualization

We assume you already computed two-dimensional embeddings emb_2d from your high-dimensional (e.g., 768-d) embeddings.
Altair can produce an interactive scatterplot (zoom, pan, tooltips).
In the snippet, we create a Pandas DataFrame named emb_data with columns [x, y, label] and pass it to alt.Chart(...).
Interactive Bar Chart for Relevance Metrics

We take the relv_df DataFrame (which might look like this when printed:
precision    recall     f1        accuracy
CF               0.70       0.80   0.74         0.75
DocSim_k=1       0.75       0.72   0.73         0.74
...
We “melt” the DataFrame to get a “long format” (one row per (method, metric) pair). This allows Altair to easily produce a grouped bar chart.
We add .interactive() to allow panning/zooming. The column='method:N' in the chart is one way to separate the bars by approach. Alternatively, you could place them on the x-axis next to metric and color them.
Interactive Bar Chart for Generation (ROUGE)

Similar approach as the relevance metrics, but using gen_df with columns [rouge1, rouge2, rougeL].
We generate a comparison between “Generation” and “Retrieval” methods.
Customization

You can tweak the size, color, sorting, or interactive features as you like.
You might prefer facet or a single chart with a legend. For instance, if you prefer a single bar chart (not columns), remove column='method:N', and use something like:
alt.Chart(...).mark_bar().encode(
    x=alt.X('method:N'),
    y=alt.Y('score:Q'),
    color='method:N',
    column='metric:N'
)
For tooltips, you can add more columns if you want to show additional details.
With these extra cells, you can incorporate Altair/Vega into your CLAUSEREC demonstration notebook, providing interactive visual analysis of embeddings, metrics, or any other numeric data you’d like to explore.




You said:
add additional explkanations for audience of lawyers, make it interesting
ChatGPT

Thought for a second
Below is a sample Jupyter Notebook outline that includes additional explanations geared toward legal professionals (lawyers) who may not have a strong technical background in machine learning or natural language processing. The extra commentary aims to make the CLAUSEREC pipeline more approachable and interesting for an audience that deals regularly with contracts.

In particular, we:

Explain what “ContractBERT,” “clause embedding,” and “relevance prediction” mean in plain legal terms.
Highlight how interactive visualizations (via Altair/Vega) can help lawyers see how the system distinguishes among clause types or identifies missing clauses for a given contract.
Use real-world analogies where possible (e.g., “collaborative filtering” reminiscent of online shopping suggestions).
################################################################################
# Jupyter Notebook: CLAUSEREC Pipeline Demonstration for Legal Professionals
################################################################################

"""
## Introduction for Lawyers

### Why Automated Clause Recommendation?

Contracts often include **boilerplate clauses** (like Governing Law, Notices, 
or Severability) which are crucial for legal enforceability. However, drafting 
a contract from scratch—or modifying an existing template—can be **time-consuming** 
and **error-prone**.

**Clause Recommendation** uses AI techniques to:
1. Suggest which **types** of clauses are “likely relevant” to a given contract (like a share purchase agreement that might be missing a “Confidentiality” clause).
2. **Generate** the text of these clauses, or **retrieve** them from a clause library, so the lawyer can review and finalize them quickly.

### How Does CLAUSEREC Work?

- **ContractBERT**: A specialized language model trained on contract text. 
  *BERT* (Bidirectional Encoder Representations from Transformers) is a 
  state-of-the-art model that learns nuances of language. When extended to 
  legal documents, we call it **ContractBERT**.
- **Clause Relevance Classifier**: Determines if a given clause type is relevant 
  for a contract, even if it’s missing.
- **Clause Generator**: Drafts suggested text for a missing clause, 
  referencing the rest of the contract to maintain consistency in style 
  and substance.
- **Collaborative Filtering & Document Similarity**: 
  - *Collaborative Filtering* is an AI technique reminiscent of e-commerce 
    “people who bought this also bought that.” Here, it’s “Contracts that had 
    Clause X often also had Clause Y.”
  - *Document Similarity* finds similar contracts (like how you might 
    compare a new contract to your library of past agreements) and checks 
    if they used a certain clause type.

By exploring the results and interactive charts below, you’ll see how AI can 
support contract drafting, which can save you time, reduce missed clauses, 
and help ensure consistent language across your deals or client matters.

---
"""

################################################################################
# 1. Imports, Setup & Logging
################################################################################

import json
import logging
import numpy as np
import torch
import pandas as pd
import altair as alt
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.manifold import TSNE
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from rouge_score import rouge_scorer

# Our local modules for this demonstration
from src.data.ledgar_dataset import LEDGARDataset
from src.models.contract_bert import ContractBERTModel
from src.models.clause_classifier import ClauseRelevanceClassifier
from src.models.clause_generator import ClauseGenerator
from src.approaches.collaborative_filtering import CFRecommender
from src.approaches.document_similarity import DocSimRecommender
from src.evaluation.metrics import evaluate_relevance, evaluate_generation

# Make sure your environment supports Altair interactions
# e.g. jupyter labextension install @jupyter-widgets/jupyterlab-manager

%matplotlib inline

# Logging setup
logger = logging.getLogger("CLAUSEREC_For_Lawyers")
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())

logger.info("Notebook ready: We will demonstrate the clause recommendation pipeline.")

################################################################################
# 2. Configuration
################################################################################

config = {
    "training": {
        "seed": 42,
        "contractbert": {
            "num_epochs": 3,
            "batch_size": 16,
            "learning_rate": 2e-5,
            "warmup_steps": 10000,
            "weight_decay": 0.01,
            "gradient_accumulation_steps": 1
        },
        "classifier": {
            "num_epochs": 5000,
            "batch_size": 64,
            "learning_rate": 1e-5,
            "early_stopping_patience": 5
        },
        "generator": {
            "num_epochs": 300,
            "batch_size": 8,
            "learning_rate": 1e-4,
            "teacher_forcing_ratio": 0.5
        }
    },
    "model": {
        "max_seq_length": 512,
        "hidden_size": 768,
        "num_attention_heads": 12,
        "num_hidden_layers": 12,
        "intermediate_size": 3072
    },
    "data": {
        "ledgar_path": "data/ledgar/ledgar.json",
        "train_ratio": 0.6,
        "val_ratio": 0.2,
        "cache_dir": "data/cache"
    },
    "doc_sim": {
        "k_values": [1, 3, 5]
    },
    "cf": {
        "threshold": 0.5
    }
}

# Ensure reproducibility
np.random.seed(config["training"]["seed"])
torch.manual_seed(config["training"]["seed"])

################################################################################
# 3. Data: LEDGAR Contracts & Proxy Datasets
################################################################################

"""
We load the LEDGAR dataset, which is a large collection of publicly available 
contracts, typically from the SEC's EDGAR system. For privacy reasons or 
time constraints, we may limit to a subset.

**Key Idea**: We create "proxy" examples of missing clauses by artificially 
removing them from certain contracts. That way, we can train the AI to detect 
that a contract is "missing a relevant clause."
"""

dataset = LEDGARDataset(
    data_path=config['data']['ledgar_path'],
    max_length=config['model']['max_seq_length'],
    cache_dir=config['data']['cache_dir']
)

logger.info("Loading LEDGAR data (this may take a moment)...")
dataset.load_data()
logger.info(f"Loaded {len(dataset)} contracts total in dataset.")

# Split into train, val, test
train_data, val_data, test_data = dataset.create_proxy_datasets(
    train_ratio=config['data']['train_ratio'],
    val_ratio=config['data']['val_ratio'],
    seed=config["training"]["seed"]
)

logger.info(f"Train: {len(train_data['contracts'])} examples.")
logger.info(f"Val:   {len(val_data['contracts'])} examples.")
logger.info(f"Test:  {len(test_data['contracts'])} examples.")

# Quick bar chart using seaborn
split_names = ["train", "val", "test"]
split_data = [train_data, val_data, test_data]
dist_list = []

for sname, sdata in zip(split_names, split_data):
    total_examples = len(sdata["contracts"])
    positives = sum(sdata["labels"].values())
    dist_list.append({"split": sname, "label": "positive", "count": positives})
    dist_list.append({"split": sname, "label": "negative", "count": total_examples - positives})

dist_df = pd.DataFrame(dist_list)
plt.figure(figsize=(6,4))
sns.barplot(x="split", y="count", hue="label", data=dist_df)
plt.title("Distribution of Positive vs Negative Clause 'Relevance' Examples")
plt.show()

################################################################################
# 4. Training Our ContractBERT
################################################################################

"""
**Why does a lawyer care about ContractBERT?**

This specialized model "understands" contract language:
- Phrases like "This agreement shall be governed by the laws of Delaware" 
  are recognized as "Governing Law" clauses.
- Repetitive legal text becomes more efficiently encoded.

We first do some "continued pretraining" on additional tasks:
1. Clause-word prediction (detect which tokens belong to a specific clause).
2. Sentence-pair prediction (determine if two sentences belong to the same clause).

These tasks help the model cluster similar clauses together, 
resulting in more accurate recognition and generation downstream.
"""

from transformers import BertConfig, BertTokenizer

bert_config = BertConfig(
    vocab_size=30522,
    hidden_size=config['model']['hidden_size'],
    num_hidden_layers=config['model']['num_hidden_layers'],
    num_attention_heads=config['model']['num_attention_heads'],
    intermediate_size=config['model']['intermediate_size']
)
bert_config.max_length = config['model']['max_seq_length']

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

contract_bert = ContractBERTModel(
    bert_config,
    tokenizer=tokenizer
).cuda()

logger.info("Training ContractBERT on these proxy data sets...")

contract_bert.train_model(
    train_data=train_data,
    val_data=val_data,
    num_epochs=config['training']['contractbert']['num_epochs'],
    batch_size=config['training']['contractbert']['batch_size'],
    learning_rate=config['training']['contractbert']['learning_rate'],
    warmup_steps=config['training']['contractbert']['warmup_steps'],
    weight_decay=config['training']['contractbert']['weight_decay'],
    gradient_accumulation_steps=config['training']['contractbert']['gradient_accumulation_steps']
)

logger.info("Finished training ContractBERT.")

################################################################################
# 5. Quick Embedding Visualization with Altair (Interactive!)
################################################################################

"""
Lawyers might wonder, "Is the AI truly distinguishing among different types of 
clauses or is it random?" Let's do a t-SNE on a sample of embeddings from our 
freshly trained model. Then we color by the clause type (if known).
"""

from sklearn.manifold import TSNE

# Sample some example_ids from train_data
sample_ids = list(train_data["contracts"].keys())
np.random.shuffle(sample_ids)
sample_ids = sample_ids[:200]  # pick 200 for a small demonstration

embeddings = []
clause_labels = []

contract_bert.eval()
with torch.no_grad():
    for ex_id in sample_ids:
        text = train_data["contracts"][ex_id]
        ctype = train_data["clause_types"][ex_id]  # e.g., 'Governing Law'

        # For short demonstration, embed the entire text (though it's truncated to 512).
        emb = contract_bert.get_clause_embedding(text).cpu().numpy()

        embeddings.append(emb)
        clause_labels.append(ctype)

embeddings = np.array(embeddings)  # shape [200, 768] maybe

logger.info("Running t-SNE on 200 sample embeddings. This can take time, please wait...")
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
emb_2d = tsne.fit_transform(embeddings)

# Create a DataFrame for Altair
tsne_df = pd.DataFrame({
    "x": emb_2d[:,0],
    "y": emb_2d[:,1],
    "clause_type": clause_labels
})

import altair as alt

tsne_chart = alt.Chart(tsne_df).mark_circle(size=60).encode(
    x=alt.X('x', axis=alt.Axis(title='t-SNE Dim 1')),
    y=alt.Y('y', axis=alt.Axis(title='t-SNE Dim 2')),
    color=alt.Color('clause_type', legend=alt.Legend(title='Clause Type')),
    tooltip=['clause_type', 'x', 'y']
).properties(
    width=600,
    height=400,
    title='t-SNE Embeddings (Interactive)'
).interactive()

tsne_chart

"""
Hover over points to see the clause type. 
If the model is learning well, similar clause types should cluster somewhat, 
though real contracts can be quite varied and we only have 200 samples here.
"""

################################################################################
# 6. Clause Relevance Classifier
################################################################################

"""
**"Is Clause X relevant to this contract?"** 
This is akin to a lawyer noticing that a template 
doesn't have the typical "Confidentiality" clause in an NDA. 
We train a 7-layer neural network that uses the ContractBERT embeddings 
to decide "Yes/No" on each missing clause type.
"""

classifier_config = {
    "input_size": 768,
    "hidden_sizes": [512, 256, 128, 64, 32, 16],
    "dropout_prob": 0.3
}

clause_classifier = ClauseRelevanceClassifier(classifier_config).cuda()

logger.info("Training Clause Relevance Classifier...")

clause_classifier.train_model(
    train_data=train_data,
    val_data=val_data,
    contract_bert_model=contract_bert,
    num_epochs=config['training']['classifier']['num_epochs'],
    batch_size=config['training']['classifier']['batch_size'],
    learning_rate=config['training']['classifier']['learning_rate'],
    device='cuda'
)

logger.info("Classifier training complete.")

################################################################################
# 7. Clause Generator
################################################################################

"""
This is the "magic wand" from a lawyer's perspective: 
**the model can propose actual text** for a missing clause. 
We train a sequence-to-sequence approach that uses:
- ContractBERT embeddings for the contract context,
- A Transformer decoder that tries to generate a plausible clause text.

Lawyers can then edit or accept that suggestion.
"""

gen_config = bert_config
gen_config.num_decoder_layers = 6
gen_config.num_attention_heads = 8
gen_config.intermediate_size = 2048
gen_config.hidden_dropout_prob = 0.1
gen_config.hidden_act = "gelu"

clause_generator = ClauseGenerator(
    gen_config,
    contract_bert,
    tokenizer
).cuda()

logger.info("Training Clause Generator...")

clause_generator.train_model(
    train_data=train_data,
    val_data=val_data,
    num_epochs=config['training']['generator']['num_epochs'],
    batch_size=config['training']['generator']['batch_size'],
    learning_rate=config['training']['generator']['learning_rate']
)

logger.info("Generator training complete.")

################################################################################
# 8. Baseline Approaches: Collaborative Filtering & Document Similarity
################################################################################

"""
**Collaborative Filtering**: If many 'similar' contracts used clause type X, 
the new contract might want it too. 
Analogy: "People who watch the 'Star Wars' movies also watch 'Star Trek'!"

**Document Similarity**: Find the closest contracts. If they used a certain 
clause type, propose it for the new contract.
"""

cf_recommender = CFRecommender(config['cf'])
logger.info("Fitting Collaborative Filtering on training data...")
cf_recommender.fit(train_data)

doc_sim_recommender = DocSimRecommender(config['doc_sim'])
logger.info("Fitting Document Similarity Recommender on training data...")
doc_sim_recommender.fit(train_data, contract_bert)

################################################################################
# 9. Evaluation
################################################################################

logger.info("Evaluating all approaches on the test set...")

# 9.1 CF approach
cf_preds = cf_recommender.predict_all(test_data)
cf_metrics = evaluate_relevance(cf_preds, test_data["labels"])

# 9.2 Doc-sim for each k
doc_sim_results = {}
for k in config["doc_sim"]["k_values"]:
    ds_preds = doc_sim_recommender.predict_all(test_data, k, contract_bert)
    doc_sim_results[k] = evaluate_relevance(ds_preds, test_data["labels"])

# 9.3 Classifier
clf_preds = clause_classifier.predict_all(test_data)
clf_metrics = evaluate_relevance(clf_preds, test_data["labels"])

# 9.4 Clause Generation vs. Retrieval
generated_clauses = clause_generator.generate_all(test_data)
retrieved_clauses = doc_sim_recommender.retrieve_clauses(test_data, contract_bert)

gen_eval = evaluate_generation(generated_clauses, test_data["target_clauses"])
ret_eval = evaluate_generation(retrieved_clauses, test_data["target_clauses"])

logger.info("Done evaluating everything!")

################################################################################
# 10. Presenting the Results
################################################################################

print("\n========== CLAUSE TYPE RELEVANCE PREDICTION ===========")
print("CF (Collaborative Filtering):", cf_metrics)
for k, metrics_k in doc_sim_results.items():
    print(f"Document Similarity (k={k}):", metrics_k)
print("Binary Classifier:", clf_metrics)

print("\n========== CLAUSE CONTENT GENERATION (ROUGE) ==========")
print("Generation-based:", gen_eval)
print("Retrieval-based:", ret_eval)

# Make a DataFrame of relevance results
relv_df = pd.DataFrame({
    "CF": cf_metrics,
    "Binary_Classifier": clf_metrics,
    **{f"DocSim_k={k}": doc_sim_results[k] for k in doc_sim_results}
}).transpose()

print("\n**Relevance Results**:")
relv_df

# Make a DataFrame for generation
gen_df = pd.DataFrame({
    "Generation": gen_eval,
    "Retrieval": ret_eval
}).transpose()

print("\n**Generation Results**:")
gen_df

################################################################################
# 11. Interactive Bar Charts with Altair for a Lawyer-Friendly Dashboard
################################################################################

relv_reset = relv_df.reset_index().rename(columns={"index": "method"})
relv_melted = relv_reset.melt(
    id_vars='method',
    var_name='metric',
    value_name='score'
)

# Bar chart for relevance metrics
relevance_chart = alt.Chart(relv_melted).mark_bar().encode(
    x=alt.X('metric:N', sort=['precision', 'recall', 'f1', 'accuracy']),
    y=alt.Y('score:Q'),
    color=alt.Color('method:N', legend=alt.Legend(title='Approach')),
    column='method:N', 
    tooltip=['method', 'metric', 'score']
).properties(
    width=150,
    height=300,
    title='Clause Type Relevance: Methods Comparison'
).interactive()

relevance_chart

"""
Interpretation for lawyers:
- CF or doc-sim might have *higher recall* (rarely misses a clause) but lower 
  precision (sometimes suggests irrelevant clauses).
- The binary classifier might have a more balanced precision/recall, 
  leading to a better F1 score overall.
"""

# Generation vs. retrieval
gen_reset = gen_df.reset_index().rename(columns={"index": "method"})
gen_melted = gen_reset.melt(
    id_vars='method',
    var_name='metric',
    value_name='score'
)

gen_chart = alt.Chart(gen_melted).mark_bar().encode(
    x=alt.X('metric:N', sort=['rouge1','rouge2','rougeL']),
    y=alt.Y('score:Q'),
    color=alt.Color('method:N', legend=alt.Legend(title='Approach')),
    column='method:N',
    tooltip=['method','metric','score']
).properties(
    width=150,
    height=300,
    title='Clause Generation vs. Retrieval: ROUGE Metrics'
).interactive()

gen_chart

"""
Interpretation for lawyers:
- "ROUGE" is a measure of textual similarity. 
- If "Generation" has higher ROUGE, that means the AI's *draft* is closer 
  to the ground-truth clause text that was originally in the contract.
- "Retrieval" is a simpler approach, grabbing an existing clause from 
  a 'clause library'. This might be safer but not always as context-aware.
"""

"""
# Conclusion

Our AI pipeline suggests which clauses might be relevant—and can even draft 
their content for you to review. We can see from these interactive charts 
how each approach measures up. 
In real practice, a law firm or legal department can integrate this system 
into their contract authoring workflow, 
thus ensuring no crucial clauses are missed and 
streamlining the drafting process for standard provisions.
"""
Key Lawyer-Friendly Points in This Notebook
Plain English descriptions of each model (ContractBERT, classifier, generator), tying them to everyday contract authoring tasks.
Analogies: For instance, collaborative filtering like “people who watched/streamed this also watched...,” but for “Contracts that had X also had Y.”
Interactive Embeddings: A t-SNE visualization that lawyers can hover over and see clause types. This helps them see that “Governing Law” clauses cluster separately from “Confidentiality” clauses (if data is robust).
Clearly labeled metrics: “Precision,” “Recall,” “F1,” with short lawyer-friendly clarifications:
Precision: Of all the times we predicted a clause was relevant, how often were we correct?
Recall: Of all the times a clause was truly relevant, how often did we catch it? (i.e., no missed clauses)
F1: The balance between them.
ROUGE: A measure of how similar our generated text is to the real (gold) text.
Conclusion: Real-world usage scenario where the system is integrated as a drafting assistant, with the lawyer always reviewing the suggestions.
Feel free to expand with your own anecdotes or more robust data points (like graphs of how performance improves with more training data or how often the system proposes certain clause types). The above code snippet provides a skeleton for a fully interactive, lawyer-friendly CLAUSEREC demonstration.




You said:
dont need to provide everything again, add the difference of chat gpt (which lawyers know) and bert and explasionm the choices of the authors for metjhodology in plain english for lawyers\
ChatGPT

**Thought about presenting CLAUSEREC to lawyers** for 6 seconds
Below is a short, lawyer-friendly addendum that clarifies:

How ChatGPT differs from BERT and why this paper’s authors opted for BERT-based methods instead of (or in addition to) something like ChatGPT.
The authors’ methodological choices in plain English, focusing on why they structured the system as they did.
How is ChatGPT Different from BERT?
Lawyers may have seen ChatGPT (or similar tools) respond with entire paragraphs of legal text, draft emails, or summarize complex documents. In short:

ChatGPT is a large, general-purpose “chatbot” powered by a language model in the GPT family. It’s trained to respond in a conversational manner, generating entire passages of text and handling follow-up questions. However, it’s not deeply specialized to legal tasks out of the box—it’s more of a “jack of all trades” that can generate text on almost any topic.

BERT (short for Bidirectional Encoder Representations from Transformers) is another type of AI model that is highly popular for analytical tasks like classification, finding similarities, or extracting key information.

BERT does not typically generate long, freeform text in the same way ChatGPT does. Instead, it’s optimized to produce detailed “embeddings” or numerical representations of text.
Because BERT can be fine-tuned on specific tasks—like classifying if a clause is relevant or grouping similar clauses—it’s extremely powerful for step-by-step legal analysis within a pipeline.
In a nutshell:

ChatGPT is great for broad, flexible conversation or free-text drafting.
BERT is great for internal, specialized tasks like “Is this clause relevant or not?” or “How similar is this clause to my library of standard clauses?”
Why the Paper’s Authors Chose BERT (and “ContractBERT”)
1. Focusing on Step-by-Step Contract Analysis
The paper aims to break contract drafting into two major stages:

Deciding which clause types are missing (that is, “Is a ‘Governing Law’ clause needed here?”).
Generating or retrieving the actual text for that missing clause.
They modified BERT into a specialized version called ContractBERT by continuing the training on lots of contract data. That helps the model “understand” typical language patterns used in legal clauses—for instance, the repeated phrase “governed by and construed in accordance with the laws of…”.

2. Control Over Fine-Tuning and Domain Adaptation
Because BERT is open, smaller, and more customizable than ChatGPT, the authors can:

Integrate BERT with specific tasks (e.g., “clause-word prediction,” “sentence-pair prediction”).
Keep data in-house without sending large amounts of confidential text to an external API (which might be required if using ChatGPT in the cloud).
Achieve robust, consistent “embedding” of contract text, easily feeding these embeddings into a classifier for relevance or a transformer decoder for clause generation.
3. Pipeline Approach
They also wanted multiple ways to recommend clauses (Collaborative Filtering, Document Similarity, or BERT-based Classifier/Generator) and to compare them with standard metrics (precision, recall, F1, ROUGE). A BERT-based approach is easier to break into subcomponents than a purely end-to-end ChatGPT approach.

4. Legal-Specific Gains
Finally, by pretraining BERT on a large corpus of contracts (turning it into “ContractBERT”), they ensure it clusters or represents legally relevant phrases more effectively. Lawyers often prefer this specialized focus so the model better captures the nuance of repeated legal structures.

Methodology in Plain English
Here’s how they structured their system step by step:

Build a Specialized Contract Model: Take BERT and “continue” its training on a big library of contracts (some 60,000+ from LEDGAR). They add tasks like clause-word prediction so it “knows” if a word belongs to a particular legal clause.

Create a “Proxy” Dataset: They artificially remove a clause from a contract to simulate “missing a relevant clause.” That way, the system has examples of “When a contract is missing a clause it should have.”

Split the Problem:

Stage 1: Relevance Prediction. A classifier predicts if a type of clause (e.g., “Confidentiality”) is missing but needed.
Stage 2: Clause Text Recommendation. They do two versions:
A retrieval approach (like “grab from top 5 similar library clauses”)
A generation approach (like “draft from scratch, in the style of the rest of the contract”).
Compare with Baselines: They also included simpler methods:

Collaborative Filtering (“If most contracts that have clause X also have clause Y, suggest Y”)
Document Similarity (“Find similar contracts. If a majority had a particular clause type, we recommend it”).
Evaluate with standard numbers lawyers can interpret:

Precision/Recall/F1 for how often it suggests the right missing clause type.
ROUGE for how close the system’s suggested text is to a known “gold” clause.
Bottom Line for Lawyers
ChatGPT is excellent for interactive, freeform drafting or Q&A. But for tasks like systematically scanning a library of 60,000 contracts, classifying which clauses are present or absent, and methodically generating standard text, a BERT-based approach with specialized fine-tuning is often more controlled, more transparent, and easier to integrate into existing contract-drafting pipelines.
This CLAUSEREC pipeline doesn’t discard ChatGPT-like models altogether—in principle, one could pair them. But the authors’ methodology aims to build a self-contained system that deeply understands the “structure” of contracts, can handle classification, and has consistent generation specialized for legal language.
</criticism>
<final_instructions>
The goal is to reproduce the results of the paper. I will provide the paper, my original plan and some criticism. Please be aware of context limitations. You are in an environment managed by "uv" (NOT uvicorn). To install dependencies, use "uv pip install [dependency]".
Your tasks are:
1. Split these instructions into "digestible" parts. Remember to add a general index to these instructions (to each file, add 'You must consult the .index.txt file to understand the overall instructions').
2. Determine the best way to reproduce the results of the paper.
3. Provide a plan to reproduce the results of the paper that is compatible to the models' context.
Ideas: 
a. Split the plan in several parts: 
a1. .general_plan.txt: a general plan to reproduce the results of the paper, including the tree structure of the project.
a2. .consolidated_classes_functions.txt a simple list of classes and functions that are used in our project.
a3, etc: .class_implementation[class_name].txt: a detailed plan for the implementation of the class.
Feel free to organize the plan in a way that is most convenient for you.
</final_instructions>
</instructions>

aidra -m "Your instructions are contained in '/Users/arthrod/Library/CloudStorage/GoogleDrive-arthursrodrigues@gmail.com/My Drive/acode/contractrec/instructions.txt'. Remember, use quotes when trying to access files, including and specially the instructions file. Please read your instructions contained on .instructions.txt. Don't forget the quotes for files. When using aider, remember to pass --files argument AND openrouter as the provider of the inference (no changes to model choice). Fucking quotes when trying to access files, don't forget. Do NOT add a single placeholder! Implement one by one." --cowboy-mode --provider openrouter --model "anthropic/claude-3.5-sonnet:beta" --verbose
aidra -m "Remember, use quotes when trying to access files. Please read your instructions contained on instructions.txt. Don't forget the quotes for files. They are here: '/Users/arthrod/Library/CloudStorage/GoogleDrive-arthursrodrigues@gmail.com/My Drive/acode/contractrec/.instructions.txt' when using aider, remember to pass --files argument AND openrouter as the provider of the inference (no changes to model choice). Fucking quotes when trying to access files, don't forget" --cowboy-mode --provider openrouter --model "anthropic/claude-3.5-sonnet:beta" --verbose